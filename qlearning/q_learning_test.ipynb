{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "In order to perform the taular style q-learning, I have to make some sacrifice on the state\n",
    "representation. The original state representation is in the form of:\n",
    "\n",
    "`[server_type, response_time_on_server_0, response_time_on_server_1, ..., response_time_on_server_10]`,\n",
    "    \n",
    "which is impossible to store the corresponding q values due to the infinite possible states. \n",
    "\n",
    "The changes I made is to perform a sorting on the response times over all servers and use the relative \n",
    "scale indices of them as the new state, so the new state now looks like this:\n",
    "\n",
    "`[server_type, argsort(original_response_times_of_the_servers)]`\n",
    "    \n",
    "For example, if the orginal state is:\n",
    "\n",
    "`[1, 22.2324, 5.231, 0.1645, 3.21]`\n",
    "    \n",
    "then the new state will be:\n",
    "\n",
    "`[1, 2, 3, 1, 0]`,\n",
    "    \n",
    "where the first digits in both states represent the server type (1 for CPU and 0 for I/O)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initialization template is either None or invalid, the servers and jobs will be generated randomly\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from env_test import env\n",
    "from env_test import policies as baseline_policy\n",
    "from q_learning import trainer as QTrainer\n",
    "from q_learning import QLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward by bseline policies:\n",
      "random:      -2391.301937\n",
      "earlist:     -2370.109287\n",
      "round_robin: -2396.273717\n",
      "sensible:    -2392.052417\n",
      "bestfit:     -2275.335028\n"
     ]
    }
   ],
   "source": [
    "def policy_reward(env, policy):\n",
    "    env.reset()\n",
    "    while not env.is_terminal():\n",
    "        env.step(policy())\n",
    "    return env.cum_reward()\n",
    "print(\"Reward by bseline policies:\\n\"\n",
    "      \"random:      {:3f}\\n\"\n",
    "      \"earlist:     {:3f}\\n\"\n",
    "      \"round_robin: {:3f}\\n\"\n",
    "      \"sensible:    {:3f}\\n\"\n",
    "      \"bestfit:     {:3f}\".format(policy_reward(env, baseline_policy.random_policy),\n",
    "                           policy_reward(env, baseline_policy.earlist_policy),\n",
    "                           policy_reward(env, baseline_policy.round_robin_policy),\n",
    "                           policy_reward(env, baseline_policy.sensible_policy),\n",
    "                           policy_reward(env, baseline_policy.bestfit_policy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learner = QLearner(env, 0.05, 0.99)\n",
    "q_trainer = QTrainer(q_learner, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Average_updates: 0.238341, iterations: 49900,reward: -2379.088556\n",
      "Episode 200, Average_updates: 0.239164, iterations: 99800,reward: -2389.359307\n",
      "Episode 300, Average_updates: 0.239757, iterations: 149700,reward: -2393.931562\n",
      "Episode 400, Average_updates: 0.238656, iterations: 199600,reward: -2385.240810\n",
      "Episode 500, Average_updates: 0.238345, iterations: 249500,reward: -2381.485139\n",
      "Episode 600, Average_updates: 0.237800, iterations: 299400,reward: -2373.944281\n",
      "Episode 700, Average_updates: 0.237621, iterations: 349300,reward: -2373.738106\n",
      "Episode 800, Average_updates: 0.239764, iterations: 399200,reward: -2395.778802\n",
      "Episode 900, Average_updates: 0.238902, iterations: 449100,reward: -2386.371301\n",
      "Episode 1000, Average_updates: 0.238775, iterations: 499000,reward: -2385.719060\n",
      "Episode 1100, Average_updates: 0.238723, iterations: 548900,reward: -2386.772713\n",
      "Episode 1200, Average_updates: 0.240000, iterations: 598800,reward: -2392.445082\n",
      "Episode 1300, Average_updates: 0.237229, iterations: 648700,reward: -2370.462063\n",
      "Episode 1400, Average_updates: 0.240348, iterations: 698600,reward: -2401.225505\n",
      "Episode 1500, Average_updates: 0.240529, iterations: 748500,reward: -2398.298132\n",
      "Episode 1600, Average_updates: 0.238701, iterations: 798400,reward: -2379.000617\n",
      "Episode 1700, Average_updates: 0.240041, iterations: 848300,reward: -2399.310768\n",
      "Episode 1800, Average_updates: 0.238972, iterations: 898200,reward: -2385.076941\n",
      "Episode 1900, Average_updates: 0.237585, iterations: 948100,reward: -2371.367087\n",
      "Episode 2000, Average_updates: 0.240041, iterations: 998000,reward: -2398.910168\n",
      "Episode 2100, Average_updates: 0.240032, iterations: 1047900,reward: -2396.667625\n",
      "Episode 2200, Average_updates: 0.237814, iterations: 1097800,reward: -2375.637735\n",
      "Episode 2300, Average_updates: 0.239961, iterations: 1147700,reward: -2395.528940\n",
      "Episode 2400, Average_updates: 0.240840, iterations: 1197600,reward: -2405.852502\n",
      "Episode 2500, Average_updates: 0.238434, iterations: 1247500,reward: -2383.393389\n",
      "Episode 2600, Average_updates: 0.240224, iterations: 1297400,reward: -2401.367128\n",
      "Episode 2700, Average_updates: 0.238613, iterations: 1347300,reward: -2383.531055\n",
      "Episode 2800, Average_updates: 0.237886, iterations: 1397200,reward: -2375.807661\n",
      "Episode 2900, Average_updates: 0.240503, iterations: 1447100,reward: -2401.568762\n",
      "Episode 3000, Average_updates: 0.238356, iterations: 1497000,reward: -2379.727222\n",
      "Episode 3100, Average_updates: 0.239829, iterations: 1546900,reward: -2391.865004\n",
      "Episode 3200, Average_updates: 0.240268, iterations: 1596800,reward: -2392.223205\n",
      "Episode 3300, Average_updates: 0.239813, iterations: 1646700,reward: -2395.322909\n",
      "Episode 3400, Average_updates: 0.238717, iterations: 1696600,reward: -2379.203157\n",
      "Episode 3500, Average_updates: 0.241015, iterations: 1746500,reward: -2407.869997\n",
      "Episode 3600, Average_updates: 0.238491, iterations: 1796400,reward: -2377.549785\n",
      "Episode 3700, Average_updates: 0.237559, iterations: 1846300,reward: -2372.139489\n",
      "Episode 3800, Average_updates: 0.240366, iterations: 1896200,reward: -2399.942933\n",
      "Episode 3900, Average_updates: 0.238822, iterations: 1946100,reward: -2386.527515\n",
      "Episode 4000, Average_updates: 0.241281, iterations: 1996000,reward: -2410.164606\n",
      "Episode 4100, Average_updates: 0.239016, iterations: 2045900,reward: -2387.991240\n",
      "Episode 4200, Average_updates: 0.238688, iterations: 2095800,reward: -2382.581156\n",
      "Episode 4300, Average_updates: 0.239661, iterations: 2145700,reward: -2392.359198\n",
      "Episode 4400, Average_updates: 0.237695, iterations: 2195600,reward: -2369.567702\n",
      "Episode 4500, Average_updates: 0.238952, iterations: 2245500,reward: -2383.284753\n",
      "Episode 4600, Average_updates: 0.239775, iterations: 2295400,reward: -2394.922420\n",
      "Episode 4700, Average_updates: 0.238299, iterations: 2345300,reward: -2380.896837\n",
      "Episode 4800, Average_updates: 0.237507, iterations: 2395200,reward: -2370.925316\n",
      "Episode 4900, Average_updates: 0.240623, iterations: 2445100,reward: -2405.189963\n",
      "Episode 5000, Average_updates: 0.240839, iterations: 2495000,reward: -2405.982504\n",
      "Episode 5100, Average_updates: 0.239493, iterations: 2544900,reward: -2392.311127\n",
      "Episode 5200, Average_updates: 0.237337, iterations: 2594800,reward: -2369.610968\n",
      "Episode 5300, Average_updates: 0.239714, iterations: 2644700,reward: -2394.918143\n",
      "Episode 5400, Average_updates: 0.237329, iterations: 2694600,reward: -2372.942578\n",
      "Episode 5500, Average_updates: 0.236754, iterations: 2744500,reward: -2365.983783\n",
      "Episode 5600, Average_updates: 0.239080, iterations: 2794400,reward: -2382.191886\n",
      "Episode 5700, Average_updates: 0.239581, iterations: 2844300,reward: -2393.659929\n",
      "Episode 5800, Average_updates: 0.240286, iterations: 2894200,reward: -2400.608150\n",
      "Episode 5900, Average_updates: 0.237866, iterations: 2944100,reward: -2376.927224\n",
      "Episode 6000, Average_updates: 0.241474, iterations: 2994000,reward: -2412.126850\n",
      "Episode 6100, Average_updates: 0.239657, iterations: 3043900,reward: -2392.520231\n",
      "Episode 6200, Average_updates: 0.239607, iterations: 3093800,reward: -2392.017206\n",
      "Episode 6300, Average_updates: 0.239356, iterations: 3143700,reward: -2387.952518\n",
      "Episode 6400, Average_updates: 0.238263, iterations: 3193600,reward: -2381.270392\n",
      "Episode 6500, Average_updates: 0.238675, iterations: 3243500,reward: -2383.929716\n",
      "Episode 6600, Average_updates: 0.238345, iterations: 3293400,reward: -2380.201769\n",
      "Episode 6700, Average_updates: 0.238803, iterations: 3343300,reward: -2384.061395\n",
      "Episode 6800, Average_updates: 0.238336, iterations: 3393200,reward: -2381.421795\n",
      "Episode 6900, Average_updates: 0.237089, iterations: 3443100,reward: -2369.447077\n",
      "Episode 7000, Average_updates: 0.236947, iterations: 3493000,reward: -2360.685009\n",
      "Episode 7100, Average_updates: 0.239960, iterations: 3542900,reward: -2395.840794\n",
      "Episode 7200, Average_updates: 0.239306, iterations: 3592800,reward: -2390.228143\n",
      "Episode 7300, Average_updates: 0.239121, iterations: 3642700,reward: -2389.285419\n",
      "Episode 7400, Average_updates: 0.238218, iterations: 3692600,reward: -2378.606223\n",
      "Episode 7500, Average_updates: 0.240966, iterations: 3742500,reward: -2406.544516\n",
      "Episode 7600, Average_updates: 0.238393, iterations: 3792400,reward: -2380.615043\n",
      "Episode 7700, Average_updates: 0.237653, iterations: 3842300,reward: -2375.656074\n",
      "Episode 7800, Average_updates: 0.237777, iterations: 3892200,reward: -2375.591133\n",
      "Episode 7900, Average_updates: 0.238012, iterations: 3942100,reward: -2378.397733\n",
      "Episode 8000, Average_updates: 0.240151, iterations: 3992000,reward: -2398.745934\n",
      "Episode 8100, Average_updates: 0.240445, iterations: 4041900,reward: -2402.620514\n",
      "Episode 8200, Average_updates: 0.237783, iterations: 4091800,reward: -2373.289563\n",
      "Episode 8300, Average_updates: 0.239859, iterations: 4141700,reward: -2397.134915\n",
      "Episode 8400, Average_updates: 0.239165, iterations: 4191600,reward: -2383.719514\n",
      "Episode 8500, Average_updates: 0.238461, iterations: 4241500,reward: -2381.388895\n",
      "Episode 8600, Average_updates: 0.237722, iterations: 4291400,reward: -2374.765824\n",
      "Episode 8700, Average_updates: 0.239747, iterations: 4341300,reward: -2393.063749\n",
      "Episode 8800, Average_updates: 0.238198, iterations: 4391200,reward: -2379.476871\n",
      "Episode 8900, Average_updates: 0.240310, iterations: 4441100,reward: -2402.571134\n",
      "Episode 9000, Average_updates: 0.239699, iterations: 4491000,reward: -2396.867806\n",
      "Episode 9100, Average_updates: 0.237554, iterations: 4540900,reward: -2369.004316\n",
      "Episode 9200, Average_updates: 0.238690, iterations: 4590800,reward: -2383.690801\n",
      "Episode 9300, Average_updates: 0.239581, iterations: 4640700,reward: -2387.266176\n",
      "Episode 9400, Average_updates: 0.239838, iterations: 4690600,reward: -2394.756126\n",
      "Episode 9500, Average_updates: 0.238877, iterations: 4740500,reward: -2383.831587\n",
      "Episode 9600, Average_updates: 0.240287, iterations: 4790400,reward: -2395.986572\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-24ef4498edcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/JobScheduling/qlearning/q_learning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes, report_interval)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m           \u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnq_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"List out of range, with action {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/JobScheduling/qlearning/q_learning.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mqn_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mold_qval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmax_next_qval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqn_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax_next_qval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_qval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/JobScheduling/qlearning/q_learning.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_trainer.train(episodes=1000000, report_interval=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
