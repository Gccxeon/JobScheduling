{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initialization template is either None or invalid, the servers and jobs will be generated randomly\n"
     ]
    }
   ],
   "source": [
    "import trainer\n",
    "from test import env, replay_memory, collector, postprocessor\n",
    "from preprocess_dqn import process_from_replay_sample as preprocessor\n",
    "from dqn_agent_test import agent\n",
    "from transition import Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_helper = trainer.Trainer(env, \n",
    "                               agent, \n",
    "                               collector, \n",
    "                               replay_memory, \n",
    "                               postprocessor, \n",
    "                               preprocessor, \n",
    "                               Transition, \n",
    "                               300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, loss: 8953.9677734375, iterations: 0, cum_reward: 0.0\n",
      "Episode 1, loss: 3.4423160552978516, iterations: 500, cum_reward: 406617.3254521478\n",
      "Episode 2, loss: 4.045942306518555, iterations: 1000, cum_reward: 484668.11773424543\n",
      "Episode 3, loss: 3.5146381855010986, iterations: 1500, cum_reward: 333700.8259990692\n",
      "Episode 4, loss: 2.319775104522705, iterations: 2000, cum_reward: 336639.2728859027\n",
      "Episode 5, loss: 3.2727291584014893, iterations: 2500, cum_reward: 344202.5452527459\n",
      "Episode 6, loss: 3.699303388595581, iterations: 3000, cum_reward: 366121.5141564284\n",
      "Episode 7, loss: 2.81514310836792, iterations: 3500, cum_reward: 362791.8593963524\n",
      "Episode 8, loss: 7.684648513793945, iterations: 4000, cum_reward: 366121.6095501948\n",
      "Episode 9, loss: 3.547351837158203, iterations: 4500, cum_reward: 366121.4994317552\n",
      "Episode 10, loss: 2.6173291206359863, iterations: 5000, cum_reward: 380125.6828575644\n",
      "Episode 11, loss: 2.4749715328216553, iterations: 5500, cum_reward: 366252.7206061704\n",
      "Episode 12, loss: 3.4677929878234863, iterations: 6000, cum_reward: 373167.7567457146\n",
      "Episode 13, loss: 3.8904011249542236, iterations: 6500, cum_reward: 370261.8852670216\n",
      "Episode 14, loss: 2.4279234409332275, iterations: 7000, cum_reward: 370261.04238701257\n",
      "Episode 15, loss: 2.753162384033203, iterations: 7500, cum_reward: 373167.46998406533\n",
      "Episode 16, loss: 3.1448850631713867, iterations: 8000, cum_reward: 261850.6264698568\n",
      "Episode 17, loss: 2.3356873989105225, iterations: 8500, cum_reward: 336638.8729639773\n",
      "Episode 18, loss: 3.859217405319214, iterations: 9000, cum_reward: 366121.46888601925\n",
      "Episode 19, loss: 3.0769436359405518, iterations: 9500, cum_reward: 366121.70764144347\n",
      "Episode 20, loss: 4.391817092895508, iterations: 10000, cum_reward: 336639.4461893245\n",
      "Episode 21, loss: 4.519371032714844, iterations: 10500, cum_reward: 365174.60022957384\n",
      "Episode 22, loss: 2.8655126094818115, iterations: 11000, cum_reward: 370261.56878807704\n",
      "Episode 23, loss: 2.7356042861938477, iterations: 11500, cum_reward: 366121.6258187205\n",
      "Episode 24, loss: 3.7547900676727295, iterations: 12000, cum_reward: 370261.75783471414\n",
      "Episode 25, loss: 4.071774005889893, iterations: 12500, cum_reward: 376888.1703322458\n",
      "Episode 26, loss: 3.705857038497925, iterations: 13000, cum_reward: 296971.3675943822\n",
      "Episode 27, loss: 2.696690082550049, iterations: 13500, cum_reward: 362107.0661232874\n",
      "Episode 28, loss: 6.440892219543457, iterations: 14000, cum_reward: 336637.3791778611\n",
      "Episode 29, loss: 4.077558517456055, iterations: 14500, cum_reward: 370257.9791961715\n",
      "Episode 30, loss: 3.111588478088379, iterations: 15000, cum_reward: 359082.6807157464\n",
      "Episode 31, loss: 3.233522653579712, iterations: 15500, cum_reward: 336639.18018762453\n",
      "Episode 32, loss: 4.6603684425354, iterations: 16000, cum_reward: 456906.90061275766\n",
      "Episode 33, loss: 2.5340046882629395, iterations: 16500, cum_reward: 493358.9794377557\n",
      "Episode 34, loss: 2.189093828201294, iterations: 17000, cum_reward: 491623.57531566056\n",
      "Episode 35, loss: 2.8390233516693115, iterations: 17500, cum_reward: 491623.1750216986\n",
      "Episode 36, loss: 4.181589126586914, iterations: 18000, cum_reward: 491623.467819211\n",
      "Episode 37, loss: 7.235454559326172, iterations: 18500, cum_reward: 491623.38444396935\n",
      "Episode 38, loss: 8.226232528686523, iterations: 19000, cum_reward: 491623.6361483718\n",
      "Episode 39, loss: 3.660332202911377, iterations: 19500, cum_reward: 491814.4140571028\n",
      "Episode 40, loss: 3.8540353775024414, iterations: 20000, cum_reward: 404447.1975751897\n",
      "Episode 41, loss: 10948.4501953125, iterations: 20500, cum_reward: 379407.0821675637\n",
      "Episode 42, loss: 2.5228943824768066, iterations: 21000, cum_reward: 396141.5618993521\n",
      "Episode 43, loss: 1.6640897989273071, iterations: 21500, cum_reward: 393282.4999139765\n",
      "Episode 44, loss: 2.160923957824707, iterations: 22000, cum_reward: 326568.69945838285\n",
      "Episode 45, loss: 2.181519031524658, iterations: 22500, cum_reward: 398948.16941709945\n",
      "Episode 46, loss: 2.8391807079315186, iterations: 23000, cum_reward: 386050.58979549375\n",
      "Episode 47, loss: 2.8275258541107178, iterations: 23500, cum_reward: 388730.7003168795\n",
      "Episode 48, loss: 11549.255859375, iterations: 24000, cum_reward: 402731.10323733045\n",
      "Episode 49, loss: 2.072688341140747, iterations: 24500, cum_reward: 388730.07075919886\n",
      "Episode 50, loss: 2.557420015335083, iterations: 25000, cum_reward: 362908.3001802655\n",
      "Episode 51, loss: 7.09339714050293, iterations: 25500, cum_reward: 310397.79516885895\n",
      "Episode 52, loss: 3.3748178482055664, iterations: 26000, cum_reward: 310397.18181960133\n",
      "Episode 53, loss: 2.041539192199707, iterations: 26500, cum_reward: 386057.0929621577\n",
      "Episode 54, loss: 3.0822503566741943, iterations: 27000, cum_reward: 388733.84653169324\n",
      "Episode 55, loss: 2.3935482501983643, iterations: 27500, cum_reward: 388733.7788096286\n",
      "Episode 56, loss: 2.5784826278686523, iterations: 28000, cum_reward: 388733.36728385615\n",
      "Episode 57, loss: 2.7143290042877197, iterations: 28500, cum_reward: 388732.7001704519\n",
      "Episode 58, loss: 1.959205985069275, iterations: 29000, cum_reward: 388731.54417844774\n",
      "Episode 59, loss: 2.828461170196533, iterations: 29500, cum_reward: 402737.32764007314\n",
      "Episode 60, loss: 2.9194164276123047, iterations: 30000, cum_reward: 388730.22490931087\n",
      "Episode 61, loss: 3.625156879425049, iterations: 30500, cum_reward: 310397.1370507768\n",
      "Episode 62, loss: 7.358325004577637, iterations: 31000, cum_reward: 310398.67902326764\n",
      "Episode 63, loss: 4.454017639160156, iterations: 31500, cum_reward: 395778.760007377\n",
      "Episode 64, loss: 4.136530876159668, iterations: 32000, cum_reward: 395779.748047982\n",
      "Episode 65, loss: 3.3389599323272705, iterations: 32500, cum_reward: 395779.06922979874\n",
      "Episode 66, loss: 3.121805191040039, iterations: 33000, cum_reward: 284409.01740456914\n",
      "Episode 67, loss: 2.823259115219116, iterations: 33500, cum_reward: 395779.84133122483\n",
      "Episode 68, loss: 2.8136539459228516, iterations: 34000, cum_reward: 398454.41822548525\n",
      "Episode 69, loss: 1.3382683992385864, iterations: 34500, cum_reward: 401170.14215147955\n",
      "Episode 70, loss: 3.1370673179626465, iterations: 35000, cum_reward: 390959.20161599456\n",
      "Episode 71, loss: 2.9343087673187256, iterations: 35500, cum_reward: 227153.64272533255\n",
      "Episode 72, loss: 3.29889178276062, iterations: 36000, cum_reward: 325351.00564741174\n",
      "Episode 73, loss: 3.0786542892456055, iterations: 36500, cum_reward: 310397.2730978792\n",
      "Episode 74, loss: 5.431278705596924, iterations: 37000, cum_reward: 394071.93093555985\n",
      "Episode 75, loss: 2.497304677963257, iterations: 37500, cum_reward: 388732.8251014941\n",
      "Episode 76, loss: 4.088932991027832, iterations: 38000, cum_reward: 388729.56189367303\n",
      "Episode 77, loss: 3.043811082839966, iterations: 38500, cum_reward: 388726.8694816531\n",
      "Episode 78, loss: 3.663846492767334, iterations: 39000, cum_reward: 388729.791391524\n",
      "Episode 79, loss: 2.393509864807129, iterations: 39500, cum_reward: 385654.47324226744\n",
      "Episode 80, loss: 4.223259449005127, iterations: 40000, cum_reward: 388729.14695416443\n",
      "Episode 81, loss: 2.9489970207214355, iterations: 40500, cum_reward: 310396.5776457394\n",
      "Episode 82, loss: 1.9028486013412476, iterations: 41000, cum_reward: 411156.9142481927\n",
      "Episode 83, loss: 3.9159934520721436, iterations: 41500, cum_reward: 388806.60132031346\n",
      "Episode 84, loss: 3.687408685684204, iterations: 42000, cum_reward: 310397.73374890414\n",
      "Episode 85, loss: 11478.5927734375, iterations: 42500, cum_reward: 390770.10587462224\n",
      "Episode 86, loss: 2.9758288860321045, iterations: 43000, cum_reward: 390770.5723390371\n",
      "Episode 87, loss: 3.614283323287964, iterations: 43500, cum_reward: 284409.38975015946\n",
      "Episode 88, loss: 4.19141960144043, iterations: 44000, cum_reward: 390767.92639875255\n",
      "Episode 89, loss: 1.7710072994232178, iterations: 44500, cum_reward: 310398.1660417112\n",
      "Episode 90, loss: 1.7024645805358887, iterations: 45000, cum_reward: 310397.1739784912\n",
      "Episode 91, loss: 3.91839599609375, iterations: 45500, cum_reward: 377464.1332639119\n",
      "Episode 92, loss: 2.5976738929748535, iterations: 46000, cum_reward: 388807.65108436486\n",
      "Episode 93, loss: 2.9622585773468018, iterations: 46500, cum_reward: 284405.75625139184\n",
      "Episode 94, loss: 2.0236778259277344, iterations: 47000, cum_reward: 284405.861564099\n",
      "Episode 95, loss: 2.184661626815796, iterations: 47500, cum_reward: 284404.67603979644\n",
      "Episode 96, loss: 2.4588465690612793, iterations: 48000, cum_reward: 388807.5643103238\n",
      "Episode 97, loss: 1.5799696445465088, iterations: 48500, cum_reward: 374183.77634237736\n",
      "Episode 98, loss: 2.1344358921051025, iterations: 49000, cum_reward: 394068.2274339226\n",
      "Episode 99, loss: 2.1903793811798096, iterations: 49500, cum_reward: 388806.3855269872\n",
      "Episode 100, loss: 2.5518248081207275, iterations: 50000, cum_reward: 387643.92695382837\n",
      "Episode 101, loss: 2.453925132751465, iterations: 50500, cum_reward: 388807.3229714675\n",
      "Episode 102, loss: 2.1864185333251953, iterations: 51000, cum_reward: 388806.5950016193\n",
      "Episode 103, loss: 2.842937707901001, iterations: 51500, cum_reward: 379067.29873885924\n",
      "Episode 104, loss: 2.288353443145752, iterations: 52000, cum_reward: 388806.98150442523\n",
      "Episode 105, loss: 2.9572250843048096, iterations: 52500, cum_reward: 388805.53801343235\n",
      "Episode 106, loss: 2.5694820880889893, iterations: 53000, cum_reward: 401895.5576686957\n",
      "Episode 107, loss: 1.9018971920013428, iterations: 53500, cum_reward: 388807.2392142364\n",
      "Episode 108, loss: 1.643337607383728, iterations: 54000, cum_reward: 386131.31650891126\n",
      "Episode 109, loss: 1.9219050407409668, iterations: 54500, cum_reward: 408210.25294234673\n",
      "Episode 110, loss: 9206.662109375, iterations: 55000, cum_reward: 394068.33234614163\n",
      "Episode 111, loss: 2.439100742340088, iterations: 55500, cum_reward: 284405.8438751273\n",
      "Episode 112, loss: 5.1647114753723145, iterations: 56000, cum_reward: 284405.89475022984\n",
      "Episode 113, loss: 2.594414472579956, iterations: 56500, cum_reward: 284405.80567574565\n",
      "Episode 114, loss: 4603.06591796875, iterations: 57000, cum_reward: 284405.8055516456\n",
      "Episode 115, loss: 4568.01611328125, iterations: 57500, cum_reward: 284405.8618737988\n",
      "Episode 116, loss: 2.095611333847046, iterations: 58000, cum_reward: 310397.38055560866\n",
      "Episode 117, loss: 2.325120687484741, iterations: 58500, cum_reward: 390766.1044888583\n",
      "Episode 118, loss: 1.592208981513977, iterations: 59000, cum_reward: 284405.81805830164\n",
      "Episode 119, loss: 2.4875638484954834, iterations: 59500, cum_reward: 284405.8342921332\n",
      "Episode 120, loss: 3.1748433113098145, iterations: 60000, cum_reward: 378665.78328541963\n",
      "Episode 121, loss: 3.017824411392212, iterations: 60500, cum_reward: 316949.88484025263\n",
      "Episode 122, loss: 4.015545845031738, iterations: 61000, cum_reward: 384792.38721028465\n",
      "Episode 123, loss: 2.0188565254211426, iterations: 61500, cum_reward: 394508.76597408386\n",
      "Episode 124, loss: 2.4379723072052, iterations: 62000, cum_reward: 284405.97969486465\n",
      "Episode 125, loss: 2.3070969581604004, iterations: 62500, cum_reward: 284405.83675672644\n",
      "Episode 126, loss: 2.337721586227417, iterations: 63000, cum_reward: 284403.9876098747\n",
      "Episode 127, loss: 3.6807162761688232, iterations: 63500, cum_reward: 317955.1324024288\n",
      "Episode 128, loss: 3.193685531616211, iterations: 64000, cum_reward: 238021.17634800234\n",
      "Episode 129, loss: 2.5332982540130615, iterations: 64500, cum_reward: 238021.17634800234\n",
      "Episode 130, loss: 10271.7939453125, iterations: 65000, cum_reward: 238024.19881016747\n",
      "Episode 131, loss: 5.509520530700684, iterations: 65500, cum_reward: 238024.45031587378\n",
      "Episode 132, loss: 15.509169578552246, iterations: 66000, cum_reward: 322419.68104818836\n",
      "Episode 133, loss: 2.765934705734253, iterations: 66500, cum_reward: 394823.7720439048\n",
      "Episode 134, loss: 4.431971549987793, iterations: 67000, cum_reward: 395783.25611838483\n",
      "Episode 135, loss: 3.069132089614868, iterations: 67500, cum_reward: 318557.3858818889\n",
      "Episode 136, loss: 3.928948163986206, iterations: 68000, cum_reward: 246784.70153035404\n",
      "Episode 137, loss: 4.136760234832764, iterations: 68500, cum_reward: 318634.9701381609\n",
      "Episode 138, loss: 2.794797658920288, iterations: 69000, cum_reward: 318635.033387621\n",
      "Episode 139, loss: 2.0733659267425537, iterations: 69500, cum_reward: 329207.8323856167\n",
      "Episode 140, loss: 3.0188872814178467, iterations: 70000, cum_reward: 392306.8418575222\n",
      "Episode 141, loss: 3.84324049949646, iterations: 70500, cum_reward: 388266.96397890465\n",
      "Episode 142, loss: 2.738936424255371, iterations: 71000, cum_reward: 395784.1694406407\n",
      "Episode 143, loss: 2.026549816131592, iterations: 71500, cum_reward: 384714.5827553354\n",
      "Episode 144, loss: 4.908666610717773, iterations: 72000, cum_reward: 371065.69214525446\n",
      "Episode 145, loss: 2.2876546382904053, iterations: 72500, cum_reward: 322575.38888939127\n",
      "Episode 146, loss: 2.076141834259033, iterations: 73000, cum_reward: 318556.3942696056\n",
      "Episode 147, loss: 2.198974609375, iterations: 73500, cum_reward: 249999.8538324295\n",
      "Episode 148, loss: 2.2291243076324463, iterations: 74000, cum_reward: 318557.5344831494\n",
      "Episode 149, loss: 3.2936301231384277, iterations: 74500, cum_reward: 328279.80724350346\n",
      "Episode 150, loss: 2.3911240100860596, iterations: 75000, cum_reward: 318634.9723176161\n",
      "Episode 151, loss: 9.29672908782959, iterations: 75500, cum_reward: 318557.4891622888\n",
      "Episode 152, loss: 2.38254714012146, iterations: 76000, cum_reward: 318557.7290064172\n",
      "Episode 153, loss: 3.1520943641662598, iterations: 76500, cum_reward: 318557.79542862746\n",
      "Episode 154, loss: 2.5464231967926025, iterations: 77000, cum_reward: 318557.66288526513\n",
      "Episode 155, loss: 2.555703639984131, iterations: 77500, cum_reward: 318557.89794553915\n",
      "Episode 156, loss: 7311.19970703125, iterations: 78000, cum_reward: 318557.7336436944\n",
      "Episode 157, loss: 2.048628330230713, iterations: 78500, cum_reward: 318557.4240966672\n",
      "Episode 158, loss: 1.5382049083709717, iterations: 79000, cum_reward: 318557.450518121\n",
      "Episode 159, loss: 3.0936942100524902, iterations: 79500, cum_reward: 318557.5306840285\n",
      "Episode 160, loss: 1.9593690633773804, iterations: 80000, cum_reward: 318557.60765056644\n",
      "Episode 161, loss: 2.164755344390869, iterations: 80500, cum_reward: 318557.5761632291\n",
      "Episode 162, loss: 3.1529388427734375, iterations: 81000, cum_reward: 318557.64620459446\n",
      "Episode 163, loss: 9314.8779296875, iterations: 81500, cum_reward: 318557.5925156642\n",
      "Episode 164, loss: 1.9262040853500366, iterations: 82000, cum_reward: 318557.4828025188\n",
      "Episode 165, loss: 2.3032898902893066, iterations: 82500, cum_reward: 398827.36892231595\n",
      "Episode 166, loss: 2.816042184829712, iterations: 83000, cum_reward: 328510.94295558846\n",
      "Episode 167, loss: 2.413963556289673, iterations: 83500, cum_reward: 238023.8141707012\n",
      "Episode 168, loss: 2.4740493297576904, iterations: 84000, cum_reward: 381888.6744767298\n",
      "Episode 169, loss: 1.9628630876541138, iterations: 84500, cum_reward: 314816.12732915056\n",
      "Episode 170, loss: 2.502671718597412, iterations: 85000, cum_reward: 312411.3689182976\n",
      "Episode 171, loss: 1.8804577589035034, iterations: 85500, cum_reward: 377720.1380001844\n",
      "Episode 172, loss: 2.5193660259246826, iterations: 86000, cum_reward: 370756.04409270495\n",
      "Episode 173, loss: 2.702589750289917, iterations: 86500, cum_reward: 315862.7640174542\n",
      "Episode 174, loss: 2.1472134590148926, iterations: 87000, cum_reward: 318557.64688655373\n",
      "Episode 175, loss: 2.2743468284606934, iterations: 87500, cum_reward: 318557.37909082905\n",
      "Episode 176, loss: 3.232027292251587, iterations: 88000, cum_reward: 318557.96692679427\n",
      "Episode 177, loss: 1.6007128953933716, iterations: 88500, cum_reward: 318558.07243850967\n",
      "Episode 178, loss: 2.7769999504089355, iterations: 89000, cum_reward: 318558.260581875\n",
      "Episode 179, loss: 2.52502703666687, iterations: 89500, cum_reward: 397351.2955862622\n",
      "Episode 180, loss: 35.2963981628418, iterations: 90000, cum_reward: 475490.727084621\n",
      "Episode 181, loss: 2.9970638751983643, iterations: 90500, cum_reward: 364771.43280568154\n",
      "Episode 182, loss: 3.6103267669677734, iterations: 91000, cum_reward: 335412.97366102255\n",
      "Episode 183, loss: 3.1899518966674805, iterations: 91500, cum_reward: 372690.3545488239\n",
      "Episode 184, loss: 2.6235811710357666, iterations: 92000, cum_reward: 238024.84119661653\n",
      "Episode 185, loss: 8.947376251220703, iterations: 92500, cum_reward: 238024.82138106087\n",
      "Episode 186, loss: 25.40299415588379, iterations: 93000, cum_reward: 238024.90345904022\n",
      "Episode 187, loss: 2.624281167984009, iterations: 93500, cum_reward: 314260.12716387765\n",
      "Episode 188, loss: 2.883810520172119, iterations: 94000, cum_reward: 388936.11685148755\n",
      "Episode 189, loss: 2.3992528915405273, iterations: 94500, cum_reward: 386156.6527900609\n",
      "Episode 190, loss: 1.6165568828582764, iterations: 95000, cum_reward: 329207.94521857484\n",
      "Episode 191, loss: 2.8705904483795166, iterations: 95500, cum_reward: 296229.5034227949\n",
      "Episode 192, loss: 2.3623316287994385, iterations: 96000, cum_reward: 295131.18629904883\n",
      "Episode 193, loss: 1.7077852487564087, iterations: 96500, cum_reward: 388334.8544535242\n",
      "Episode 194, loss: 1.327721357345581, iterations: 97000, cum_reward: 290338.6556951221\n",
      "Episode 195, loss: 3.397378444671631, iterations: 97500, cum_reward: 397471.91966161394\n",
      "Episode 196, loss: 2.111565351486206, iterations: 98000, cum_reward: 375507.46654548764\n",
      "Episode 197, loss: 2.7832822799682617, iterations: 98500, cum_reward: 383403.86361569125\n",
      "Episode 198, loss: 2.2704081535339355, iterations: 99000, cum_reward: 284409.2640362479\n",
      "Episode 199, loss: 2.1194345951080322, iterations: 99500, cum_reward: 383444.0232041063\n",
      "Episode 200, loss: 3.173682928085327, iterations: 100000, cum_reward: 294912.57506452425\n",
      "Episode 201, loss: 1.45326828956604, iterations: 100500, cum_reward: 363708.0081016222\n",
      "Episode 202, loss: 1.3560928106307983, iterations: 101000, cum_reward: 281881.7386984435\n",
      "Episode 203, loss: 2.1810994148254395, iterations: 101500, cum_reward: 398593.9149552255\n",
      "Episode 204, loss: 1.2915675640106201, iterations: 102000, cum_reward: 284408.11384111986\n",
      "Episode 205, loss: 1.944836139678955, iterations: 102500, cum_reward: 398593.3506707362\n",
      "Episode 206, loss: 2.0398008823394775, iterations: 103000, cum_reward: 284406.2926856794\n",
      "Episode 207, loss: 2.455575704574585, iterations: 103500, cum_reward: 282781.358457753\n",
      "Episode 208, loss: 1.2075871229171753, iterations: 104000, cum_reward: 297234.29382986866\n",
      "Episode 209, loss: 1.7776820659637451, iterations: 104500, cum_reward: 284408.9979450454\n",
      "Episode 210, loss: 11100.6435546875, iterations: 105000, cum_reward: 240421.09499448605\n",
      "Episode 211, loss: 2.3514673709869385, iterations: 105500, cum_reward: 247746.77483085523\n",
      "Episode 212, loss: 2.203233480453491, iterations: 106000, cum_reward: 239055.0437395496\n",
      "Episode 213, loss: 3758.823486328125, iterations: 106500, cum_reward: 234237.18043988448\n",
      "Episode 214, loss: 1.7766332626342773, iterations: 107000, cum_reward: 234237.212064837\n",
      "Episode 215, loss: 1.5761371850967407, iterations: 107500, cum_reward: 247746.84711744543\n",
      "Episode 216, loss: 1.4998703002929688, iterations: 108000, cum_reward: 225422.82269416665\n",
      "Episode 217, loss: 2.021449565887451, iterations: 108500, cum_reward: 144423.81493445733\n",
      "Episode 218, loss: 2.2815160751342773, iterations: 109000, cum_reward: 247746.82238957655\n",
      "Episode 219, loss: 1.5007230043411255, iterations: 109500, cum_reward: 239055.0621093325\n",
      "Episode 220, loss: 2.160038471221924, iterations: 110000, cum_reward: 247692.14588063353\n",
      "Episode 221, loss: 1.896278738975525, iterations: 110500, cum_reward: 141384.98274364363\n",
      "Episode 222, loss: 1.5847889184951782, iterations: 111000, cum_reward: 239055.11126652497\n",
      "Episode 223, loss: 1.8441680669784546, iterations: 111500, cum_reward: 238294.89713992106\n",
      "Episode 224, loss: 2.0633137226104736, iterations: 112000, cum_reward: 238025.0084622723\n",
      "Episode 225, loss: 1.761493444442749, iterations: 112500, cum_reward: 245787.45214404236\n",
      "Episode 226, loss: 2.126915216445923, iterations: 113000, cum_reward: 238025.11754624412\n",
      "Episode 227, loss: 1.6964391469955444, iterations: 113500, cum_reward: 238025.14743340184\n",
      "Episode 228, loss: 1.6799967288970947, iterations: 114000, cum_reward: 241773.02886445314\n",
      "Episode 229, loss: 1.8236219882965088, iterations: 114500, cum_reward: 145517.89140393492\n",
      "Episode 230, loss: 1.8859922885894775, iterations: 115000, cum_reward: 251488.349089176\n",
      "Episode 231, loss: 0.9693673253059387, iterations: 115500, cum_reward: 141384.9707618151\n",
      "Episode 232, loss: 1.1821876764297485, iterations: 116000, cum_reward: 238025.06644562056\n",
      "Episode 233, loss: 2.0839591026306152, iterations: 116500, cum_reward: 238025.1488144443\n",
      "Episode 234, loss: 3.294900894165039, iterations: 117000, cum_reward: 244358.74691254218\n",
      "Episode 235, loss: 1.5049993991851807, iterations: 117500, cum_reward: 241773.1178644681\n",
      "Episode 236, loss: 1.903179407119751, iterations: 118000, cum_reward: 238025.14910258903\n",
      "Episode 237, loss: 1.4331392049789429, iterations: 118500, cum_reward: 238025.1475539077\n",
      "Episode 238, loss: 1.4260770082473755, iterations: 119000, cum_reward: 238025.11830814308\n",
      "Episode 239, loss: 1.691482663154602, iterations: 119500, cum_reward: 238025.1011016515\n",
      "Episode 240, loss: 1.578176498413086, iterations: 120000, cum_reward: 258357.39626003968\n",
      "Episode 241, loss: 2.427673578262329, iterations: 120500, cum_reward: 238025.00858432884\n",
      "Episode 242, loss: 2.5870718955993652, iterations: 121000, cum_reward: 238024.65551918861\n",
      "Episode 243, loss: 2.6537859439849854, iterations: 121500, cum_reward: 241773.08961440172\n",
      "Episode 244, loss: 2.3724610805511475, iterations: 122000, cum_reward: 251023.63064403064\n",
      "Episode 245, loss: 2.331850051879883, iterations: 122500, cum_reward: 247692.20711069653\n",
      "Episode 246, loss: 2.1867570877075195, iterations: 123000, cum_reward: 140225.97131035122\n",
      "Episode 247, loss: 2.611262083053589, iterations: 123500, cum_reward: 247746.8107545225\n",
      "Episode 248, loss: 3.1666178703308105, iterations: 124000, cum_reward: 247746.87933437148\n",
      "Episode 249, loss: 3.036611318588257, iterations: 124500, cum_reward: 238025.1044672668\n",
      "Episode 250, loss: 2.3791346549987793, iterations: 125000, cum_reward: 245787.41993964836\n",
      "Episode 251, loss: 1.9367029666900635, iterations: 125500, cum_reward: 238025.07265996185\n",
      "Episode 252, loss: 9.742982864379883, iterations: 126000, cum_reward: 241772.91846495646\n",
      "Episode 253, loss: 1.679713487625122, iterations: 126500, cum_reward: 238025.0484915393\n",
      "Episode 254, loss: 1.842128038406372, iterations: 127000, cum_reward: 238294.6699441339\n",
      "Episode 255, loss: 2.2662253379821777, iterations: 127500, cum_reward: 238025.067736478\n",
      "Episode 256, loss: 1.738961935043335, iterations: 128000, cum_reward: 245787.21240591482\n",
      "Episode 257, loss: 2.4043498039245605, iterations: 128500, cum_reward: 247746.8339138336\n",
      "Episode 258, loss: 2.144918918609619, iterations: 129000, cum_reward: 247746.72613550982\n",
      "Episode 259, loss: 10113.119140625, iterations: 129500, cum_reward: 238025.0157608125\n",
      "Episode 260, loss: 1.8363356590270996, iterations: 130000, cum_reward: 238024.8639004168\n",
      "Episode 261, loss: 2.9565956592559814, iterations: 130500, cum_reward: 238024.4833132752\n",
      "Episode 262, loss: 4.169428825378418, iterations: 131000, cum_reward: 231884.16931328547\n",
      "Episode 263, loss: 2904.392822265625, iterations: 131500, cum_reward: 238025.03779950505\n",
      "Episode 264, loss: 3.1995089054107666, iterations: 132000, cum_reward: 144424.14777424702\n",
      "Episode 265, loss: 1.5074856281280518, iterations: 132500, cum_reward: 247746.83384762102\n",
      "Episode 266, loss: 1.9044021368026733, iterations: 133000, cum_reward: 238025.0265144325\n",
      "Episode 267, loss: 1.5724855661392212, iterations: 133500, cum_reward: 241773.05343571672\n",
      "Episode 268, loss: 2156.4912109375, iterations: 134000, cum_reward: 238025.16988703917\n",
      "Episode 269, loss: 1.7073891162872314, iterations: 134500, cum_reward: 238025.1562268306\n",
      "Episode 270, loss: 1.8846162557601929, iterations: 135000, cum_reward: 241773.10013307605\n",
      "Episode 271, loss: 3.219306468963623, iterations: 135500, cum_reward: 248184.27430368136\n",
      "Episode 272, loss: 2.169783115386963, iterations: 136000, cum_reward: 238025.18033302113\n",
      "Episode 273, loss: 3573.986328125, iterations: 136500, cum_reward: 234257.6437078527\n",
      "Episode 274, loss: 3.226072072982788, iterations: 137000, cum_reward: 247746.78754409726\n",
      "Episode 275, loss: 1.9659829139709473, iterations: 137500, cum_reward: 247746.79678372387\n",
      "Episode 276, loss: 1.7606356143951416, iterations: 138000, cum_reward: 238025.1523536749\n",
      "Episode 277, loss: 2.569777488708496, iterations: 138500, cum_reward: 238025.05790890817\n",
      "Episode 278, loss: 1.6092171669006348, iterations: 139000, cum_reward: 238025.06897880978\n",
      "Episode 279, loss: 2.754237174987793, iterations: 139500, cum_reward: 139758.45398531476\n",
      "Episode 280, loss: 1.437363862991333, iterations: 140000, cum_reward: 247746.7798449802\n",
      "Episode 281, loss: 1.725474238395691, iterations: 140500, cum_reward: 238025.00224798912\n",
      "Episode 282, loss: 2.0627901554107666, iterations: 141000, cum_reward: 244358.52069627462\n",
      "Episode 283, loss: 1.588405728340149, iterations: 141500, cum_reward: 238025.1060867589\n",
      "Episode 284, loss: 59.52930450439453, iterations: 142000, cum_reward: 141099.85432937098\n",
      "Episode 285, loss: 1.9039469957351685, iterations: 142500, cum_reward: 238294.87103476125\n",
      "Episode 286, loss: 1878.5780029296875, iterations: 143000, cum_reward: 238024.99946430718\n",
      "Episode 287, loss: 2.8803062438964844, iterations: 143500, cum_reward: 241773.03865433138\n",
      "Episode 288, loss: 2.33259654045105, iterations: 144000, cum_reward: 241772.9820824499\n",
      "Episode 289, loss: 1.1078288555145264, iterations: 144500, cum_reward: 238294.46765514463\n",
      "Episode 290, loss: 1.1820470094680786, iterations: 145000, cum_reward: 139758.45977712076\n",
      "Episode 291, loss: 1.12274169921875, iterations: 145500, cum_reward: 243971.4434548387\n",
      "Episode 292, loss: 1.6258481740951538, iterations: 146000, cum_reward: 238025.06782444054\n",
      "Episode 293, loss: 1.4124748706817627, iterations: 146500, cum_reward: 238025.14288898304\n",
      "Episode 294, loss: 2.6476666927337646, iterations: 147000, cum_reward: 238025.07676987594\n",
      "Episode 295, loss: 1.3972573280334473, iterations: 147500, cum_reward: 244358.59025653458\n",
      "Episode 296, loss: 2.012319326400757, iterations: 148000, cum_reward: 241773.06324320557\n",
      "Episode 297, loss: 1.726883888244629, iterations: 148500, cum_reward: 247746.73694937557\n",
      "Episode 298, loss: 1.8870537281036377, iterations: 149000, cum_reward: 247746.69968561525\n",
      "Episode 299, loss: 2.8905997276306152, iterations: 149500, cum_reward: 243529.62008247557\n"
     ]
    }
   ],
   "source": [
    "train_helper.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "env.reset()\n",
    "actions = [0] * 10\n",
    "for i in range(499):\n",
    "    t = Transition(*env.step(action))\n",
    "    t = postprocessor.process(t)\n",
    "    t = preprocessor(t)\n",
    "    action = int(agent.default_policy(t.next_state_nt))\n",
    "    actions[action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895708915bf747ee86fe4e94c14954ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'name': 'CPU intensive jobs',\n",
       "              'type': 'bar',\n",
       "              'uid': 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from status_render import StatusRender\n",
    "render = StatusRender(env)\n",
    "render.job_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'IO', 'cpu_power': 1187.3829711512403, 'io_power': 1430.0619669374523}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_server_info(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_helper._agent._global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_helper._env.is_terminal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9000]],\n",
       "\n",
       "        [[0.3262]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_helper._agent._policy_net.parameters())[0].detach(.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.6288]],\n",
       "\n",
       "        [[-0.3396]]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_helper._agent._target_net.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = preprocessor(replay_memory.sample(1)[0]).state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([69.9008], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([7]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._policy_net(state).max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2124)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.rand(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
