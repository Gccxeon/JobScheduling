{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initialization template is either None or invalid, the servers and jobs will be generated randomly\n"
     ]
    }
   ],
   "source": [
    "from env_test import env, replay_memory, collector, samplizer_\n",
    "from dqn_agent_test import agent\n",
    "from transition import Transition\n",
    "from preprocess_dqn import process_from_replay_sample as dqn_preprocess\n",
    "import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_helper = trainer.Trainer(env, \n",
    "                               agent, \n",
    "                               collector, \n",
    "                               replay_memory,  \n",
    "                               samplizer_, \n",
    "                               Transition, \n",
    "                               10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, loss: 2.4333243370056152, iterations: 500, cum_reward: -3918.9623597493046\n",
      "Episode 2, loss: 5.167402267456055, iterations: 1000, cum_reward: -4923.8607812479895\n",
      "Episode 3, loss: 3.8592824935913086, iterations: 1500, cum_reward: -3459.0034848504893\n",
      "Episode 4, loss: 3.946061611175537, iterations: 2000, cum_reward: -5065.437294281213\n",
      "Episode 5, loss: 4.802456378936768, iterations: 2500, cum_reward: -5411.970650728077\n",
      "Episode 6, loss: 1.2275452613830566, iterations: 3000, cum_reward: -3951.7301460584367\n",
      "Episode 7, loss: 3.7121424674987793, iterations: 3500, cum_reward: -2722.5262993332562\n",
      "Episode 8, loss: 0.6804080605506897, iterations: 4000, cum_reward: -2946.6629538609373\n",
      "Episode 9, loss: 3.6844418048858643, iterations: 4500, cum_reward: -2806.019024302261\n",
      "Episode 10, loss: 2.6282525062561035, iterations: 5000, cum_reward: -3563.24346102396\n",
      "Episode 11, loss: 4.985025405883789, iterations: 5500, cum_reward: -4184.113740767657\n",
      "Episode 12, loss: 3.7167282104492188, iterations: 6000, cum_reward: -3341.0332940713956\n",
      "Episode 13, loss: 3.4767165184020996, iterations: 6500, cum_reward: -3546.8994697153316\n",
      "Episode 14, loss: 0.6800054311752319, iterations: 7000, cum_reward: -3415.624565614896\n",
      "Episode 15, loss: 4.521424770355225, iterations: 7500, cum_reward: -3260.6409356557806\n",
      "Episode 16, loss: 2.5136075019836426, iterations: 8000, cum_reward: -3010.677370983622\n",
      "Episode 17, loss: 3.0447678565979004, iterations: 8500, cum_reward: -3570.3597215826007\n",
      "Episode 18, loss: 3.0201752185821533, iterations: 9000, cum_reward: -3430.1993909701982\n",
      "Episode 19, loss: 0.4834771752357483, iterations: 9500, cum_reward: -2880.0211642143445\n",
      "Episode 20, loss: 1.1577327251434326, iterations: 10000, cum_reward: -2625.7554653407387\n",
      "Episode 21, loss: 3.698561191558838, iterations: 10500, cum_reward: -3551.9176675860567\n",
      "Episode 22, loss: 1.2541465759277344, iterations: 11000, cum_reward: -3440.4553030509837\n",
      "Episode 23, loss: 5.1000871658325195, iterations: 11500, cum_reward: -3579.892655613801\n",
      "Episode 24, loss: 1.4569216966629028, iterations: 12000, cum_reward: -2751.16042283292\n",
      "Episode 25, loss: 1.455052375793457, iterations: 12500, cum_reward: -3895.3219261436625\n",
      "Episode 26, loss: 2.998814344406128, iterations: 13000, cum_reward: -3772.899806105604\n",
      "Episode 27, loss: 0.40998727083206177, iterations: 13500, cum_reward: -2237.3754311435237\n",
      "Episode 28, loss: 2.868765354156494, iterations: 14000, cum_reward: -2645.0941245305103\n",
      "Episode 29, loss: 5.575676441192627, iterations: 14500, cum_reward: -3622.2806866603974\n",
      "Episode 30, loss: 2.62553334236145, iterations: 15000, cum_reward: -2562.503808592295\n",
      "Episode 31, loss: 2.4606542587280273, iterations: 15500, cum_reward: -2459.359296769097\n",
      "Episode 32, loss: 11.94658374786377, iterations: 16000, cum_reward: -2941.406361597274\n",
      "Episode 33, loss: 2.903347969055176, iterations: 16500, cum_reward: -3136.21861660863\n",
      "Episode 34, loss: 4.714062213897705, iterations: 17000, cum_reward: -3227.806849719669\n",
      "Episode 35, loss: 2.1570920944213867, iterations: 17500, cum_reward: -2681.547075372798\n",
      "Episode 36, loss: 3.604602336883545, iterations: 18000, cum_reward: -2546.6173210263973\n",
      "Episode 37, loss: 2.2246646881103516, iterations: 18500, cum_reward: -2861.8268593162807\n",
      "Episode 38, loss: 0.7490392327308655, iterations: 19000, cum_reward: -2682.0684848768788\n",
      "Episode 39, loss: 3.154998540878296, iterations: 19500, cum_reward: -2408.7899832239386\n",
      "Episode 40, loss: 2.204857349395752, iterations: 20000, cum_reward: -2500.6797055633233\n",
      "Episode 41, loss: 4.226768493652344, iterations: 20500, cum_reward: -2676.0362109264515\n",
      "Episode 42, loss: 2.4034628868103027, iterations: 21000, cum_reward: -2382.515002980494\n",
      "Episode 43, loss: 1.8081378936767578, iterations: 21500, cum_reward: -2631.430646527471\n",
      "Episode 44, loss: 1.3805925846099854, iterations: 22000, cum_reward: -2413.440750767822\n",
      "Episode 45, loss: 2.6326024532318115, iterations: 22500, cum_reward: -2161.778615893926\n",
      "Episode 46, loss: 1.435809850692749, iterations: 23000, cum_reward: -2444.7825868560194\n",
      "Episode 47, loss: 4.3711137771606445, iterations: 23500, cum_reward: -2375.4954993939596\n",
      "Episode 48, loss: 2.2232465744018555, iterations: 24000, cum_reward: -2836.1603159872857\n",
      "Episode 49, loss: 4.250078201293945, iterations: 24500, cum_reward: -2376.5232251143952\n",
      "Episode 50, loss: 4.176329612731934, iterations: 25000, cum_reward: -2443.925952348885\n",
      "Episode 51, loss: 3.5459530353546143, iterations: 25500, cum_reward: -2818.7269201797994\n",
      "Episode 52, loss: 2.995255708694458, iterations: 26000, cum_reward: -2167.435583912081\n",
      "Episode 53, loss: 3.8192272186279297, iterations: 26500, cum_reward: -2246.7271742112725\n",
      "Episode 54, loss: 2.515751361846924, iterations: 27000, cum_reward: -2619.5474496148827\n",
      "Episode 55, loss: 2.479463577270508, iterations: 27500, cum_reward: -2334.7953437862075\n",
      "Episode 56, loss: 2.715804100036621, iterations: 28000, cum_reward: -2256.2245605713993\n",
      "Episode 57, loss: 1.485582709312439, iterations: 28500, cum_reward: -1972.8334734430414\n",
      "Episode 58, loss: 1.972698450088501, iterations: 29000, cum_reward: -2135.9790579918686\n",
      "Episode 59, loss: 5.349928379058838, iterations: 29500, cum_reward: -2457.7540932867237\n",
      "Episode 60, loss: 3.8496060371398926, iterations: 30000, cum_reward: -2253.4301260488496\n",
      "Episode 61, loss: 4.674644470214844, iterations: 30500, cum_reward: -2143.887391096514\n",
      "Episode 62, loss: 2.4973716735839844, iterations: 31000, cum_reward: -2670.382790444582\n",
      "Episode 63, loss: 2.6194088459014893, iterations: 31500, cum_reward: -2049.7079439428044\n",
      "Episode 64, loss: 2.6957461833953857, iterations: 32000, cum_reward: -2116.4299691882843\n",
      "Episode 65, loss: 2.9018335342407227, iterations: 32500, cum_reward: -2806.9165603024326\n",
      "Episode 66, loss: 2.6246156692504883, iterations: 33000, cum_reward: -2366.321918344899\n",
      "Episode 67, loss: 4.787260055541992, iterations: 33500, cum_reward: -2414.811836052966\n",
      "Episode 68, loss: 2.640043258666992, iterations: 34000, cum_reward: -2066.8825990040277\n",
      "Episode 69, loss: 3.284933090209961, iterations: 34500, cum_reward: -2140.0076193671207\n",
      "Episode 70, loss: 2.1024255752563477, iterations: 35000, cum_reward: -2299.17819916775\n",
      "Episode 71, loss: 4.299986839294434, iterations: 35500, cum_reward: -2341.9664588296205\n",
      "Episode 72, loss: 1.8653275966644287, iterations: 36000, cum_reward: -2552.652742470783\n",
      "Episode 73, loss: 1.9576187133789062, iterations: 36500, cum_reward: -2357.845252529026\n",
      "Episode 74, loss: 2.0933823585510254, iterations: 37000, cum_reward: -2342.29185224249\n",
      "Episode 75, loss: 2.0539908409118652, iterations: 37500, cum_reward: -2475.2812116585183\n",
      "Episode 76, loss: 2.341280937194824, iterations: 38000, cum_reward: -2216.842023772368\n",
      "Episode 77, loss: 2.599242925643921, iterations: 38500, cum_reward: -2336.8913610150644\n",
      "Episode 78, loss: 1.6698007583618164, iterations: 39000, cum_reward: -2323.0596150061206\n",
      "Episode 79, loss: 1.2118841409683228, iterations: 39500, cum_reward: -2283.6329116064067\n",
      "Episode 80, loss: 1.530326247215271, iterations: 40000, cum_reward: -2447.9458004884655\n",
      "Episode 81, loss: 1.3754444122314453, iterations: 40500, cum_reward: -2076.1968559547036\n",
      "Episode 82, loss: 2.362271308898926, iterations: 41000, cum_reward: -2021.6111651361796\n",
      "Episode 83, loss: 1.0914273262023926, iterations: 41500, cum_reward: -2092.5842498360744\n",
      "Episode 84, loss: 2.1014864444732666, iterations: 42000, cum_reward: -1964.275387120015\n",
      "Episode 85, loss: 3.982950210571289, iterations: 42500, cum_reward: -2130.5926159274336\n",
      "Episode 86, loss: 4.087725639343262, iterations: 43000, cum_reward: -2034.2191960450552\n",
      "Episode 88, loss: 2.559394359588623, iterations: 44000, cum_reward: -2185.989403565382\n",
      "Episode 89, loss: 2.2151269912719727, iterations: 44500, cum_reward: -2158.9678784984535\n",
      "Episode 90, loss: 3.807793140411377, iterations: 45000, cum_reward: -2407.3447857816477\n",
      "Episode 91, loss: 1.9549707174301147, iterations: 45500, cum_reward: -2166.5646119558965\n",
      "Episode 92, loss: 2.684932231903076, iterations: 46000, cum_reward: -2061.082990326823\n",
      "Episode 93, loss: 2.949678659439087, iterations: 46500, cum_reward: -2117.010140567622\n",
      "Episode 94, loss: 2.744708776473999, iterations: 47000, cum_reward: -2070.1994639640343\n",
      "Episode 95, loss: 3.3722429275512695, iterations: 47500, cum_reward: -2184.1168482655153\n",
      "Episode 96, loss: 3.411087989807129, iterations: 48000, cum_reward: -1983.8695551435985\n",
      "Episode 97, loss: 1.4844963550567627, iterations: 48500, cum_reward: -1998.8726646073442\n",
      "Episode 98, loss: 3.302326202392578, iterations: 49000, cum_reward: -2011.1213661184083\n",
      "Episode 99, loss: 2.869945526123047, iterations: 49500, cum_reward: -2188.800710654961\n",
      "Episode 100, loss: 3.7647900581359863, iterations: 50000, cum_reward: -2205.4529274913143\n",
      "Episode 101, loss: 2.247894048690796, iterations: 50500, cum_reward: -1919.884418854612\n",
      "Episode 102, loss: 5.692755699157715, iterations: 51000, cum_reward: -2033.2131390193333\n",
      "Episode 103, loss: 1.1517165899276733, iterations: 51500, cum_reward: -1978.7519948500212\n",
      "Episode 104, loss: 5.780072212219238, iterations: 52000, cum_reward: -2051.9013982037623\n",
      "Episode 105, loss: 2.619363307952881, iterations: 52500, cum_reward: -1851.1344720258726\n",
      "Episode 106, loss: 3.0512940883636475, iterations: 53000, cum_reward: -1993.4521478929069\n",
      "Episode 107, loss: 3.0349714756011963, iterations: 53500, cum_reward: -2080.0719137836318\n",
      "Episode 108, loss: 3.3164713382720947, iterations: 54000, cum_reward: -1942.4239322735114\n",
      "Episode 109, loss: 2.7711942195892334, iterations: 54500, cum_reward: -2093.099046064986\n",
      "Episode 110, loss: 4.299731254577637, iterations: 55000, cum_reward: -2095.879513773806\n",
      "Episode 111, loss: 1.2638343572616577, iterations: 55500, cum_reward: -1994.6496262667217\n",
      "Episode 112, loss: 2.1137313842773438, iterations: 56000, cum_reward: -2011.7810934515335\n",
      "Episode 113, loss: 3.7667715549468994, iterations: 56500, cum_reward: -2054.0729937069596\n",
      "Episode 114, loss: 2.4440746307373047, iterations: 57000, cum_reward: -1993.7247499445368\n",
      "Episode 115, loss: 2.858367919921875, iterations: 57500, cum_reward: -2226.38346577892\n",
      "Episode 116, loss: 3.957697868347168, iterations: 58000, cum_reward: -2088.1650310937816\n",
      "Episode 117, loss: 4.43287467956543, iterations: 58500, cum_reward: -2073.930128803949\n",
      "Episode 118, loss: 3.4018516540527344, iterations: 59000, cum_reward: -2083.0862127334717\n",
      "Episode 119, loss: 1.5157074928283691, iterations: 59500, cum_reward: -2004.6554017023172\n",
      "Episode 120, loss: 2.641543388366699, iterations: 60000, cum_reward: -2054.892606993257\n",
      "Episode 121, loss: 3.1498219966888428, iterations: 60500, cum_reward: -2200.0950189775244\n",
      "Episode 122, loss: 1.9617892503738403, iterations: 61000, cum_reward: -2012.4189697078264\n",
      "Episode 123, loss: 0.7164351940155029, iterations: 61500, cum_reward: -2090.3441963263813\n",
      "Episode 124, loss: 2.7018895149230957, iterations: 62000, cum_reward: -2040.2719268675648\n",
      "Episode 125, loss: 1.476014256477356, iterations: 62500, cum_reward: -2065.131013812124\n",
      "Episode 126, loss: 0.4274560213088989, iterations: 63000, cum_reward: -1905.0019864245116\n",
      "Episode 127, loss: 2.7181549072265625, iterations: 63500, cum_reward: -2097.5054313521177\n",
      "Episode 128, loss: 5.077890396118164, iterations: 64000, cum_reward: -1986.6612872339736\n",
      "Episode 129, loss: 0.9233876466751099, iterations: 64500, cum_reward: -1997.4592016501422\n",
      "Episode 130, loss: 1.963322401046753, iterations: 65000, cum_reward: -1945.9791424647208\n",
      "Episode 131, loss: 1.403586745262146, iterations: 65500, cum_reward: -2096.6012890191682\n",
      "Episode 132, loss: 5.579516410827637, iterations: 66000, cum_reward: -2126.3553508134178\n",
      "Episode 133, loss: 2.6533758640289307, iterations: 66500, cum_reward: -2026.139874517569\n",
      "Episode 134, loss: 2.1442041397094727, iterations: 67000, cum_reward: -2144.681739900517\n",
      "Episode 135, loss: 2.109395980834961, iterations: 67500, cum_reward: -2087.6504103186044\n",
      "Episode 136, loss: 3.018524169921875, iterations: 68000, cum_reward: -2049.492465224649\n",
      "Episode 137, loss: 0.7399075627326965, iterations: 68500, cum_reward: -2082.056143090502\n",
      "Episode 138, loss: 1.5170537233352661, iterations: 69000, cum_reward: -1938.1075549222928\n",
      "Episode 139, loss: 1.9113435745239258, iterations: 69500, cum_reward: -2047.8540467765097\n",
      "Episode 140, loss: 1.399169683456421, iterations: 70000, cum_reward: -2051.321251529146\n",
      "Episode 141, loss: 2.016834259033203, iterations: 70500, cum_reward: -2069.4666238913414\n",
      "Episode 142, loss: 2.8762426376342773, iterations: 71000, cum_reward: -1980.3386873500722\n",
      "Episode 143, loss: 0.9757469892501831, iterations: 71500, cum_reward: -1999.503470343847\n",
      "Episode 144, loss: 2.342435836791992, iterations: 72000, cum_reward: -2029.0181398993443\n",
      "Episode 145, loss: 2.4608101844787598, iterations: 72500, cum_reward: -2139.139815245304\n",
      "Episode 146, loss: 1.1601526737213135, iterations: 73000, cum_reward: -2087.374593128789\n",
      "Episode 147, loss: 1.9568305015563965, iterations: 73500, cum_reward: -2086.0027501578934\n",
      "Episode 148, loss: 2.726670265197754, iterations: 74000, cum_reward: -2212.639506366683\n",
      "Episode 149, loss: 1.6306425333023071, iterations: 74500, cum_reward: -2158.098935117023\n",
      "Episode 150, loss: 2.739670753479004, iterations: 75000, cum_reward: -2114.2755759463944\n",
      "Episode 151, loss: 0.8701717853546143, iterations: 75500, cum_reward: -2111.565077838534\n",
      "Episode 152, loss: 1.3847752809524536, iterations: 76000, cum_reward: -2074.6894385451037\n",
      "Episode 153, loss: 2.790067672729492, iterations: 76500, cum_reward: -2217.1247809484657\n",
      "Episode 154, loss: 2.0268287658691406, iterations: 77000, cum_reward: -2008.7557350377324\n",
      "Episode 155, loss: 2.1600112915039062, iterations: 77500, cum_reward: -2111.623517305108\n",
      "Episode 156, loss: 3.5305867195129395, iterations: 78000, cum_reward: -2185.1133935237694\n",
      "Episode 157, loss: 3.425800085067749, iterations: 78500, cum_reward: -2117.037999236524\n",
      "Episode 158, loss: 4.557084083557129, iterations: 79000, cum_reward: -2102.5997622848367\n",
      "Episode 159, loss: 1.5462483167648315, iterations: 79500, cum_reward: -2038.140094615522\n",
      "Episode 160, loss: 2.094667434692383, iterations: 80000, cum_reward: -1913.001149822323\n",
      "Episode 161, loss: 2.142599105834961, iterations: 80500, cum_reward: -2129.924296476007\n",
      "Episode 162, loss: 1.573240041732788, iterations: 81000, cum_reward: -2017.8658559375324\n",
      "Episode 163, loss: 0.7012104392051697, iterations: 81500, cum_reward: -2083.039227826248\n",
      "Episode 164, loss: 2.6421055793762207, iterations: 82000, cum_reward: -2184.4248167425512\n",
      "Episode 165, loss: 4.279463768005371, iterations: 82500, cum_reward: -2152.616429255246\n",
      "Episode 166, loss: 0.6318649053573608, iterations: 83000, cum_reward: -2027.762956409869\n",
      "Episode 167, loss: 2.8846607208251953, iterations: 83500, cum_reward: -2088.523631054676\n",
      "Episode 168, loss: 2.404571533203125, iterations: 84000, cum_reward: -2061.533880152661\n",
      "Episode 169, loss: 2.6857261657714844, iterations: 84500, cum_reward: -2142.85919475499\n",
      "Episode 170, loss: 3.799314260482788, iterations: 85000, cum_reward: -1919.7740452285052\n",
      "Episode 171, loss: 5.647900581359863, iterations: 85500, cum_reward: -1991.4122380729327\n",
      "Episode 172, loss: 2.058828353881836, iterations: 86000, cum_reward: -1964.4958878454229\n",
      "Episode 173, loss: 1.8877296447753906, iterations: 86500, cum_reward: -2125.5345814344555\n",
      "Episode 174, loss: 3.260695695877075, iterations: 87000, cum_reward: -2100.1454389896444\n",
      "Episode 175, loss: 4.251448631286621, iterations: 87500, cum_reward: -2060.573220729267\n",
      "Episode 176, loss: 2.8201990127563477, iterations: 88000, cum_reward: -2317.9469836286344\n",
      "Episode 177, loss: 2.485231399536133, iterations: 88500, cum_reward: -2152.028799797885\n",
      "Episode 178, loss: 1.9673917293548584, iterations: 89000, cum_reward: -2052.440812439679\n",
      "Episode 179, loss: 0.9444558620452881, iterations: 89500, cum_reward: -2064.8187611852572\n",
      "Episode 180, loss: 3.9675087928771973, iterations: 90000, cum_reward: -2068.8375887611064\n",
      "Episode 181, loss: 2.3067283630371094, iterations: 90500, cum_reward: -2066.0501077012086\n",
      "Episode 182, loss: 4.321474075317383, iterations: 91000, cum_reward: -2128.364060070407\n",
      "Episode 183, loss: 1.3473482131958008, iterations: 91500, cum_reward: -2029.479842657475\n",
      "Episode 184, loss: 3.1458587646484375, iterations: 92000, cum_reward: -2053.605222058705\n",
      "Episode 185, loss: 1.7595782279968262, iterations: 92500, cum_reward: -2056.3322884048735\n",
      "Episode 186, loss: 0.9138287901878357, iterations: 93000, cum_reward: -2114.6907624987434\n",
      "Episode 187, loss: 2.045703887939453, iterations: 93500, cum_reward: -2281.0997219021638\n",
      "Episode 188, loss: 2.472437620162964, iterations: 94000, cum_reward: -2084.3753525047955\n",
      "Episode 189, loss: 2.0181660652160645, iterations: 94500, cum_reward: -2181.0236366043882\n",
      "Episode 190, loss: 2.6320674419403076, iterations: 95000, cum_reward: -2156.152460354949\n",
      "Episode 191, loss: 2.815702438354492, iterations: 95500, cum_reward: -2072.090239304524\n",
      "Episode 192, loss: 1.5971763134002686, iterations: 96000, cum_reward: -2155.999727228927\n",
      "Episode 193, loss: 1.3298683166503906, iterations: 96500, cum_reward: -2051.6579223167496\n",
      "Episode 194, loss: 1.445648193359375, iterations: 97000, cum_reward: -2042.1538555595976\n",
      "Episode 195, loss: 2.0416154861450195, iterations: 97500, cum_reward: -2068.9437878768204\n",
      "Episode 196, loss: 1.2297029495239258, iterations: 98000, cum_reward: -2009.983292277709\n",
      "Episode 197, loss: 2.61812686920166, iterations: 98500, cum_reward: -2190.3618783487173\n",
      "Episode 198, loss: 1.9725017547607422, iterations: 99000, cum_reward: -1990.6798398731125\n",
      "Episode 199, loss: 3.757129669189453, iterations: 99500, cum_reward: -2101.769580854036\n",
      "Episode 200, loss: 2.6772403717041016, iterations: 100000, cum_reward: -2115.7348241115506\n",
      "Episode 201, loss: 3.216672420501709, iterations: 100500, cum_reward: -1996.617492046567\n",
      "Episode 202, loss: 2.616145133972168, iterations: 101000, cum_reward: -2206.309967376421\n",
      "Episode 203, loss: 2.3285698890686035, iterations: 101500, cum_reward: -2030.671600408166\n",
      "Episode 204, loss: 1.1039564609527588, iterations: 102000, cum_reward: -2196.2709241841067\n",
      "Episode 205, loss: 1.7932623624801636, iterations: 102500, cum_reward: -2120.1280851531487\n",
      "Episode 206, loss: 2.3828248977661133, iterations: 103000, cum_reward: -2116.3570274155145\n",
      "Episode 207, loss: 5.150857925415039, iterations: 103500, cum_reward: -2075.0790569841583\n",
      "Episode 208, loss: 3.3754382133483887, iterations: 104000, cum_reward: -2171.2769255217536\n",
      "Episode 209, loss: 1.9052083492279053, iterations: 104500, cum_reward: -2178.580818220715\n",
      "Episode 210, loss: 2.1225366592407227, iterations: 105000, cum_reward: -2186.269857952225\n",
      "Episode 211, loss: 2.483067512512207, iterations: 105500, cum_reward: -2184.9048487580126\n",
      "Episode 212, loss: 2.5004477500915527, iterations: 106000, cum_reward: -2224.358426987857\n",
      "Episode 213, loss: 3.6967239379882812, iterations: 106500, cum_reward: -2073.975161439112\n",
      "Episode 214, loss: 0.8441264033317566, iterations: 107000, cum_reward: -2135.6934308017776\n",
      "Episode 215, loss: 2.9440488815307617, iterations: 107500, cum_reward: -2075.376015806106\n",
      "Episode 216, loss: 2.428870916366577, iterations: 108000, cum_reward: -2051.2808151223203\n",
      "Episode 217, loss: 2.567298412322998, iterations: 108500, cum_reward: -2209.8759380223464\n",
      "Episode 218, loss: 2.0884857177734375, iterations: 109000, cum_reward: -2306.1267076403988\n",
      "Episode 219, loss: 2.238260269165039, iterations: 109500, cum_reward: -2111.747546675212\n",
      "Episode 220, loss: 3.3995888233184814, iterations: 110000, cum_reward: -2222.3259293363612\n",
      "Episode 221, loss: 2.3431825637817383, iterations: 110500, cum_reward: -2062.4519484606276\n",
      "Episode 222, loss: 2.988081932067871, iterations: 111000, cum_reward: -2352.9097824676714\n",
      "Episode 223, loss: 3.1144609451293945, iterations: 111500, cum_reward: -2036.0251036000448\n",
      "Episode 224, loss: 2.2659108638763428, iterations: 112000, cum_reward: -2144.654180725665\n",
      "Episode 226, loss: 1.554245948791504, iterations: 113000, cum_reward: -2251.7646182236726\n",
      "Episode 227, loss: 1.5339733362197876, iterations: 113500, cum_reward: -2204.777103718799\n",
      "Episode 228, loss: 4.491335868835449, iterations: 114000, cum_reward: -2191.9534726357883\n",
      "Episode 229, loss: 2.823019027709961, iterations: 114500, cum_reward: -2077.06427722762\n",
      "Episode 230, loss: 2.217639923095703, iterations: 115000, cum_reward: -2059.386817961603\n",
      "Episode 231, loss: 1.8139064311981201, iterations: 115500, cum_reward: -2277.268833745564\n",
      "Episode 232, loss: 1.784224033355713, iterations: 116000, cum_reward: -2202.1867248970043\n",
      "Episode 233, loss: 2.496148109436035, iterations: 116500, cum_reward: -2123.135326429381\n",
      "Episode 234, loss: 1.6550298929214478, iterations: 117000, cum_reward: -2342.4536726703154\n",
      "Episode 235, loss: 4.163600921630859, iterations: 117500, cum_reward: -2204.8223971044054\n",
      "Episode 236, loss: 2.0628366470336914, iterations: 118000, cum_reward: -2357.543109083143\n",
      "Episode 237, loss: 0.3767976462841034, iterations: 118500, cum_reward: -1985.8928041217516\n",
      "Episode 238, loss: 3.498077154159546, iterations: 119000, cum_reward: -2184.228734474309\n",
      "Episode 239, loss: 3.3418455123901367, iterations: 119500, cum_reward: -2257.211425782818\n",
      "Episode 240, loss: 3.3188257217407227, iterations: 120000, cum_reward: -2185.8019224727263\n",
      "Episode 241, loss: 1.465084433555603, iterations: 120500, cum_reward: -2195.900600364171\n",
      "Episode 242, loss: 1.285239815711975, iterations: 121000, cum_reward: -2164.8504106748587\n",
      "Episode 243, loss: 3.6321401596069336, iterations: 121500, cum_reward: -2311.4974789855887\n",
      "Episode 244, loss: 1.8010004758834839, iterations: 122000, cum_reward: -2172.0163878872704\n",
      "Episode 245, loss: 2.627155303955078, iterations: 122500, cum_reward: -2317.6044034495703\n",
      "Episode 246, loss: 4.022336006164551, iterations: 123000, cum_reward: -2325.83832795725\n",
      "Episode 247, loss: 4.212828636169434, iterations: 123500, cum_reward: -2420.6118309533076\n",
      "Episode 248, loss: 1.0414143800735474, iterations: 124000, cum_reward: -2281.5229241961497\n",
      "Episode 249, loss: 2.5438461303710938, iterations: 124500, cum_reward: -2168.877482834595\n",
      "Episode 250, loss: 2.8871922492980957, iterations: 125000, cum_reward: -2159.4475281997593\n",
      "Episode 251, loss: 1.7594947814941406, iterations: 125500, cum_reward: -2069.0080783028975\n",
      "Episode 252, loss: 1.275558352470398, iterations: 126000, cum_reward: -2237.5545807829167\n",
      "Episode 253, loss: 2.811814785003662, iterations: 126500, cum_reward: -2190.3923004540447\n",
      "Episode 254, loss: 2.5403614044189453, iterations: 127000, cum_reward: -2084.9468040121087\n",
      "Episode 255, loss: 2.940584182739258, iterations: 127500, cum_reward: -2108.6128719279745\n",
      "Episode 256, loss: 3.1303863525390625, iterations: 128000, cum_reward: -2103.1840249513425\n",
      "Episode 257, loss: 1.5779327154159546, iterations: 128500, cum_reward: -2091.4419011525624\n",
      "Episode 258, loss: 2.914823055267334, iterations: 129000, cum_reward: -2096.8891940032527\n",
      "Episode 259, loss: 2.579333782196045, iterations: 129500, cum_reward: -2263.03846919346\n",
      "Episode 260, loss: 2.5364062786102295, iterations: 130000, cum_reward: -2098.4204303229894\n",
      "Episode 261, loss: 2.7086868286132812, iterations: 130500, cum_reward: -2027.0827695761159\n",
      "Episode 262, loss: 2.4021918773651123, iterations: 131000, cum_reward: -2219.8208559772334\n",
      "Episode 263, loss: 3.377143621444702, iterations: 131500, cum_reward: -2204.6816461116723\n",
      "Episode 264, loss: 1.1429587602615356, iterations: 132000, cum_reward: -2294.271318272089\n",
      "Episode 265, loss: 1.268812656402588, iterations: 132500, cum_reward: -2155.4732491034924\n",
      "Episode 266, loss: 1.9442591667175293, iterations: 133000, cum_reward: -2085.6074894645194\n",
      "Episode 267, loss: 3.0429916381835938, iterations: 133500, cum_reward: -2015.6081106397164\n",
      "Episode 268, loss: 0.4945041537284851, iterations: 134000, cum_reward: -2075.7162536673304\n",
      "Episode 269, loss: 3.305643081665039, iterations: 134500, cum_reward: -2325.0638708135984\n",
      "Episode 270, loss: 1.414193034172058, iterations: 135000, cum_reward: -2699.913922719478\n",
      "Episode 271, loss: 2.8776814937591553, iterations: 135500, cum_reward: -2180.2268050836183\n",
      "Episode 272, loss: 1.9192190170288086, iterations: 136000, cum_reward: -2165.4756250153996\n",
      "Episode 273, loss: 2.7040843963623047, iterations: 136500, cum_reward: -2179.8973175640076\n",
      "Episode 274, loss: 3.1463403701782227, iterations: 137000, cum_reward: -2241.2729398804036\n",
      "Episode 275, loss: 2.947108268737793, iterations: 137500, cum_reward: -2227.528267343767\n",
      "Episode 276, loss: 3.3840417861938477, iterations: 138000, cum_reward: -2055.0619911172366\n",
      "Episode 277, loss: 1.8214149475097656, iterations: 138500, cum_reward: -2129.5989640034545\n",
      "Episode 278, loss: 3.3500399589538574, iterations: 139000, cum_reward: -2157.134169027292\n",
      "Episode 279, loss: 0.5137513875961304, iterations: 139500, cum_reward: -2225.756601432597\n"
     ]
    }
   ],
   "source": [
    "train_helper.train(400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "env.reset()\n",
    "actions = [0] * 10\n",
    "for i in range(500):\n",
    "    t = Transition(*env.step(action))\n",
    "    t = samplizer_.process(t)\n",
    "    t = dqn_preprocess(t)\n",
    "    action = int(agent.predict(t.state))\n",
    "    actions[action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from status_render import StatusRender\n",
    "render = StatusRender(env)\n",
    "render.job_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CPU', 'cpu_power': 970.0372678541216, 'io_power': 713.6889755928189}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_server_info(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DqnNet(\n",
       "  (_conv_net): Sequential(\n",
       "    (convolution_0): Conv1d(1, 10, kernel_size=(3,), stride=(1,))\n",
       "    (conv_ReLU_0): ReLU()\n",
       "  )\n",
       "  (_fc_net): Sequential(\n",
       "    (fully_connected_0): Linear(in_features=100, out_features=32, bias=True)\n",
       "    (fc_activation_0): ReLU()\n",
       "    (fully_connected_1): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc_activation_1): ReLU()\n",
       "    (final): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       "  (_rnn_net): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_helper._agent._global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_helper._env.is_terminal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999890144805657"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._eps_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._eps_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = preprocessor(replay_memory.sample(1)[0]).state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999890144805657"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._eps_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2124)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.rand(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SchedulingEnv' object has no attribute 'actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ed12f4dcbcf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SchedulingEnv' object has no attribute 'actions'"
     ]
    }
   ],
   "source": [
    "env.ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998848773724686"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.pow(0.9, agent._eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(10, (1,)).view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
