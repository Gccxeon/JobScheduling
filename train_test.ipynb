{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initialization template is either None or invalid, the servers and jobs will be generated randomly\n"
     ]
    }
   ],
   "source": [
    "from env_test import env, replay_memory, collector, samplizer_\n",
    "from env_test import policies as baseline_policy\n",
    "from dqn_agent_test import agent\n",
    "from transition import Transition\n",
    "from preprocess_dqn import process_from_replay_sample as dqn_preprocess\n",
    "import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_helper = trainer.Trainer(env, \n",
    "                               agent, \n",
    "                               collector, \n",
    "                               replay_memory,  \n",
    "                               samplizer_, \n",
    "                               Transition, \n",
    "                               10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1, loss: 4.623970, iterations: 222994, cum_reward: -2481.736978\n",
      "Episode  2, loss: 3.813508, iterations: 223494, cum_reward: -2459.528719\n",
      "Episode  3, loss: 4.520837, iterations: 223994, cum_reward: -2467.306643\n",
      "Episode  4, loss: 4.176261, iterations: 224494, cum_reward: -2485.352963\n",
      "Episode  5, loss: 4.470107, iterations: 224994, cum_reward: -2473.708820\n",
      "Episode  6, loss: 4.041163, iterations: 225494, cum_reward: -2471.559417\n",
      "Episode  7, loss: 4.139332, iterations: 225994, cum_reward: -2486.567438\n",
      "Episode  8, loss: 4.656955, iterations: 226494, cum_reward: -2479.473928\n",
      "Episode  9, loss: 3.081053, iterations: 226994, cum_reward: -2477.138554\n",
      "Episode 10, loss: 4.174129, iterations: 227494, cum_reward: -2471.118558\n",
      "Episode 11, loss: 3.926579, iterations: 227994, cum_reward: -2475.103093\n",
      "Episode 12, loss: 4.470034, iterations: 228494, cum_reward: -2479.859206\n",
      "Episode 13, loss: 3.925013, iterations: 228994, cum_reward: -2473.670938\n",
      "Episode 14, loss: 4.309769, iterations: 229494, cum_reward: -2481.110267\n",
      "Episode 15, loss: 4.400422, iterations: 229994, cum_reward: -2484.542512\n",
      "Episode 16, loss: 4.203813, iterations: 230494, cum_reward: -2484.196537\n",
      "Episode 17, loss: 3.367078, iterations: 230994, cum_reward: -2462.187752\n",
      "Episode 18, loss: 4.251854, iterations: 231494, cum_reward: -2467.095615\n",
      "Episode 19, loss: 4.539281, iterations: 231994, cum_reward: -2473.208927\n",
      "Episode 20, loss: 4.608294, iterations: 232494, cum_reward: -2471.327982\n",
      "Episode 21, loss: 4.287713, iterations: 232994, cum_reward: -2464.678495\n",
      "Episode 22, loss: 3.969789, iterations: 233494, cum_reward: -2474.974383\n",
      "Episode 23, loss: 4.357219, iterations: 233994, cum_reward: -2484.339742\n",
      "Episode 24, loss: 4.237274, iterations: 234494, cum_reward: -2479.592352\n",
      "Episode 25, loss: 4.493752, iterations: 234994, cum_reward: -2478.009091\n",
      "Episode 26, loss: 4.061224, iterations: 235494, cum_reward: -2471.976468\n",
      "Episode 27, loss: 4.179217, iterations: 235994, cum_reward: -2458.928945\n",
      "Episode 28, loss: 4.406439, iterations: 236494, cum_reward: -2474.329323\n",
      "Episode 29, loss: 4.625106, iterations: 236994, cum_reward: -2474.280283\n",
      "Episode 30, loss: 4.219688, iterations: 237494, cum_reward: -2475.050469\n",
      "Episode 31, loss: 3.057855, iterations: 237994, cum_reward: -2479.469096\n",
      "Episode 32, loss: 4.356677, iterations: 238494, cum_reward: -2473.058941\n",
      "Episode 33, loss: 3.763314, iterations: 238994, cum_reward: -2477.219635\n",
      "Episode 34, loss: 4.389304, iterations: 239494, cum_reward: -2470.220079\n",
      "Episode 35, loss: 4.278993, iterations: 239994, cum_reward: -2478.653847\n",
      "Episode 36, loss: 4.000565, iterations: 240494, cum_reward: -2476.128266\n",
      "Episode 37, loss: 3.858739, iterations: 240994, cum_reward: -2466.515807\n",
      "Episode 38, loss: 3.959126, iterations: 241494, cum_reward: -2465.047781\n",
      "Episode 39, loss: 4.496725, iterations: 241994, cum_reward: -2474.616146\n",
      "Episode 40, loss: 4.546841, iterations: 242494, cum_reward: -2476.617921\n",
      "Episode 41, loss: 3.271188, iterations: 242994, cum_reward: -2483.459044\n",
      "Episode 42, loss: 4.305395, iterations: 243494, cum_reward: -2467.726458\n",
      "Episode 43, loss: 4.038100, iterations: 243994, cum_reward: -2457.871438\n",
      "Episode 44, loss: 3.400302, iterations: 244494, cum_reward: -2477.140602\n",
      "Episode 45, loss: 4.402319, iterations: 244994, cum_reward: -2471.416824\n",
      "Episode 46, loss: 4.410946, iterations: 245494, cum_reward: -2485.656620\n",
      "Episode 47, loss: 4.268625, iterations: 245994, cum_reward: -2472.566086\n",
      "Episode 48, loss: 4.307927, iterations: 246494, cum_reward: -2473.438137\n",
      "Episode 49, loss: 4.646191, iterations: 246994, cum_reward: -2486.555174\n",
      "Episode 50, loss: 4.316534, iterations: 247494, cum_reward: -2476.025968\n",
      "Episode 51, loss: 3.791123, iterations: 247994, cum_reward: -2464.873526\n",
      "Episode 52, loss: 4.366673, iterations: 248494, cum_reward: -2461.537305\n",
      "Episode 53, loss: 4.067764, iterations: 248994, cum_reward: -2469.939164\n",
      "Episode 54, loss: 4.639788, iterations: 249494, cum_reward: -2468.003006\n",
      "Episode 55, loss: 3.450147, iterations: 249994, cum_reward: -2471.400693\n",
      "Episode 56, loss: 4.042392, iterations: 250494, cum_reward: -2473.446911\n",
      "Episode 57, loss: 4.488357, iterations: 250994, cum_reward: -2483.595178\n",
      "Episode 58, loss: 4.382864, iterations: 251494, cum_reward: -2478.354450\n",
      "Episode 59, loss: 3.919985, iterations: 251994, cum_reward: -2478.442027\n",
      "Episode 60, loss: 4.479271, iterations: 252494, cum_reward: -2480.888259\n",
      "Episode 61, loss: 3.156909, iterations: 252994, cum_reward: -2472.049705\n",
      "Episode 62, loss: 4.247856, iterations: 253494, cum_reward: -2466.192790\n",
      "Episode 63, loss: 4.671452, iterations: 253994, cum_reward: -2472.968900\n",
      "Episode 64, loss: 4.468119, iterations: 254494, cum_reward: -2477.522629\n",
      "Episode 65, loss: 4.735984, iterations: 254994, cum_reward: -2475.348614\n",
      "Episode 66, loss: 3.990307, iterations: 255494, cum_reward: -2468.260936\n",
      "Episode 67, loss: 4.291509, iterations: 255994, cum_reward: -2484.569586\n",
      "Episode 68, loss: 4.533118, iterations: 256494, cum_reward: -2477.814852\n",
      "Episode 69, loss: 3.740820, iterations: 256994, cum_reward: -2480.919090\n",
      "Episode 70, loss: 4.379528, iterations: 257494, cum_reward: -2475.847656\n",
      "Episode 71, loss: 4.527200, iterations: 257994, cum_reward: -2467.971976\n",
      "Episode 72, loss: 4.339677, iterations: 258494, cum_reward: -2476.759965\n",
      "Episode 73, loss: 4.274180, iterations: 258994, cum_reward: -2466.084359\n",
      "Episode 74, loss: 3.389563, iterations: 259494, cum_reward: -2462.650287\n",
      "Episode 75, loss: 4.572853, iterations: 259994, cum_reward: -2459.100139\n",
      "Episode 76, loss: 3.476944, iterations: 260494, cum_reward: -2485.250996\n",
      "Episode 77, loss: 3.940188, iterations: 260994, cum_reward: -2464.377706\n",
      "Episode 78, loss: 4.451256, iterations: 261494, cum_reward: -2468.891017\n",
      "Episode 79, loss: 4.467923, iterations: 261994, cum_reward: -2483.580980\n",
      "Episode 80, loss: 4.826836, iterations: 262494, cum_reward: -2481.422803\n",
      "Episode 81, loss: 3.636263, iterations: 262994, cum_reward: -2459.677749\n",
      "Episode 82, loss: 3.760684, iterations: 263494, cum_reward: -2490.545250\n",
      "Episode 83, loss: 4.492723, iterations: 263994, cum_reward: -2474.800543\n",
      "Episode 84, loss: 3.557153, iterations: 264494, cum_reward: -2476.498759\n",
      "Episode 85, loss: 3.384380, iterations: 264994, cum_reward: -2475.059085\n",
      "Episode 86, loss: 3.482709, iterations: 265494, cum_reward: -2478.952836\n",
      "Episode 87, loss: 4.403926, iterations: 265994, cum_reward: -2479.833166\n",
      "Episode 88, loss: 4.004764, iterations: 266494, cum_reward: -2470.402472\n",
      "Episode 89, loss: 4.024974, iterations: 266994, cum_reward: -2478.017359\n",
      "Episode 90, loss: 3.761703, iterations: 267494, cum_reward: -2474.992820\n",
      "Episode 91, loss: 4.286194, iterations: 267994, cum_reward: -2469.441801\n",
      "Episode 92, loss: 4.354616, iterations: 268494, cum_reward: -2468.329898\n",
      "Episode 93, loss: 3.637550, iterations: 268994, cum_reward: -2471.833503\n",
      "Episode 94, loss: 4.586220, iterations: 269494, cum_reward: -2465.521625\n",
      "Episode 95, loss: 3.769770, iterations: 269994, cum_reward: -2463.023972\n",
      "Episode 96, loss: 3.151245, iterations: 270494, cum_reward: -2475.631542\n",
      "Episode 97, loss: 3.951183, iterations: 270994, cum_reward: -2479.225226\n",
      "Episode 98, loss: 4.628192, iterations: 271494, cum_reward: -2472.359496\n",
      "Episode 99, loss: 4.448136, iterations: 271994, cum_reward: -2469.848175\n",
      "Episode 100, loss: 4.565284, iterations: 272494, cum_reward: -2463.303801\n",
      "Episode 101, loss: 4.474973, iterations: 272994, cum_reward: -2470.706227\n",
      "Episode 102, loss: 3.937755, iterations: 273494, cum_reward: -2478.401929\n",
      "Episode 103, loss: 4.292494, iterations: 273994, cum_reward: -2482.870896\n",
      "Episode 104, loss: 3.503905, iterations: 274494, cum_reward: -2479.480023\n",
      "Episode 105, loss: 4.601162, iterations: 274994, cum_reward: -2476.909751\n",
      "Episode 106, loss: 4.492984, iterations: 275494, cum_reward: -2469.809715\n",
      "Episode 107, loss: 4.223417, iterations: 275994, cum_reward: -2470.765585\n",
      "Episode 108, loss: 4.289021, iterations: 276494, cum_reward: -2474.853924\n",
      "Episode 109, loss: 4.853890, iterations: 276994, cum_reward: -2464.378101\n",
      "Episode 110, loss: 3.724926, iterations: 277494, cum_reward: -2480.462731\n",
      "Episode 111, loss: 3.998002, iterations: 277994, cum_reward: -2480.093971\n",
      "Episode 112, loss: 4.771129, iterations: 278494, cum_reward: -2487.952278\n",
      "Episode 113, loss: 4.049343, iterations: 278994, cum_reward: -2475.121278\n",
      "Episode 114, loss: 3.768501, iterations: 279494, cum_reward: -2474.402916\n",
      "Episode 115, loss: 4.538388, iterations: 279994, cum_reward: -2481.277767\n",
      "Episode 116, loss: 4.542378, iterations: 280494, cum_reward: -2487.240339\n",
      "Episode 117, loss: 3.932747, iterations: 280994, cum_reward: -2467.825234\n",
      "Episode 118, loss: 3.850521, iterations: 281494, cum_reward: -2480.777330\n",
      "Episode 119, loss: 3.127023, iterations: 281994, cum_reward: -2468.428690\n",
      "Episode 120, loss: 4.095633, iterations: 282494, cum_reward: -2489.583563\n",
      "Episode 121, loss: 4.597028, iterations: 282994, cum_reward: -2482.269545\n",
      "Episode 122, loss: 3.685614, iterations: 283494, cum_reward: -2474.028148\n",
      "Episode 123, loss: 4.492304, iterations: 283994, cum_reward: -2489.099709\n",
      "Episode 124, loss: 4.496113, iterations: 284494, cum_reward: -2478.616722\n",
      "Episode 125, loss: 4.372614, iterations: 284994, cum_reward: -2479.390552\n",
      "Episode 126, loss: 4.196438, iterations: 285494, cum_reward: -2459.334931\n",
      "Episode 127, loss: 3.737285, iterations: 285994, cum_reward: -2472.895545\n",
      "Episode 128, loss: 3.230080, iterations: 286494, cum_reward: -2483.848552\n",
      "Episode 129, loss: 4.543495, iterations: 286994, cum_reward: -2471.824510\n",
      "Episode 130, loss: 4.760798, iterations: 287494, cum_reward: -2478.034021\n",
      "Episode 131, loss: 4.374790, iterations: 287994, cum_reward: -2479.532582\n",
      "Episode 132, loss: 4.271694, iterations: 288494, cum_reward: -2473.071524\n",
      "Episode 133, loss: 4.215852, iterations: 288994, cum_reward: -2468.497242\n",
      "Episode 134, loss: 2.452687, iterations: 289494, cum_reward: -2468.863377\n",
      "Episode 135, loss: 4.557895, iterations: 289994, cum_reward: -2472.741004\n",
      "Episode 136, loss: 4.588678, iterations: 290494, cum_reward: -2465.445429\n",
      "Episode 137, loss: 4.464066, iterations: 290994, cum_reward: -2487.501171\n",
      "Episode 138, loss: 4.390628, iterations: 291494, cum_reward: -2470.635854\n",
      "Episode 139, loss: 4.089001, iterations: 291994, cum_reward: -2471.597863\n",
      "Episode 140, loss: 3.990285, iterations: 292494, cum_reward: -2476.219559\n",
      "Episode 141, loss: 3.872658, iterations: 292994, cum_reward: -2471.488426\n",
      "Episode 142, loss: 4.695236, iterations: 293494, cum_reward: -2469.792215\n",
      "Episode 143, loss: 4.464661, iterations: 293994, cum_reward: -2475.044186\n",
      "Episode 144, loss: 2.708378, iterations: 294494, cum_reward: -2482.233578\n",
      "Episode 145, loss: 3.790340, iterations: 294994, cum_reward: -2463.596569\n",
      "Episode 146, loss: 3.989824, iterations: 295494, cum_reward: -2474.374160\n",
      "Episode 147, loss: 4.262133, iterations: 295994, cum_reward: -2468.934396\n",
      "Episode 148, loss: 3.942581, iterations: 296494, cum_reward: -2475.154381\n",
      "Episode 149, loss: 3.336864, iterations: 296994, cum_reward: -2479.612877\n",
      "Episode 150, loss: 4.802721, iterations: 297494, cum_reward: -2475.489047\n",
      "Episode 151, loss: 3.800395, iterations: 297994, cum_reward: -2473.011428\n",
      "Episode 152, loss: 4.664310, iterations: 298494, cum_reward: -2482.504777\n",
      "Episode 153, loss: 4.141431, iterations: 298994, cum_reward: -2474.273203\n",
      "Episode 154, loss: 4.488414, iterations: 299494, cum_reward: -2479.823580\n",
      "Episode 155, loss: 2.531498, iterations: 299994, cum_reward: -2483.780800\n",
      "Episode 156, loss: 3.441277, iterations: 300494, cum_reward: -2487.201182\n",
      "Episode 157, loss: 4.200379, iterations: 300994, cum_reward: -2480.562390\n",
      "Episode 158, loss: 3.902443, iterations: 301494, cum_reward: -2480.789906\n",
      "Episode 159, loss: 4.232492, iterations: 301994, cum_reward: -2473.853717\n",
      "Episode 160, loss: 3.744301, iterations: 302494, cum_reward: -2470.614872\n",
      "Episode 161, loss: 4.285032, iterations: 302994, cum_reward: -2475.010579\n",
      "Episode 162, loss: 4.641145, iterations: 303494, cum_reward: -2474.393026\n",
      "Episode 163, loss: 4.419188, iterations: 303994, cum_reward: -2462.964434\n",
      "Episode 164, loss: 4.676084, iterations: 304494, cum_reward: -2487.904679\n",
      "Episode 165, loss: 4.152790, iterations: 304994, cum_reward: -2466.977064\n",
      "Episode 166, loss: 3.948416, iterations: 305494, cum_reward: -2475.335371\n",
      "Episode 167, loss: 3.969161, iterations: 305994, cum_reward: -2464.570609\n",
      "Episode 168, loss: 3.474689, iterations: 306494, cum_reward: -2474.366404\n",
      "Episode 169, loss: 4.577790, iterations: 306994, cum_reward: -2472.558801\n",
      "Episode 170, loss: 4.510360, iterations: 307494, cum_reward: -2472.359156\n",
      "Episode 171, loss: 4.182795, iterations: 307994, cum_reward: -2488.593119\n",
      "Episode 172, loss: 3.788666, iterations: 308494, cum_reward: -2472.257386\n",
      "Episode 173, loss: 2.959624, iterations: 308994, cum_reward: -2473.751290\n",
      "Episode 174, loss: 3.875252, iterations: 309494, cum_reward: -2471.149870\n",
      "Episode 175, loss: 4.626202, iterations: 309994, cum_reward: -2478.325091\n",
      "Episode 176, loss: 4.206976, iterations: 310494, cum_reward: -2482.762468\n",
      "Episode 177, loss: 4.532257, iterations: 310994, cum_reward: -2476.193618\n",
      "Episode 178, loss: 4.515313, iterations: 311494, cum_reward: -2466.087631\n",
      "Episode 179, loss: 4.082970, iterations: 311994, cum_reward: -2456.322553\n",
      "Episode 180, loss: 3.947711, iterations: 312494, cum_reward: -2484.023984\n",
      "Episode 181, loss: 3.980092, iterations: 312994, cum_reward: -2474.370522\n",
      "Episode 182, loss: 4.178719, iterations: 313494, cum_reward: -2478.317868\n",
      "Episode 183, loss: 3.928081, iterations: 313994, cum_reward: -2471.119468\n",
      "Episode 184, loss: 4.362824, iterations: 314494, cum_reward: -2471.371629\n",
      "Episode 185, loss: 3.406502, iterations: 314994, cum_reward: -2472.376308\n",
      "Episode 186, loss: 4.308080, iterations: 315494, cum_reward: -2468.156939\n",
      "Episode 187, loss: 4.456421, iterations: 315994, cum_reward: -2484.402367\n",
      "Episode 188, loss: 3.436312, iterations: 316494, cum_reward: -2469.817616\n",
      "Episode 189, loss: 4.777072, iterations: 316994, cum_reward: -2460.889532\n",
      "Episode 190, loss: 3.706798, iterations: 317494, cum_reward: -2479.691611\n",
      "Episode 191, loss: 4.621240, iterations: 317994, cum_reward: -2458.283479\n",
      "Episode 192, loss: 4.683417, iterations: 318494, cum_reward: -2475.689666\n",
      "Episode 193, loss: 4.335295, iterations: 318994, cum_reward: -2472.478610\n",
      "Episode 194, loss: 4.538556, iterations: 319494, cum_reward: -2487.137794\n",
      "Episode 195, loss: 4.382104, iterations: 319994, cum_reward: -2475.656808\n",
      "Episode 196, loss: 4.586368, iterations: 320494, cum_reward: -2466.448077\n",
      "Episode 197, loss: 4.046842, iterations: 320994, cum_reward: -2475.725533\n",
      "Episode 198, loss: 4.227953, iterations: 321494, cum_reward: -2476.808721\n",
      "Episode 199, loss: 3.987929, iterations: 321994, cum_reward: -2470.554027\n",
      "Episode 200, loss: 4.614076, iterations: 322494, cum_reward: -2480.538147\n",
      "Episode 201, loss: 4.749675, iterations: 322994, cum_reward: -2487.604514\n",
      "Episode 202, loss: 3.890066, iterations: 323494, cum_reward: -2464.342233\n",
      "Episode 203, loss: 4.083907, iterations: 323994, cum_reward: -2474.012348\n",
      "Episode 204, loss: 4.610689, iterations: 324494, cum_reward: -2459.327210\n",
      "Episode 205, loss: 4.547911, iterations: 324994, cum_reward: -2464.599014\n",
      "Episode 206, loss: 4.260077, iterations: 325494, cum_reward: -2471.534072\n",
      "Episode 207, loss: 3.178812, iterations: 325994, cum_reward: -2473.668861\n",
      "Episode 208, loss: 4.395607, iterations: 326494, cum_reward: -2461.594932\n",
      "Episode 209, loss: 4.367827, iterations: 326994, cum_reward: -2464.494918\n",
      "Episode 210, loss: 3.077462, iterations: 327494, cum_reward: -2481.314777\n",
      "Episode 211, loss: 3.667816, iterations: 327994, cum_reward: -2470.048070\n",
      "Episode 212, loss: 3.718237, iterations: 328494, cum_reward: -2472.802084\n",
      "Episode 213, loss: 4.311872, iterations: 328994, cum_reward: -2469.822678\n",
      "Episode 214, loss: 4.481611, iterations: 329494, cum_reward: -2485.300199\n",
      "Episode 215, loss: 3.396036, iterations: 329994, cum_reward: -2457.645930\n",
      "Episode 216, loss: 3.520370, iterations: 330494, cum_reward: -2461.286791\n",
      "Episode 217, loss: 3.742630, iterations: 330994, cum_reward: -2476.299983\n",
      "Episode 218, loss: 4.398277, iterations: 331494, cum_reward: -2468.078064\n",
      "Episode 219, loss: 3.620418, iterations: 331994, cum_reward: -2463.909387\n",
      "Episode 220, loss: 4.034532, iterations: 332494, cum_reward: -2480.196717\n",
      "Episode 221, loss: 4.690780, iterations: 332994, cum_reward: -2451.271321\n",
      "Episode 222, loss: 4.470062, iterations: 333494, cum_reward: -2472.580652\n",
      "Episode 223, loss: 4.252391, iterations: 333994, cum_reward: -2463.669210\n",
      "Episode 224, loss: 4.734669, iterations: 334494, cum_reward: -2476.795458\n",
      "Episode 225, loss: 3.662696, iterations: 334994, cum_reward: -2467.192130\n",
      "Episode 226, loss: 3.828742, iterations: 335494, cum_reward: -2477.208120\n",
      "Episode 227, loss: 3.958092, iterations: 335994, cum_reward: -2478.632915\n",
      "Episode 228, loss: 4.676306, iterations: 336494, cum_reward: -2466.460905\n",
      "Episode 229, loss: 3.976283, iterations: 336994, cum_reward: -2461.478258\n",
      "Episode 230, loss: 4.624062, iterations: 337494, cum_reward: -2479.320634\n",
      "Episode 231, loss: 4.401994, iterations: 337994, cum_reward: -2473.256110\n",
      "Episode 232, loss: 4.385109, iterations: 338494, cum_reward: -2463.193068\n",
      "Episode 233, loss: 3.945601, iterations: 338994, cum_reward: -2486.366135\n",
      "Episode 234, loss: 2.648051, iterations: 339494, cum_reward: -2465.086222\n",
      "Episode 235, loss: 4.151500, iterations: 339994, cum_reward: -2453.975381\n",
      "Episode 236, loss: 4.205945, iterations: 340494, cum_reward: -2474.010459\n",
      "Episode 237, loss: 4.459670, iterations: 340994, cum_reward: -2478.812961\n",
      "Episode 238, loss: 4.468629, iterations: 341494, cum_reward: -2479.614638\n",
      "Episode 239, loss: 4.043226, iterations: 341994, cum_reward: -2471.663631\n",
      "Episode 240, loss: 4.233150, iterations: 342494, cum_reward: -2471.820262\n",
      "Episode 241, loss: 4.404186, iterations: 342994, cum_reward: -2468.643784\n",
      "Episode 242, loss: 4.112648, iterations: 343494, cum_reward: -2484.348665\n",
      "Episode 243, loss: 4.369586, iterations: 343994, cum_reward: -2476.128697\n",
      "Episode 244, loss: 3.638134, iterations: 344494, cum_reward: -2469.075680\n",
      "Episode 245, loss: 3.365600, iterations: 344994, cum_reward: -2479.813373\n",
      "Episode 246, loss: 4.446849, iterations: 345494, cum_reward: -2482.212642\n",
      "Episode 247, loss: 4.536663, iterations: 345994, cum_reward: -2467.154608\n",
      "Episode 248, loss: 3.560687, iterations: 346494, cum_reward: -2476.765725\n",
      "Episode 249, loss: 4.442641, iterations: 346994, cum_reward: -2460.152372\n",
      "Episode 250, loss: 4.641304, iterations: 347494, cum_reward: -2472.101406\n",
      "Episode 251, loss: 3.710588, iterations: 347994, cum_reward: -2475.614305\n",
      "Episode 252, loss: 4.193558, iterations: 348494, cum_reward: -2473.310439\n",
      "Episode 253, loss: 3.257339, iterations: 348994, cum_reward: -2475.917721\n",
      "Episode 254, loss: 3.868484, iterations: 349494, cum_reward: -2486.036480\n",
      "Episode 255, loss: 4.515126, iterations: 349994, cum_reward: -2481.378604\n",
      "Episode 256, loss: 4.195774, iterations: 350494, cum_reward: -2479.101769\n",
      "Episode 257, loss: 4.564104, iterations: 350994, cum_reward: -2473.165837\n",
      "Episode 258, loss: 3.886114, iterations: 351494, cum_reward: -2475.796662\n",
      "Episode 259, loss: 4.518594, iterations: 351994, cum_reward: -2476.132483\n",
      "Episode 260, loss: 4.413424, iterations: 352494, cum_reward: -2470.994229\n",
      "Episode 261, loss: 3.641526, iterations: 352994, cum_reward: -2472.561389\n",
      "Episode 262, loss: 4.568850, iterations: 353494, cum_reward: -2477.303773\n",
      "Episode 263, loss: 4.375949, iterations: 353994, cum_reward: -2480.778938\n",
      "Episode 264, loss: 4.217784, iterations: 354494, cum_reward: -2481.837526\n",
      "Episode 265, loss: 3.839920, iterations: 354994, cum_reward: -2477.211736\n",
      "Episode 266, loss: 4.178094, iterations: 355494, cum_reward: -2470.913453\n",
      "Episode 267, loss: 4.418528, iterations: 355994, cum_reward: -2462.181011\n",
      "Episode 268, loss: 4.330360, iterations: 356494, cum_reward: -2461.809310\n",
      "Episode 269, loss: 4.403993, iterations: 356994, cum_reward: -2481.290051\n",
      "Episode 270, loss: 4.607407, iterations: 357494, cum_reward: -2485.141260\n",
      "Episode 271, loss: 4.139191, iterations: 357994, cum_reward: -2466.372107\n",
      "Episode 272, loss: 3.615641, iterations: 358494, cum_reward: -2484.633074\n",
      "Episode 273, loss: 4.359923, iterations: 358994, cum_reward: -2468.023694\n",
      "Episode 274, loss: 4.384461, iterations: 359494, cum_reward: -2480.395478\n",
      "Episode 275, loss: 4.442165, iterations: 359994, cum_reward: -2484.321687\n",
      "Episode 276, loss: 4.384046, iterations: 360494, cum_reward: -2472.704342\n",
      "Episode 277, loss: 4.576271, iterations: 360994, cum_reward: -2458.624714\n",
      "Episode 278, loss: 3.864583, iterations: 361494, cum_reward: -2459.608614\n",
      "Episode 279, loss: 3.869187, iterations: 361994, cum_reward: -2470.646574\n",
      "Episode 280, loss: 4.515385, iterations: 362494, cum_reward: -2484.123933\n",
      "Episode 281, loss: 3.863124, iterations: 362994, cum_reward: -2480.094317\n",
      "Episode 282, loss: 4.445998, iterations: 363494, cum_reward: -2469.793862\n",
      "Episode 283, loss: 4.450409, iterations: 363994, cum_reward: -2478.509288\n",
      "Episode 284, loss: 4.103374, iterations: 364494, cum_reward: -2469.080519\n",
      "Episode 285, loss: 4.515647, iterations: 364994, cum_reward: -2465.384619\n",
      "Episode 286, loss: 3.566675, iterations: 365494, cum_reward: -2464.974943\n",
      "Episode 287, loss: 4.326580, iterations: 365994, cum_reward: -2470.113835\n",
      "Episode 288, loss: 4.430300, iterations: 366494, cum_reward: -2484.309706\n",
      "Episode 289, loss: 4.511087, iterations: 366994, cum_reward: -2478.614387\n",
      "Episode 290, loss: 4.357062, iterations: 367494, cum_reward: -2480.719012\n",
      "Episode 291, loss: 4.328452, iterations: 367994, cum_reward: -2473.786052\n",
      "Episode 292, loss: 4.573716, iterations: 368494, cum_reward: -2482.410699\n",
      "Episode 293, loss: 3.420902, iterations: 368994, cum_reward: -2479.301316\n",
      "Episode 294, loss: 4.324710, iterations: 369494, cum_reward: -2473.696035\n",
      "Episode 295, loss: 4.143245, iterations: 369994, cum_reward: -2480.534515\n",
      "Episode 296, loss: 3.094256, iterations: 370494, cum_reward: -2491.457756\n",
      "Episode 297, loss: 4.683979, iterations: 370994, cum_reward: -2483.706012\n",
      "Episode 298, loss: 4.504797, iterations: 371494, cum_reward: -2468.114610\n",
      "Episode 299, loss: 4.258897, iterations: 371994, cum_reward: -2478.582384\n",
      "Episode 300, loss: 4.498840, iterations: 372494, cum_reward: -2482.999434\n",
      "Episode 301, loss: 4.513878, iterations: 372994, cum_reward: -2479.160675\n",
      "Episode 302, loss: 3.526977, iterations: 373494, cum_reward: -2476.029404\n",
      "Episode 303, loss: 4.311073, iterations: 373994, cum_reward: -2466.372683\n",
      "Episode 304, loss: 3.632604, iterations: 374494, cum_reward: -2467.745957\n",
      "Episode 305, loss: 4.650985, iterations: 374994, cum_reward: -2470.518737\n",
      "Episode 306, loss: 4.648755, iterations: 375494, cum_reward: -2463.854459\n",
      "Episode 307, loss: 3.944041, iterations: 375994, cum_reward: -2475.358723\n",
      "Episode 308, loss: 4.799414, iterations: 376494, cum_reward: -2464.065801\n",
      "Episode 309, loss: 3.381574, iterations: 376994, cum_reward: -2475.883527\n",
      "Episode 310, loss: 4.337466, iterations: 377494, cum_reward: -2469.787572\n",
      "Episode 311, loss: 4.018766, iterations: 377994, cum_reward: -2475.897541\n",
      "Episode 312, loss: 4.301492, iterations: 378494, cum_reward: -2478.820720\n",
      "Episode 313, loss: 3.690147, iterations: 378994, cum_reward: -2472.848472\n",
      "Episode 314, loss: 4.133293, iterations: 379494, cum_reward: -2482.900306\n",
      "Episode 315, loss: 4.168834, iterations: 379994, cum_reward: -2461.464654\n",
      "Episode 316, loss: 4.042700, iterations: 380494, cum_reward: -2467.872927\n",
      "Episode 317, loss: 4.305342, iterations: 380994, cum_reward: -2479.173700\n",
      "Episode 318, loss: 4.461845, iterations: 381494, cum_reward: -2464.190064\n",
      "Episode 319, loss: 3.987426, iterations: 381994, cum_reward: -2465.804856\n",
      "Episode 320, loss: 4.086002, iterations: 382494, cum_reward: -2485.780586\n",
      "Episode 321, loss: 3.723193, iterations: 382994, cum_reward: -2480.651580\n",
      "Episode 322, loss: 3.698763, iterations: 383494, cum_reward: -2468.264130\n",
      "Episode 323, loss: 4.523308, iterations: 383994, cum_reward: -2472.989927\n",
      "Episode 324, loss: 4.143661, iterations: 384494, cum_reward: -2479.968871\n",
      "Episode 325, loss: 3.524770, iterations: 384994, cum_reward: -2485.317547\n",
      "Episode 326, loss: 3.570868, iterations: 385494, cum_reward: -2480.641493\n",
      "Episode 327, loss: 4.475631, iterations: 385994, cum_reward: -2472.417915\n",
      "Episode 328, loss: 4.609457, iterations: 386494, cum_reward: -2476.364869\n",
      "Episode 329, loss: 3.967975, iterations: 386994, cum_reward: -2474.831819\n",
      "Episode 330, loss: 4.339680, iterations: 387494, cum_reward: -2470.932074\n",
      "Episode 331, loss: 3.504907, iterations: 387994, cum_reward: -2469.468686\n",
      "Episode 332, loss: 3.904313, iterations: 388494, cum_reward: -2471.884407\n",
      "Episode 333, loss: 4.265454, iterations: 388994, cum_reward: -2480.236020\n",
      "Episode 334, loss: 4.651446, iterations: 389494, cum_reward: -2480.522096\n",
      "Episode 335, loss: 4.324923, iterations: 389994, cum_reward: -2480.076622\n",
      "Episode 336, loss: 3.942638, iterations: 390494, cum_reward: -2473.895636\n",
      "Episode 337, loss: 4.370024, iterations: 390994, cum_reward: -2475.111402\n",
      "Episode 338, loss: 4.031209, iterations: 391494, cum_reward: -2476.773282\n",
      "Episode 339, loss: 4.563506, iterations: 391994, cum_reward: -2476.007373\n",
      "Episode 340, loss: 4.524026, iterations: 392494, cum_reward: -2473.449049\n",
      "Episode 341, loss: 3.902862, iterations: 392994, cum_reward: -2465.443306\n",
      "Episode 342, loss: 3.881902, iterations: 393494, cum_reward: -2472.501492\n",
      "Episode 343, loss: 3.771901, iterations: 393994, cum_reward: -2474.580937\n",
      "Episode 344, loss: 4.636144, iterations: 394494, cum_reward: -2462.810459\n",
      "Episode 345, loss: 4.470035, iterations: 394994, cum_reward: -2476.490829\n",
      "Episode 346, loss: 3.115076, iterations: 395494, cum_reward: -2478.762045\n",
      "Episode 347, loss: 4.453552, iterations: 395994, cum_reward: -2468.313869\n",
      "Episode 348, loss: 4.507116, iterations: 396494, cum_reward: -2478.277506\n",
      "Episode 349, loss: 4.633786, iterations: 396994, cum_reward: -2471.352007\n",
      "Episode 350, loss: 4.822207, iterations: 397494, cum_reward: -2479.831132\n",
      "Episode 351, loss: 4.151270, iterations: 397994, cum_reward: -2472.582191\n",
      "Episode 352, loss: 3.988383, iterations: 398494, cum_reward: -2473.559561\n",
      "Episode 353, loss: 4.387871, iterations: 398994, cum_reward: -2480.194393\n",
      "Episode 354, loss: 3.371152, iterations: 399494, cum_reward: -2474.255581\n",
      "Episode 355, loss: 4.705625, iterations: 399994, cum_reward: -2474.285535\n",
      "Episode 356, loss: 4.893957, iterations: 400494, cum_reward: -2489.716610\n",
      "Episode 357, loss: 4.433900, iterations: 400994, cum_reward: -2479.696388\n",
      "Episode 358, loss: 4.014438, iterations: 401494, cum_reward: -2462.325836\n",
      "Episode 359, loss: 4.684542, iterations: 401994, cum_reward: -2481.684206\n",
      "Episode 360, loss: 4.442706, iterations: 402494, cum_reward: -2475.369737\n",
      "Episode 361, loss: 4.732234, iterations: 402994, cum_reward: -2472.261069\n",
      "Episode 362, loss: 4.125110, iterations: 403494, cum_reward: -2472.408588\n",
      "Episode 363, loss: 3.564071, iterations: 403994, cum_reward: -2471.876627\n",
      "Episode 364, loss: 3.964551, iterations: 404494, cum_reward: -2467.183609\n",
      "Episode 365, loss: 4.746732, iterations: 404994, cum_reward: -2464.548558\n",
      "Episode 366, loss: 4.234569, iterations: 405494, cum_reward: -2487.216532\n",
      "Episode 367, loss: 4.057442, iterations: 405994, cum_reward: -2474.526786\n",
      "Episode 368, loss: 4.025436, iterations: 406494, cum_reward: -2481.462968\n",
      "Episode 369, loss: 4.557299, iterations: 406994, cum_reward: -2454.945291\n",
      "Episode 370, loss: 4.641401, iterations: 407494, cum_reward: -2480.969825\n",
      "Episode 371, loss: 1.761765, iterations: 407994, cum_reward: -2473.346447\n",
      "Episode 372, loss: 3.645531, iterations: 408494, cum_reward: -2460.696619\n",
      "Episode 373, loss: 4.328601, iterations: 408994, cum_reward: -2474.666800\n",
      "Episode 374, loss: 4.337690, iterations: 409494, cum_reward: -2482.603756\n",
      "Episode 375, loss: 3.819046, iterations: 409994, cum_reward: -2467.696146\n",
      "Episode 376, loss: 4.652753, iterations: 410494, cum_reward: -2467.508809\n",
      "Episode 377, loss: 4.495112, iterations: 410994, cum_reward: -2480.891268\n",
      "Episode 378, loss: 3.564251, iterations: 411494, cum_reward: -2480.198200\n",
      "Episode 379, loss: 3.989598, iterations: 411994, cum_reward: -2469.997447\n",
      "Episode 380, loss: 4.461061, iterations: 412494, cum_reward: -2465.835966\n",
      "Episode 381, loss: 3.675674, iterations: 412994, cum_reward: -2463.484410\n",
      "Episode 382, loss: 4.444013, iterations: 413494, cum_reward: -2460.695133\n",
      "Episode 383, loss: 4.225608, iterations: 413994, cum_reward: -2473.524624\n",
      "Episode 384, loss: 4.391877, iterations: 414494, cum_reward: -2472.623221\n",
      "Episode 385, loss: 4.118785, iterations: 414994, cum_reward: -2466.866669\n",
      "Episode 386, loss: 4.110222, iterations: 415494, cum_reward: -2480.325234\n",
      "Episode 387, loss: 4.363383, iterations: 415994, cum_reward: -2471.434253\n",
      "Episode 388, loss: 4.320662, iterations: 416494, cum_reward: -2471.754610\n",
      "Episode 389, loss: 4.408281, iterations: 416994, cum_reward: -2467.418390\n",
      "Episode 390, loss: 4.625408, iterations: 417494, cum_reward: -2476.836317\n",
      "Episode 391, loss: 4.674732, iterations: 417994, cum_reward: -2467.643165\n",
      "Episode 392, loss: 4.602126, iterations: 418494, cum_reward: -2479.225625\n",
      "Episode 393, loss: 4.480931, iterations: 418994, cum_reward: -2478.744269\n",
      "Episode 394, loss: 4.529440, iterations: 419494, cum_reward: -2480.798087\n",
      "Episode 395, loss: 4.130674, iterations: 419994, cum_reward: -2476.917087\n",
      "Episode 396, loss: 3.909857, iterations: 420494, cum_reward: -2468.280607\n",
      "Episode 397, loss: 4.268034, iterations: 420994, cum_reward: -2464.559189\n",
      "Episode 398, loss: 4.440643, iterations: 421494, cum_reward: -2468.867884\n",
      "Episode 399, loss: 4.065555, iterations: 421994, cum_reward: -2464.386875\n",
      "Episode 400, loss: 3.644602, iterations: 422494, cum_reward: -2475.879723\n",
      "Episode 401, loss: 4.079068, iterations: 422994, cum_reward: -2471.629153\n",
      "Episode 402, loss: 4.353720, iterations: 423494, cum_reward: -2470.879198\n",
      "Episode 403, loss: 4.551612, iterations: 423994, cum_reward: -2456.468325\n",
      "Episode 404, loss: 3.642817, iterations: 424494, cum_reward: -2467.793408\n",
      "Episode 405, loss: 4.172754, iterations: 424994, cum_reward: -2476.232587\n",
      "Episode 406, loss: 4.047082, iterations: 425494, cum_reward: -2463.730939\n",
      "Episode 407, loss: 4.035193, iterations: 425994, cum_reward: -2472.916682\n",
      "Episode 408, loss: 4.058190, iterations: 426494, cum_reward: -2483.238766\n",
      "Episode 409, loss: 3.773608, iterations: 426994, cum_reward: -2480.088881\n",
      "Episode 410, loss: 4.490809, iterations: 427494, cum_reward: -2480.223588\n",
      "Episode 411, loss: 3.983413, iterations: 427994, cum_reward: -2482.056397\n",
      "Episode 412, loss: 4.651755, iterations: 428494, cum_reward: -2463.041224\n",
      "Episode 413, loss: 4.435791, iterations: 428994, cum_reward: -2478.259837\n",
      "Episode 414, loss: 4.736722, iterations: 429494, cum_reward: -2482.226931\n",
      "Episode 415, loss: 3.817173, iterations: 429994, cum_reward: -2463.214509\n",
      "Episode 416, loss: 4.645082, iterations: 430494, cum_reward: -2471.833562\n",
      "Episode 417, loss: 4.145409, iterations: 430994, cum_reward: -2465.957763\n",
      "Episode 418, loss: 3.430654, iterations: 431494, cum_reward: -2470.828462\n",
      "Episode 419, loss: 4.590445, iterations: 431994, cum_reward: -2480.019403\n",
      "Episode 420, loss: 3.826127, iterations: 432494, cum_reward: -2473.038981\n",
      "Episode 421, loss: 4.332166, iterations: 432994, cum_reward: -2472.016256\n",
      "Episode 422, loss: 4.185771, iterations: 433494, cum_reward: -2467.365563\n",
      "Episode 423, loss: 4.547883, iterations: 433994, cum_reward: -2484.651843\n",
      "Episode 424, loss: 4.216981, iterations: 434494, cum_reward: -2480.223682\n",
      "Episode 425, loss: 4.260670, iterations: 434994, cum_reward: -2492.945396\n",
      "Episode 426, loss: 4.636206, iterations: 435494, cum_reward: -2476.085552\n",
      "Episode 427, loss: 3.612051, iterations: 435994, cum_reward: -2464.609612\n",
      "Episode 428, loss: 4.433305, iterations: 436494, cum_reward: -2457.035857\n",
      "Episode 429, loss: 4.705277, iterations: 436994, cum_reward: -2471.344648\n",
      "Episode 430, loss: 4.681004, iterations: 437494, cum_reward: -2460.132763\n",
      "Episode 431, loss: 3.736020, iterations: 437994, cum_reward: -2482.245528\n",
      "Episode 432, loss: 4.612112, iterations: 438494, cum_reward: -2466.721110\n",
      "Episode 433, loss: 4.161501, iterations: 438994, cum_reward: -2481.234871\n",
      "Episode 434, loss: 4.593615, iterations: 439494, cum_reward: -2479.293337\n",
      "Episode 435, loss: 4.443615, iterations: 439994, cum_reward: -2464.504360\n",
      "Episode 436, loss: 3.977185, iterations: 440494, cum_reward: -2481.823084\n",
      "Episode 437, loss: 4.196250, iterations: 440994, cum_reward: -2477.631486\n",
      "Episode 438, loss: 4.154495, iterations: 441494, cum_reward: -2469.710560\n",
      "Episode 439, loss: 4.509259, iterations: 441994, cum_reward: -2466.600512\n",
      "Episode 440, loss: 3.348668, iterations: 442494, cum_reward: -2487.948843\n",
      "Episode 441, loss: 4.657706, iterations: 442994, cum_reward: -2471.599637\n",
      "Episode 442, loss: 4.427487, iterations: 443494, cum_reward: -2476.687589\n",
      "Episode 443, loss: 4.679317, iterations: 443994, cum_reward: -2479.903547\n",
      "Episode 444, loss: 4.706699, iterations: 444494, cum_reward: -2469.684837\n",
      "Episode 445, loss: 3.797191, iterations: 444994, cum_reward: -2479.911165\n",
      "Episode 446, loss: 4.349518, iterations: 445494, cum_reward: -2474.572191\n",
      "Episode 447, loss: 4.123463, iterations: 445994, cum_reward: -2467.021244\n",
      "Episode 448, loss: 4.375623, iterations: 446494, cum_reward: -2475.670592\n",
      "Episode 449, loss: 4.400314, iterations: 446994, cum_reward: -2479.675030\n",
      "Episode 450, loss: 3.866992, iterations: 447494, cum_reward: -2473.609938\n",
      "Episode 451, loss: 4.547756, iterations: 447994, cum_reward: -2472.653541\n",
      "Episode 452, loss: 4.522799, iterations: 448494, cum_reward: -2480.743431\n",
      "Episode 453, loss: 4.388710, iterations: 448994, cum_reward: -2478.491434\n",
      "Episode 454, loss: 4.373888, iterations: 449494, cum_reward: -2485.039905\n",
      "Episode 455, loss: 4.241122, iterations: 449994, cum_reward: -2468.067697\n",
      "Episode 456, loss: 3.185289, iterations: 450494, cum_reward: -2467.162390\n",
      "Episode 457, loss: 4.044245, iterations: 450994, cum_reward: -2477.069193\n",
      "Episode 458, loss: 4.646000, iterations: 451494, cum_reward: -2471.715140\n",
      "Episode 459, loss: 3.142537, iterations: 451994, cum_reward: -2471.765021\n",
      "Episode 460, loss: 3.993130, iterations: 452494, cum_reward: -2477.187937\n",
      "Episode 461, loss: 4.177323, iterations: 452994, cum_reward: -2480.420312\n",
      "Episode 462, loss: 4.059824, iterations: 453494, cum_reward: -2474.413678\n",
      "Episode 463, loss: 3.578722, iterations: 453994, cum_reward: -2466.973720\n",
      "Episode 464, loss: 3.922677, iterations: 454494, cum_reward: -2474.871712\n",
      "Episode 465, loss: 4.350258, iterations: 454994, cum_reward: -2464.958918\n",
      "Episode 466, loss: 4.499107, iterations: 455494, cum_reward: -2446.177077\n",
      "Episode 467, loss: 4.186758, iterations: 455994, cum_reward: -2478.082979\n",
      "Episode 468, loss: 4.441943, iterations: 456494, cum_reward: -2466.299955\n",
      "Episode 469, loss: 4.220184, iterations: 456994, cum_reward: -2462.337521\n",
      "Episode 470, loss: 4.320631, iterations: 457494, cum_reward: -2473.390038\n",
      "Episode 471, loss: 4.797699, iterations: 457994, cum_reward: -2488.446166\n",
      "Episode 472, loss: 4.210516, iterations: 458494, cum_reward: -2479.376364\n",
      "Episode 473, loss: 4.696063, iterations: 458994, cum_reward: -2476.703807\n",
      "Episode 474, loss: 3.359127, iterations: 459494, cum_reward: -2472.550315\n",
      "Episode 475, loss: 4.125101, iterations: 459994, cum_reward: -2479.793012\n",
      "Episode 476, loss: 4.830408, iterations: 460494, cum_reward: -2489.015103\n",
      "Episode 477, loss: 3.937680, iterations: 460994, cum_reward: -2475.614210\n",
      "Episode 478, loss: 4.232635, iterations: 461494, cum_reward: -2462.498598\n",
      "Episode 479, loss: 4.315252, iterations: 461994, cum_reward: -2487.550879\n",
      "Episode 480, loss: 4.571580, iterations: 462494, cum_reward: -2485.128959\n",
      "Episode 481, loss: 4.297294, iterations: 462994, cum_reward: -2493.102730\n",
      "Episode 482, loss: 4.635927, iterations: 463494, cum_reward: -2469.944529\n",
      "Episode 483, loss: 4.446842, iterations: 463994, cum_reward: -2461.174305\n",
      "Episode 484, loss: 4.244460, iterations: 464494, cum_reward: -2473.267323\n",
      "Episode 485, loss: 3.594536, iterations: 464994, cum_reward: -2476.164526\n",
      "Episode 486, loss: 3.858675, iterations: 465494, cum_reward: -2466.597845\n",
      "Episode 487, loss: 4.582108, iterations: 465994, cum_reward: -2479.223537\n",
      "Episode 488, loss: 4.553050, iterations: 466494, cum_reward: -2479.968494\n",
      "Episode 489, loss: 4.094420, iterations: 466994, cum_reward: -2481.372433\n",
      "Episode 490, loss: 3.902212, iterations: 467494, cum_reward: -2476.945141\n",
      "Episode 491, loss: 4.114289, iterations: 467994, cum_reward: -2471.375445\n",
      "Episode 492, loss: 4.586718, iterations: 468494, cum_reward: -2476.887065\n",
      "Episode 493, loss: 4.196520, iterations: 468994, cum_reward: -2474.633216\n",
      "Episode 494, loss: 4.414704, iterations: 469494, cum_reward: -2466.526820\n",
      "Episode 495, loss: 3.934577, iterations: 469994, cum_reward: -2471.920953\n",
      "Episode 496, loss: 4.513255, iterations: 470494, cum_reward: -2475.665687\n",
      "Episode 497, loss: 4.642982, iterations: 470994, cum_reward: -2463.918436\n",
      "Episode 498, loss: 4.326030, iterations: 471494, cum_reward: -2480.442967\n",
      "Episode 499, loss: 4.170598, iterations: 471994, cum_reward: -2480.414811\n",
      "Episode 500, loss: 3.191758, iterations: 472494, cum_reward: -2485.224246\n",
      "Episode 501, loss: 4.361643, iterations: 472994, cum_reward: -2459.603039\n",
      "Episode 502, loss: 3.496245, iterations: 473494, cum_reward: -2463.779079\n",
      "Episode 503, loss: 4.043562, iterations: 473994, cum_reward: -2478.333895\n",
      "Episode 504, loss: 4.698785, iterations: 474494, cum_reward: -2454.879667\n",
      "Episode 505, loss: 4.170733, iterations: 474994, cum_reward: -2459.261776\n",
      "Episode 506, loss: 3.753202, iterations: 475494, cum_reward: -2480.573936\n",
      "Episode 507, loss: 4.584318, iterations: 475994, cum_reward: -2468.839278\n",
      "Episode 508, loss: 3.443147, iterations: 476494, cum_reward: -2470.088115\n",
      "Episode 509, loss: 3.856078, iterations: 476994, cum_reward: -2464.480989\n",
      "Episode 510, loss: 3.547111, iterations: 477494, cum_reward: -2480.991381\n",
      "Episode 511, loss: 4.717946, iterations: 477994, cum_reward: -2477.898689\n",
      "Episode 512, loss: 4.336452, iterations: 478494, cum_reward: -2469.093744\n",
      "Episode 513, loss: 3.813203, iterations: 478994, cum_reward: -2445.343171\n",
      "Episode 514, loss: 2.999189, iterations: 479494, cum_reward: -2479.148025\n",
      "Episode 515, loss: 4.456125, iterations: 479994, cum_reward: -2474.954506\n",
      "Episode 516, loss: 3.620398, iterations: 480494, cum_reward: -2461.058559\n",
      "Episode 517, loss: 4.640610, iterations: 480994, cum_reward: -2490.755173\n",
      "Episode 518, loss: 3.928524, iterations: 481494, cum_reward: -2453.969660\n",
      "Episode 519, loss: 3.774728, iterations: 481994, cum_reward: -2481.634371\n",
      "Episode 520, loss: 4.370711, iterations: 482494, cum_reward: -2473.728721\n",
      "Episode 521, loss: 4.457053, iterations: 482994, cum_reward: -2461.250338\n",
      "Episode 522, loss: 4.039515, iterations: 483494, cum_reward: -2476.328939\n",
      "Episode 523, loss: 3.445465, iterations: 483994, cum_reward: -2467.528926\n",
      "Episode 524, loss: 3.983064, iterations: 484494, cum_reward: -2472.261334\n",
      "Episode 525, loss: 4.250861, iterations: 484994, cum_reward: -2483.785433\n",
      "Episode 526, loss: 4.449944, iterations: 485494, cum_reward: -2476.230934\n",
      "Episode 527, loss: 4.507926, iterations: 485994, cum_reward: -2471.337327\n",
      "Episode 528, loss: 4.176888, iterations: 486494, cum_reward: -2480.528183\n",
      "Episode 529, loss: 4.667257, iterations: 486994, cum_reward: -2471.479087\n",
      "Episode 530, loss: 4.558956, iterations: 487494, cum_reward: -2469.842381\n",
      "Episode 531, loss: 4.130589, iterations: 487994, cum_reward: -2490.143182\n",
      "Episode 532, loss: 3.111221, iterations: 488494, cum_reward: -2475.206095\n",
      "Episode 533, loss: 4.736892, iterations: 488994, cum_reward: -2463.523099\n",
      "Episode 534, loss: 4.122528, iterations: 489494, cum_reward: -2470.322869\n",
      "Episode 535, loss: 4.539994, iterations: 489994, cum_reward: -2465.034781\n",
      "Episode 536, loss: 4.358786, iterations: 490494, cum_reward: -2457.232317\n",
      "Episode 537, loss: 4.097715, iterations: 490994, cum_reward: -2484.085825\n",
      "Episode 538, loss: 3.933235, iterations: 491494, cum_reward: -2474.920202\n",
      "Episode 539, loss: 4.584163, iterations: 491994, cum_reward: -2478.193421\n",
      "Episode 540, loss: 3.584538, iterations: 492494, cum_reward: -2473.897719\n",
      "Episode 541, loss: 4.527138, iterations: 492994, cum_reward: -2473.737414\n",
      "Episode 542, loss: 3.796444, iterations: 493494, cum_reward: -2481.371375\n",
      "Episode 543, loss: 4.460876, iterations: 493994, cum_reward: -2470.895029\n",
      "Episode 544, loss: 4.667216, iterations: 494494, cum_reward: -2488.178227\n",
      "Episode 545, loss: 3.221587, iterations: 494994, cum_reward: -2467.455349\n",
      "Episode 546, loss: 2.802032, iterations: 495494, cum_reward: -2472.431315\n",
      "Episode 547, loss: 4.542430, iterations: 495994, cum_reward: -2466.487711\n",
      "Episode 548, loss: 4.332990, iterations: 496494, cum_reward: -2480.616339\n",
      "Episode 549, loss: 4.368928, iterations: 496994, cum_reward: -2478.778914\n",
      "Episode 550, loss: 4.278502, iterations: 497494, cum_reward: -2462.096795\n",
      "Episode 551, loss: 4.443530, iterations: 497994, cum_reward: -2472.790183\n",
      "Episode 552, loss: 4.041902, iterations: 498494, cum_reward: -2473.197976\n",
      "Episode 553, loss: 4.474977, iterations: 498994, cum_reward: -2478.999347\n",
      "Episode 554, loss: 4.204112, iterations: 499494, cum_reward: -2473.010265\n",
      "Episode 555, loss: 4.486722, iterations: 499994, cum_reward: -2465.199178\n",
      "Episode 556, loss: 4.664237, iterations: 500494, cum_reward: -2464.341385\n",
      "Episode 557, loss: 4.160354, iterations: 500994, cum_reward: -2483.608533\n",
      "Episode 558, loss: 4.372340, iterations: 501494, cum_reward: -2483.191205\n",
      "Episode 559, loss: 4.055124, iterations: 501994, cum_reward: -2482.734445\n",
      "Episode 560, loss: 4.254314, iterations: 502494, cum_reward: -2477.936797\n",
      "Episode 561, loss: 3.737729, iterations: 502994, cum_reward: -2472.740123\n",
      "Episode 562, loss: 4.551252, iterations: 503494, cum_reward: -2471.118443\n",
      "Episode 563, loss: 3.834832, iterations: 503994, cum_reward: -2466.188045\n",
      "Episode 564, loss: 4.722979, iterations: 504494, cum_reward: -2457.141960\n",
      "Episode 565, loss: 3.562752, iterations: 504994, cum_reward: -2466.919849\n",
      "Episode 566, loss: 4.385319, iterations: 505494, cum_reward: -2474.206711\n",
      "Episode 567, loss: 3.735710, iterations: 505994, cum_reward: -2464.563470\n",
      "Episode 568, loss: 3.523867, iterations: 506494, cum_reward: -2464.049154\n",
      "Episode 569, loss: 4.240368, iterations: 506994, cum_reward: -2473.983118\n",
      "Episode 570, loss: 4.611720, iterations: 507494, cum_reward: -2469.855613\n",
      "Episode 571, loss: 3.233766, iterations: 507994, cum_reward: -2466.979415\n",
      "Episode 572, loss: 3.820993, iterations: 508494, cum_reward: -2469.191819\n",
      "Episode 573, loss: 4.480789, iterations: 508994, cum_reward: -2468.808388\n",
      "Episode 574, loss: 4.560929, iterations: 509494, cum_reward: -2473.174102\n",
      "Episode 575, loss: 4.414136, iterations: 509994, cum_reward: -2484.292214\n",
      "Episode 576, loss: 4.635665, iterations: 510494, cum_reward: -2461.131909\n",
      "Episode 577, loss: 4.359260, iterations: 510994, cum_reward: -2474.495135\n",
      "Episode 578, loss: 4.411250, iterations: 511494, cum_reward: -2462.834361\n",
      "Episode 579, loss: 4.069463, iterations: 511994, cum_reward: -2474.973071\n",
      "Episode 580, loss: 4.001910, iterations: 512494, cum_reward: -2478.434763\n",
      "Episode 581, loss: 4.301718, iterations: 512994, cum_reward: -2479.178902\n",
      "Episode 582, loss: 3.660520, iterations: 513494, cum_reward: -2474.485657\n",
      "Episode 583, loss: 3.588768, iterations: 513994, cum_reward: -2483.772077\n",
      "Episode 584, loss: 3.749523, iterations: 514494, cum_reward: -2464.715571\n",
      "Episode 585, loss: 4.394152, iterations: 514994, cum_reward: -2477.523560\n",
      "Episode 586, loss: 4.243395, iterations: 515494, cum_reward: -2477.469255\n",
      "Episode 587, loss: 4.281129, iterations: 515994, cum_reward: -2465.510080\n",
      "Episode 588, loss: 3.138562, iterations: 516494, cum_reward: -2475.014649\n",
      "Episode 589, loss: 3.989359, iterations: 516994, cum_reward: -2480.317445\n",
      "Episode 590, loss: 4.423744, iterations: 517494, cum_reward: -2470.945813\n",
      "Episode 591, loss: 4.565652, iterations: 517994, cum_reward: -2470.582781\n",
      "Episode 592, loss: 4.609810, iterations: 518494, cum_reward: -2472.856272\n",
      "Episode 593, loss: 4.534726, iterations: 518994, cum_reward: -2477.902220\n",
      "Episode 594, loss: 3.651909, iterations: 519494, cum_reward: -2468.742197\n",
      "Episode 595, loss: 4.378254, iterations: 519994, cum_reward: -2489.972969\n",
      "Episode 596, loss: 4.183885, iterations: 520494, cum_reward: -2478.714987\n",
      "Episode 597, loss: 4.481079, iterations: 520994, cum_reward: -2479.448175\n",
      "Episode 598, loss: 4.607841, iterations: 521494, cum_reward: -2474.442843\n",
      "Episode 599, loss: 3.611228, iterations: 521994, cum_reward: -2487.432380\n",
      "Episode 600, loss: 4.636723, iterations: 522494, cum_reward: -2484.024492\n",
      "Episode 601, loss: 4.433116, iterations: 522994, cum_reward: -2464.475821\n",
      "Episode 602, loss: 3.964948, iterations: 523494, cum_reward: -2477.426365\n",
      "Episode 603, loss: 4.494648, iterations: 523994, cum_reward: -2482.458613\n",
      "Episode 604, loss: 4.379386, iterations: 524494, cum_reward: -2481.347408\n",
      "Episode 605, loss: 4.234405, iterations: 524994, cum_reward: -2477.984469\n",
      "Episode 606, loss: 4.260412, iterations: 525494, cum_reward: -2474.931284\n",
      "Episode 607, loss: 4.712601, iterations: 525994, cum_reward: -2477.170892\n",
      "Episode 608, loss: 4.722130, iterations: 526494, cum_reward: -2484.130328\n",
      "Episode 609, loss: 3.442203, iterations: 526994, cum_reward: -2476.842850\n",
      "Episode 610, loss: 3.414950, iterations: 527494, cum_reward: -2466.800632\n",
      "Episode 611, loss: 4.628228, iterations: 527994, cum_reward: -2484.090662\n",
      "Episode 612, loss: 4.685288, iterations: 528494, cum_reward: -2461.850412\n",
      "Episode 613, loss: 4.500010, iterations: 528994, cum_reward: -2486.939491\n",
      "Episode 614, loss: 3.948915, iterations: 529494, cum_reward: -2494.167578\n",
      "Episode 615, loss: 2.873233, iterations: 529994, cum_reward: -2469.870101\n",
      "Episode 616, loss: 4.229011, iterations: 530494, cum_reward: -2473.993963\n",
      "Episode 617, loss: 4.554376, iterations: 530994, cum_reward: -2485.605348\n",
      "Episode 618, loss: 3.867270, iterations: 531494, cum_reward: -2480.055694\n",
      "Episode 619, loss: 4.363608, iterations: 531994, cum_reward: -2457.627313\n",
      "Episode 620, loss: 4.541731, iterations: 532494, cum_reward: -2470.816350\n",
      "Episode 621, loss: 3.722988, iterations: 532994, cum_reward: -2456.260847\n",
      "Episode 622, loss: 4.468447, iterations: 533494, cum_reward: -2468.433386\n",
      "Episode 623, loss: 3.941536, iterations: 533994, cum_reward: -2482.488757\n",
      "Episode 624, loss: 3.379958, iterations: 534494, cum_reward: -2470.783724\n",
      "Episode 625, loss: 4.403751, iterations: 534994, cum_reward: -2472.519619\n",
      "Episode 626, loss: 4.504348, iterations: 535494, cum_reward: -2478.381449\n",
      "Episode 627, loss: 4.726157, iterations: 535994, cum_reward: -2479.134551\n",
      "Episode 628, loss: 4.316910, iterations: 536494, cum_reward: -2487.272203\n",
      "Episode 629, loss: 3.329794, iterations: 536994, cum_reward: -2466.321682\n",
      "Episode 630, loss: 3.749999, iterations: 537494, cum_reward: -2480.665815\n",
      "Episode 631, loss: 4.323318, iterations: 537994, cum_reward: -2479.134480\n",
      "Episode 632, loss: 4.612041, iterations: 538494, cum_reward: -2489.703747\n",
      "Episode 633, loss: 4.624978, iterations: 538994, cum_reward: -2481.859752\n",
      "Episode 634, loss: 3.792812, iterations: 539494, cum_reward: -2458.192556\n",
      "Episode 635, loss: 4.567312, iterations: 539994, cum_reward: -2468.812387\n",
      "Episode 636, loss: 4.579762, iterations: 540494, cum_reward: -2480.884952\n",
      "Episode 637, loss: 4.351422, iterations: 540994, cum_reward: -2473.065428\n",
      "Episode 638, loss: 4.187358, iterations: 541494, cum_reward: -2469.682116\n",
      "Episode 639, loss: 4.110529, iterations: 541994, cum_reward: -2475.026384\n",
      "Episode 640, loss: 3.882119, iterations: 542494, cum_reward: -2472.277086\n",
      "Episode 641, loss: 4.584418, iterations: 542994, cum_reward: -2486.588656\n",
      "Episode 642, loss: 4.470572, iterations: 543494, cum_reward: -2466.696295\n",
      "Episode 643, loss: 4.202279, iterations: 543994, cum_reward: -2476.422963\n",
      "Episode 644, loss: 3.574960, iterations: 544494, cum_reward: -2474.825719\n",
      "Episode 645, loss: 3.740658, iterations: 544994, cum_reward: -2473.848031\n",
      "Episode 646, loss: 3.924549, iterations: 545494, cum_reward: -2471.472897\n",
      "Episode 647, loss: 4.133729, iterations: 545994, cum_reward: -2474.501979\n",
      "Episode 648, loss: 3.609994, iterations: 546494, cum_reward: -2456.571568\n",
      "Episode 649, loss: 4.253687, iterations: 546994, cum_reward: -2476.932612\n",
      "Episode 650, loss: 3.343877, iterations: 547494, cum_reward: -2463.189692\n",
      "Episode 651, loss: 4.430648, iterations: 547994, cum_reward: -2470.922763\n",
      "Episode 652, loss: 4.304395, iterations: 548494, cum_reward: -2477.086297\n",
      "Episode 653, loss: 3.491903, iterations: 548994, cum_reward: -2467.101581\n",
      "Episode 654, loss: 4.429070, iterations: 549494, cum_reward: -2454.018083\n",
      "Episode 655, loss: 4.297565, iterations: 549994, cum_reward: -2466.908101\n",
      "Episode 656, loss: 4.551738, iterations: 550494, cum_reward: -2479.421961\n",
      "Episode 657, loss: 4.571760, iterations: 550994, cum_reward: -2475.022434\n",
      "Episode 658, loss: 4.398332, iterations: 551494, cum_reward: -2483.311103\n",
      "Episode 659, loss: 3.595482, iterations: 551994, cum_reward: -2477.412625\n",
      "Episode 660, loss: 4.484653, iterations: 552494, cum_reward: -2474.448666\n",
      "Episode 661, loss: 4.483745, iterations: 552994, cum_reward: -2469.413614\n",
      "Episode 662, loss: 4.334012, iterations: 553494, cum_reward: -2479.857748\n",
      "Episode 663, loss: 3.856441, iterations: 553994, cum_reward: -2458.384617\n",
      "Episode 664, loss: 4.724949, iterations: 554494, cum_reward: -2482.447188\n",
      "Episode 665, loss: 4.234446, iterations: 554994, cum_reward: -2474.441123\n",
      "Episode 666, loss: 2.737912, iterations: 555494, cum_reward: -2478.158184\n",
      "Episode 667, loss: 4.110573, iterations: 555994, cum_reward: -2452.705448\n",
      "Episode 668, loss: 4.524513, iterations: 556494, cum_reward: -2468.508434\n",
      "Episode 669, loss: 4.232756, iterations: 556994, cum_reward: -2473.734857\n",
      "Episode 670, loss: 4.326084, iterations: 557494, cum_reward: -2473.018392\n",
      "Episode 671, loss: 4.534304, iterations: 557994, cum_reward: -2466.464507\n",
      "Episode 672, loss: 4.222025, iterations: 558494, cum_reward: -2489.098968\n",
      "Episode 673, loss: 4.111779, iterations: 558994, cum_reward: -2468.155097\n",
      "Episode 674, loss: 3.464249, iterations: 559494, cum_reward: -2474.400384\n",
      "Episode 675, loss: 3.911350, iterations: 559994, cum_reward: -2482.606977\n",
      "Episode 676, loss: 3.895856, iterations: 560494, cum_reward: -2468.447409\n",
      "Episode 677, loss: 4.485061, iterations: 560994, cum_reward: -2478.389062\n",
      "Episode 678, loss: 4.206651, iterations: 561494, cum_reward: -2466.507462\n",
      "Episode 679, loss: 4.517180, iterations: 561994, cum_reward: -2461.329041\n",
      "Episode 680, loss: 4.604224, iterations: 562494, cum_reward: -2478.726348\n",
      "Episode 681, loss: 3.582823, iterations: 562994, cum_reward: -2489.732894\n",
      "Episode 682, loss: 3.825820, iterations: 563494, cum_reward: -2479.524212\n",
      "Episode 683, loss: 4.393073, iterations: 563994, cum_reward: -2476.882565\n",
      "Episode 684, loss: 3.990474, iterations: 564494, cum_reward: -2485.067635\n",
      "Episode 685, loss: 4.511531, iterations: 564994, cum_reward: -2474.981647\n",
      "Episode 686, loss: 3.347177, iterations: 565494, cum_reward: -2463.789055\n",
      "Episode 687, loss: 4.006887, iterations: 565994, cum_reward: -2476.454877\n",
      "Episode 688, loss: 3.965569, iterations: 566494, cum_reward: -2482.199502\n",
      "Episode 689, loss: 4.565145, iterations: 566994, cum_reward: -2488.020415\n",
      "Episode 690, loss: 4.452531, iterations: 567494, cum_reward: -2489.135913\n",
      "Episode 691, loss: 4.532866, iterations: 567994, cum_reward: -2476.493403\n",
      "Episode 692, loss: 4.410100, iterations: 568494, cum_reward: -2468.751434\n",
      "Episode 693, loss: 4.558361, iterations: 568994, cum_reward: -2470.874535\n",
      "Episode 694, loss: 4.279763, iterations: 569494, cum_reward: -2480.350181\n",
      "Episode 695, loss: 4.552168, iterations: 569994, cum_reward: -2471.014775\n",
      "Episode 696, loss: 4.354242, iterations: 570494, cum_reward: -2486.222518\n",
      "Episode 697, loss: 4.144466, iterations: 570994, cum_reward: -2471.578165\n",
      "Episode 698, loss: 4.220160, iterations: 571494, cum_reward: -2478.442679\n",
      "Episode 699, loss: 4.563805, iterations: 571994, cum_reward: -2472.172776\n",
      "Episode 700, loss: 4.449039, iterations: 572494, cum_reward: -2471.010604\n",
      "Episode 701, loss: 4.341924, iterations: 572994, cum_reward: -2470.495446\n",
      "Episode 702, loss: 4.575632, iterations: 573494, cum_reward: -2478.094242\n",
      "Episode 703, loss: 4.615809, iterations: 573994, cum_reward: -2468.270069\n",
      "Episode 704, loss: 4.604477, iterations: 574494, cum_reward: -2464.272223\n",
      "Episode 705, loss: 4.208695, iterations: 574994, cum_reward: -2480.201226\n",
      "Episode 706, loss: 4.354011, iterations: 575494, cum_reward: -2479.764638\n",
      "Episode 707, loss: 4.722833, iterations: 575994, cum_reward: -2469.153160\n",
      "Episode 708, loss: 4.064542, iterations: 576494, cum_reward: -2473.310255\n",
      "Episode 709, loss: 4.137611, iterations: 576994, cum_reward: -2480.071106\n",
      "Episode 710, loss: 4.539260, iterations: 577494, cum_reward: -2467.527546\n",
      "Episode 711, loss: 4.452049, iterations: 577994, cum_reward: -2473.820432\n",
      "Episode 712, loss: 4.684586, iterations: 578494, cum_reward: -2459.472439\n",
      "Episode 713, loss: 4.022163, iterations: 578994, cum_reward: -2472.587270\n",
      "Episode 714, loss: 3.634530, iterations: 579494, cum_reward: -2491.665517\n",
      "Episode 715, loss: 4.665231, iterations: 579994, cum_reward: -2475.600218\n",
      "Episode 716, loss: 4.183456, iterations: 580494, cum_reward: -2475.040444\n",
      "Episode 717, loss: 4.611321, iterations: 580994, cum_reward: -2477.484794\n",
      "Episode 718, loss: 4.728222, iterations: 581494, cum_reward: -2468.719497\n",
      "Episode 719, loss: 4.590935, iterations: 581994, cum_reward: -2466.826121\n",
      "Episode 720, loss: 4.005384, iterations: 582494, cum_reward: -2487.008722\n",
      "Episode 721, loss: 3.934433, iterations: 582994, cum_reward: -2470.877435\n",
      "Episode 722, loss: 3.236921, iterations: 583494, cum_reward: -2470.607401\n",
      "Episode 723, loss: 3.929130, iterations: 583994, cum_reward: -2486.320985\n",
      "Episode 724, loss: 4.481448, iterations: 584494, cum_reward: -2476.714411\n",
      "Episode 725, loss: 4.685880, iterations: 584994, cum_reward: -2469.054474\n",
      "Episode 726, loss: 3.676952, iterations: 585494, cum_reward: -2473.946693\n",
      "Episode 727, loss: 4.290453, iterations: 585994, cum_reward: -2477.756485\n",
      "Episode 728, loss: 3.663983, iterations: 586494, cum_reward: -2473.035355\n",
      "Episode 729, loss: 4.537482, iterations: 586994, cum_reward: -2471.607607\n",
      "Episode 730, loss: 4.493041, iterations: 587494, cum_reward: -2478.438962\n",
      "Episode 731, loss: 4.366902, iterations: 587994, cum_reward: -2471.203943\n",
      "Episode 732, loss: 3.686284, iterations: 588494, cum_reward: -2473.163917\n",
      "Episode 733, loss: 4.114283, iterations: 588994, cum_reward: -2461.779341\n",
      "Episode 734, loss: 3.627084, iterations: 589494, cum_reward: -2477.233984\n",
      "Episode 735, loss: 4.575429, iterations: 589994, cum_reward: -2485.650724\n",
      "Episode 736, loss: 3.791820, iterations: 590494, cum_reward: -2471.487353\n",
      "Episode 737, loss: 3.528125, iterations: 590994, cum_reward: -2476.284547\n",
      "Episode 738, loss: 4.402941, iterations: 591494, cum_reward: -2481.268574\n",
      "Episode 739, loss: 4.134688, iterations: 591994, cum_reward: -2471.945308\n",
      "Episode 740, loss: 4.817899, iterations: 592494, cum_reward: -2473.734907\n",
      "Episode 741, loss: 4.335310, iterations: 592994, cum_reward: -2469.647845\n",
      "Episode 742, loss: 3.310255, iterations: 593494, cum_reward: -2480.581357\n",
      "Episode 743, loss: 4.382159, iterations: 593994, cum_reward: -2470.839567\n",
      "Episode 744, loss: 3.277740, iterations: 594494, cum_reward: -2488.111489\n",
      "Episode 745, loss: 3.121195, iterations: 594994, cum_reward: -2472.172194\n",
      "Episode 746, loss: 3.229395, iterations: 595494, cum_reward: -2469.394171\n",
      "Episode 747, loss: 4.083804, iterations: 595994, cum_reward: -2472.725047\n",
      "Episode 748, loss: 3.878835, iterations: 596494, cum_reward: -2469.081348\n",
      "Episode 749, loss: 4.363784, iterations: 596994, cum_reward: -2468.042305\n",
      "Episode 750, loss: 4.529150, iterations: 597494, cum_reward: -2479.389969\n",
      "Episode 751, loss: 4.311681, iterations: 597994, cum_reward: -2457.964093\n",
      "Episode 752, loss: 4.554079, iterations: 598494, cum_reward: -2480.011402\n",
      "Episode 753, loss: 4.410472, iterations: 598994, cum_reward: -2467.925636\n",
      "Episode 754, loss: 3.574104, iterations: 599494, cum_reward: -2472.433268\n",
      "Episode 755, loss: 4.214903, iterations: 599994, cum_reward: -2474.611718\n",
      "Episode 756, loss: 4.357767, iterations: 600494, cum_reward: -2475.805485\n",
      "Episode 757, loss: 4.648113, iterations: 600994, cum_reward: -2473.146948\n",
      "Episode 758, loss: 3.866055, iterations: 601494, cum_reward: -2477.575992\n",
      "Episode 759, loss: 4.467286, iterations: 601994, cum_reward: -2465.021285\n",
      "Episode 760, loss: 3.719315, iterations: 602494, cum_reward: -2478.930222\n",
      "Episode 761, loss: 3.931464, iterations: 602994, cum_reward: -2475.339563\n",
      "Episode 762, loss: 4.306178, iterations: 603494, cum_reward: -2471.199129\n",
      "Episode 763, loss: 4.254784, iterations: 603994, cum_reward: -2479.467750\n",
      "Episode 764, loss: 4.316253, iterations: 604494, cum_reward: -2477.599060\n",
      "Episode 765, loss: 4.332774, iterations: 604994, cum_reward: -2485.964623\n",
      "Episode 766, loss: 4.356060, iterations: 605494, cum_reward: -2478.309061\n",
      "Episode 767, loss: 4.218695, iterations: 605994, cum_reward: -2474.619552\n",
      "Episode 768, loss: 3.644721, iterations: 606494, cum_reward: -2468.379764\n",
      "Episode 769, loss: 3.614967, iterations: 606994, cum_reward: -2465.171546\n",
      "Episode 770, loss: 4.142396, iterations: 607494, cum_reward: -2476.198913\n",
      "Episode 771, loss: 4.332342, iterations: 607994, cum_reward: -2479.396068\n",
      "Episode 772, loss: 4.296900, iterations: 608494, cum_reward: -2469.312023\n",
      "Episode 773, loss: 4.451891, iterations: 608994, cum_reward: -2472.849783\n",
      "Episode 774, loss: 4.324603, iterations: 609494, cum_reward: -2477.460014\n",
      "Episode 775, loss: 3.607809, iterations: 609994, cum_reward: -2446.511263\n",
      "Episode 776, loss: 3.623299, iterations: 610494, cum_reward: -2464.723187\n",
      "Episode 777, loss: 4.188787, iterations: 610994, cum_reward: -2460.582085\n",
      "Episode 778, loss: 4.030212, iterations: 611494, cum_reward: -2474.122245\n",
      "Episode 779, loss: 4.630991, iterations: 611994, cum_reward: -2471.496573\n",
      "Episode 780, loss: 4.635679, iterations: 612494, cum_reward: -2486.650682\n",
      "Episode 781, loss: 4.488215, iterations: 612994, cum_reward: -2465.370729\n",
      "Episode 782, loss: 4.059510, iterations: 613494, cum_reward: -2468.406161\n",
      "Episode 783, loss: 3.858091, iterations: 613994, cum_reward: -2482.536354\n",
      "Episode 784, loss: 4.145370, iterations: 614494, cum_reward: -2458.070649\n",
      "Episode 785, loss: 3.722309, iterations: 614994, cum_reward: -2475.100540\n",
      "Episode 786, loss: 4.285122, iterations: 615494, cum_reward: -2471.355117\n",
      "Episode 787, loss: 4.089299, iterations: 615994, cum_reward: -2477.685521\n",
      "Episode 788, loss: 4.020353, iterations: 616494, cum_reward: -2472.630413\n",
      "Episode 789, loss: 4.318390, iterations: 616994, cum_reward: -2466.706475\n",
      "Episode 790, loss: 4.767281, iterations: 617494, cum_reward: -2481.256147\n",
      "Episode 791, loss: 4.351217, iterations: 617994, cum_reward: -2478.391630\n",
      "Episode 792, loss: 4.573515, iterations: 618494, cum_reward: -2476.509081\n",
      "Episode 793, loss: 3.943617, iterations: 618994, cum_reward: -2476.467077\n",
      "Episode 794, loss: 3.464745, iterations: 619494, cum_reward: -2476.762496\n",
      "Episode 795, loss: 4.175331, iterations: 619994, cum_reward: -2453.540257\n",
      "Episode 796, loss: 4.132278, iterations: 620494, cum_reward: -2476.274820\n",
      "Episode 797, loss: 4.572222, iterations: 620994, cum_reward: -2463.602807\n",
      "Episode 798, loss: 4.122303, iterations: 621494, cum_reward: -2484.432826\n",
      "Episode 799, loss: 4.327909, iterations: 621994, cum_reward: -2482.976162\n",
      "Episode 800, loss: 4.608584, iterations: 622494, cum_reward: -2468.802425\n",
      "Episode 801, loss: 4.274003, iterations: 622994, cum_reward: -2476.934540\n",
      "Episode 802, loss: 4.292372, iterations: 623494, cum_reward: -2468.217890\n",
      "Episode 803, loss: 4.451360, iterations: 623994, cum_reward: -2471.381181\n",
      "Episode 804, loss: 3.955416, iterations: 624494, cum_reward: -2468.254680\n",
      "Episode 805, loss: 3.472507, iterations: 624994, cum_reward: -2482.887026\n",
      "Episode 806, loss: 4.023642, iterations: 625494, cum_reward: -2471.806312\n",
      "Episode 807, loss: 4.566059, iterations: 625994, cum_reward: -2474.151730\n",
      "Episode 808, loss: 3.977401, iterations: 626494, cum_reward: -2474.226765\n",
      "Episode 809, loss: 4.521872, iterations: 626994, cum_reward: -2478.629156\n",
      "Episode 810, loss: 4.626613, iterations: 627494, cum_reward: -2466.372396\n",
      "Episode 811, loss: 4.094607, iterations: 627994, cum_reward: -2477.600029\n",
      "Episode 812, loss: 4.148223, iterations: 628494, cum_reward: -2473.078575\n",
      "Episode 813, loss: 4.272872, iterations: 628994, cum_reward: -2455.112212\n",
      "Episode 814, loss: 4.205024, iterations: 629494, cum_reward: -2476.735464\n",
      "Episode 815, loss: 4.662081, iterations: 629994, cum_reward: -2483.918405\n",
      "Episode 816, loss: 4.340654, iterations: 630494, cum_reward: -2474.476731\n",
      "Episode 817, loss: 4.398208, iterations: 630994, cum_reward: -2479.245370\n",
      "Episode 818, loss: 4.300502, iterations: 631494, cum_reward: -2458.503820\n",
      "Episode 819, loss: 4.244058, iterations: 631994, cum_reward: -2470.227334\n",
      "Episode 820, loss: 3.749342, iterations: 632494, cum_reward: -2474.729246\n",
      "Episode 821, loss: 4.404789, iterations: 632994, cum_reward: -2476.384744\n",
      "Episode 822, loss: 4.415957, iterations: 633494, cum_reward: -2467.347691\n",
      "Episode 823, loss: 4.007526, iterations: 633994, cum_reward: -2475.650487\n",
      "Episode 824, loss: 3.566314, iterations: 634494, cum_reward: -2465.631663\n",
      "Episode 825, loss: 4.127799, iterations: 634994, cum_reward: -2458.376286\n",
      "Episode 826, loss: 3.400120, iterations: 635494, cum_reward: -2476.162307\n",
      "Episode 827, loss: 4.210554, iterations: 635994, cum_reward: -2472.610432\n",
      "Episode 828, loss: 4.645607, iterations: 636494, cum_reward: -2468.363577\n",
      "Episode 829, loss: 4.333706, iterations: 636994, cum_reward: -2461.643443\n",
      "Episode 830, loss: 4.325899, iterations: 637494, cum_reward: -2468.897953\n",
      "Episode 831, loss: 4.564189, iterations: 637994, cum_reward: -2477.579624\n",
      "Episode 832, loss: 3.273862, iterations: 638494, cum_reward: -2487.119950\n",
      "Episode 833, loss: 4.235819, iterations: 638994, cum_reward: -2475.246241\n",
      "Episode 834, loss: 4.178441, iterations: 639494, cum_reward: -2474.298231\n",
      "Episode 835, loss: 4.333477, iterations: 639994, cum_reward: -2480.342839\n",
      "Episode 836, loss: 4.770079, iterations: 640494, cum_reward: -2474.727245\n",
      "Episode 837, loss: 4.400032, iterations: 640994, cum_reward: -2477.615912\n",
      "Episode 838, loss: 4.245594, iterations: 641494, cum_reward: -2462.716007\n",
      "Episode 839, loss: 3.816916, iterations: 641994, cum_reward: -2475.072988\n",
      "Episode 840, loss: 3.018430, iterations: 642494, cum_reward: -2491.772933\n",
      "Episode 841, loss: 3.718761, iterations: 642994, cum_reward: -2476.142285\n",
      "Episode 842, loss: 4.070137, iterations: 643494, cum_reward: -2482.473681\n",
      "Episode 843, loss: 4.442978, iterations: 643994, cum_reward: -2475.638680\n",
      "Episode 844, loss: 4.334969, iterations: 644494, cum_reward: -2479.161103\n",
      "Episode 845, loss: 4.482984, iterations: 644994, cum_reward: -2472.146078\n",
      "Episode 846, loss: 3.879959, iterations: 645494, cum_reward: -2471.950414\n",
      "Episode 847, loss: 4.132545, iterations: 645994, cum_reward: -2467.307785\n",
      "Episode 848, loss: 3.929839, iterations: 646494, cum_reward: -2458.677749\n",
      "Episode 849, loss: 3.834712, iterations: 646994, cum_reward: -2470.292336\n",
      "Episode 850, loss: 4.031399, iterations: 647494, cum_reward: -2471.870741\n",
      "Episode 851, loss: 4.547740, iterations: 647994, cum_reward: -2485.477865\n",
      "Episode 852, loss: 4.421044, iterations: 648494, cum_reward: -2476.552098\n",
      "Episode 853, loss: 4.027910, iterations: 648994, cum_reward: -2471.926967\n",
      "Episode 854, loss: 4.416742, iterations: 649494, cum_reward: -2464.817190\n",
      "Episode 855, loss: 4.495439, iterations: 649994, cum_reward: -2480.607234\n",
      "Episode 856, loss: 4.573570, iterations: 650494, cum_reward: -2478.663676\n",
      "Episode 857, loss: 4.503164, iterations: 650994, cum_reward: -2480.482452\n",
      "Episode 858, loss: 4.453702, iterations: 651494, cum_reward: -2461.934720\n",
      "Episode 859, loss: 4.522026, iterations: 651994, cum_reward: -2483.852633\n",
      "Episode 860, loss: 4.473017, iterations: 652494, cum_reward: -2468.524711\n",
      "Episode 861, loss: 3.971512, iterations: 652994, cum_reward: -2479.318354\n",
      "Episode 862, loss: 3.801718, iterations: 653494, cum_reward: -2468.643600\n",
      "Episode 863, loss: 4.420038, iterations: 653994, cum_reward: -2471.948690\n",
      "Episode 864, loss: 4.145797, iterations: 654494, cum_reward: -2467.949374\n",
      "Episode 865, loss: 3.687837, iterations: 654994, cum_reward: -2471.136409\n",
      "Episode 866, loss: 4.585483, iterations: 655494, cum_reward: -2479.540210\n",
      "Episode 867, loss: 4.412124, iterations: 655994, cum_reward: -2469.755273\n",
      "Episode 868, loss: 3.903126, iterations: 656494, cum_reward: -2479.231471\n",
      "Episode 869, loss: 4.258928, iterations: 656994, cum_reward: -2478.468076\n",
      "Episode 870, loss: 3.837180, iterations: 657494, cum_reward: -2477.043067\n",
      "Episode 871, loss: 4.779752, iterations: 657994, cum_reward: -2476.033135\n",
      "Episode 872, loss: 4.049121, iterations: 658494, cum_reward: -2462.707092\n",
      "Episode 873, loss: 4.382257, iterations: 658994, cum_reward: -2476.551608\n",
      "Episode 874, loss: 4.600975, iterations: 659494, cum_reward: -2473.594648\n",
      "Episode 875, loss: 4.001562, iterations: 659994, cum_reward: -2461.730096\n",
      "Episode 876, loss: 4.555642, iterations: 660494, cum_reward: -2470.478342\n",
      "Episode 877, loss: 4.450485, iterations: 660994, cum_reward: -2459.021985\n",
      "Episode 878, loss: 3.278285, iterations: 661494, cum_reward: -2487.942463\n",
      "Episode 879, loss: 4.558058, iterations: 661994, cum_reward: -2471.709547\n",
      "Episode 880, loss: 3.695005, iterations: 662494, cum_reward: -2478.911843\n",
      "Episode 881, loss: 4.017609, iterations: 662994, cum_reward: -2474.054729\n",
      "Episode 882, loss: 3.529192, iterations: 663494, cum_reward: -2475.005835\n",
      "Episode 883, loss: 4.562346, iterations: 663994, cum_reward: -2479.972799\n",
      "Episode 884, loss: 4.507134, iterations: 664494, cum_reward: -2497.063728\n",
      "Episode 885, loss: 4.566596, iterations: 664994, cum_reward: -2469.824421\n",
      "Episode 886, loss: 4.687838, iterations: 665494, cum_reward: -2480.840318\n",
      "Episode 887, loss: 4.510342, iterations: 665994, cum_reward: -2474.216915\n",
      "Episode 888, loss: 3.187504, iterations: 666494, cum_reward: -2477.071407\n",
      "Episode 889, loss: 4.131551, iterations: 666994, cum_reward: -2472.931783\n",
      "Episode 890, loss: 3.946738, iterations: 667494, cum_reward: -2464.123061\n",
      "Episode 891, loss: 4.647361, iterations: 667994, cum_reward: -2456.377159\n",
      "Episode 892, loss: 4.035430, iterations: 668494, cum_reward: -2468.950100\n",
      "Episode 893, loss: 4.595414, iterations: 668994, cum_reward: -2477.920539\n",
      "Episode 894, loss: 4.157265, iterations: 669494, cum_reward: -2460.704481\n",
      "Episode 895, loss: 4.535555, iterations: 669994, cum_reward: -2463.995625\n",
      "Episode 896, loss: 4.688731, iterations: 670494, cum_reward: -2474.611177\n",
      "Episode 897, loss: 4.148745, iterations: 670994, cum_reward: -2478.896516\n",
      "Episode 898, loss: 4.167927, iterations: 671494, cum_reward: -2471.118017\n",
      "Episode 899, loss: 4.709578, iterations: 671994, cum_reward: -2483.217527\n",
      "Episode 900, loss: 4.496259, iterations: 672494, cum_reward: -2481.471806\n",
      "Episode 901, loss: 4.403936, iterations: 672994, cum_reward: -2487.786490\n",
      "Episode 902, loss: 4.036007, iterations: 673494, cum_reward: -2475.661038\n",
      "Episode 903, loss: 4.436504, iterations: 673994, cum_reward: -2480.495898\n",
      "Episode 904, loss: 3.348526, iterations: 674494, cum_reward: -2470.804541\n",
      "Episode 905, loss: 4.329758, iterations: 674994, cum_reward: -2472.828777\n",
      "Episode 906, loss: 4.051497, iterations: 675494, cum_reward: -2474.583852\n",
      "Episode 907, loss: 3.793252, iterations: 675994, cum_reward: -2473.368739\n",
      "Episode 908, loss: 4.123434, iterations: 676494, cum_reward: -2481.195986\n",
      "Episode 909, loss: 4.385848, iterations: 676994, cum_reward: -2482.499022\n",
      "Episode 910, loss: 4.277080, iterations: 677494, cum_reward: -2472.730795\n",
      "Episode 911, loss: 4.440655, iterations: 677994, cum_reward: -2470.843555\n",
      "Episode 912, loss: 3.993873, iterations: 678494, cum_reward: -2472.379524\n",
      "Episode 913, loss: 3.628962, iterations: 678994, cum_reward: -2478.984030\n",
      "Episode 914, loss: 4.468981, iterations: 679494, cum_reward: -2473.460786\n",
      "Episode 915, loss: 4.423792, iterations: 679994, cum_reward: -2468.899709\n",
      "Episode 916, loss: 4.650284, iterations: 680494, cum_reward: -2481.077998\n",
      "Episode 917, loss: 4.410548, iterations: 680994, cum_reward: -2479.188096\n",
      "Episode 918, loss: 4.494729, iterations: 681494, cum_reward: -2483.473255\n",
      "Episode 919, loss: 4.148470, iterations: 681994, cum_reward: -2484.254482\n",
      "Episode 920, loss: 4.460007, iterations: 682494, cum_reward: -2474.436164\n",
      "Episode 921, loss: 4.124897, iterations: 682994, cum_reward: -2474.049889\n",
      "Episode 922, loss: 4.384713, iterations: 683494, cum_reward: -2456.950032\n",
      "Episode 923, loss: 4.274406, iterations: 683994, cum_reward: -2474.827598\n",
      "Episode 924, loss: 3.722100, iterations: 684494, cum_reward: -2478.523138\n",
      "Episode 925, loss: 3.636553, iterations: 684994, cum_reward: -2478.268288\n",
      "Episode 926, loss: 4.052220, iterations: 685494, cum_reward: -2472.675436\n",
      "Episode 927, loss: 4.502886, iterations: 685994, cum_reward: -2477.199736\n",
      "Episode 928, loss: 4.516340, iterations: 686494, cum_reward: -2476.492328\n",
      "Episode 929, loss: 4.960725, iterations: 686994, cum_reward: -2480.065943\n",
      "Episode 930, loss: 4.749886, iterations: 687494, cum_reward: -2476.310789\n",
      "Episode 931, loss: 4.434673, iterations: 687994, cum_reward: -2459.170408\n",
      "Episode 932, loss: 3.483438, iterations: 688494, cum_reward: -2476.943060\n",
      "Episode 933, loss: 4.676113, iterations: 688994, cum_reward: -2468.121680\n",
      "Episode 934, loss: 4.293211, iterations: 689494, cum_reward: -2479.855574\n",
      "Episode 935, loss: 4.339550, iterations: 689994, cum_reward: -2485.246730\n",
      "Episode 936, loss: 4.252016, iterations: 690494, cum_reward: -2476.954800\n",
      "Episode 937, loss: 4.329278, iterations: 690994, cum_reward: -2471.480566\n",
      "Episode 938, loss: 3.894174, iterations: 691494, cum_reward: -2473.370108\n",
      "Episode 939, loss: 4.283299, iterations: 691994, cum_reward: -2468.738580\n",
      "Episode 940, loss: 4.289400, iterations: 692494, cum_reward: -2473.363094\n",
      "Episode 941, loss: 4.510498, iterations: 692994, cum_reward: -2471.716118\n",
      "Episode 942, loss: 3.961296, iterations: 693494, cum_reward: -2469.723076\n",
      "Episode 943, loss: 4.150608, iterations: 693994, cum_reward: -2474.611773\n",
      "Episode 944, loss: 4.395163, iterations: 694494, cum_reward: -2474.835909\n",
      "Episode 945, loss: 4.581845, iterations: 694994, cum_reward: -2477.128089\n",
      "Episode 946, loss: 3.901149, iterations: 695494, cum_reward: -2469.204948\n",
      "Episode 947, loss: 4.459353, iterations: 695994, cum_reward: -2472.525620\n",
      "Episode 948, loss: 4.579286, iterations: 696494, cum_reward: -2474.477475\n",
      "Episode 949, loss: 4.036778, iterations: 696994, cum_reward: -2477.553194\n",
      "Episode 950, loss: 4.335092, iterations: 697494, cum_reward: -2472.828228\n",
      "Episode 951, loss: 4.556790, iterations: 697994, cum_reward: -2461.873804\n",
      "Episode 952, loss: 4.234327, iterations: 698494, cum_reward: -2483.158384\n",
      "Episode 953, loss: 3.041087, iterations: 698994, cum_reward: -2465.869971\n",
      "Episode 954, loss: 3.891129, iterations: 699494, cum_reward: -2477.431346\n",
      "Episode 955, loss: 4.614984, iterations: 699994, cum_reward: -2465.565024\n",
      "Episode 956, loss: 3.221742, iterations: 700494, cum_reward: -2488.834062\n",
      "Episode 957, loss: 3.612593, iterations: 700994, cum_reward: -2475.839268\n",
      "Episode 958, loss: 4.161773, iterations: 701494, cum_reward: -2473.103138\n",
      "Episode 959, loss: 4.631475, iterations: 701994, cum_reward: -2479.886284\n",
      "Episode 960, loss: 4.164639, iterations: 702494, cum_reward: -2470.951102\n",
      "Episode 961, loss: 4.437330, iterations: 702994, cum_reward: -2471.421033\n",
      "Episode 962, loss: 4.576886, iterations: 703494, cum_reward: -2461.762436\n",
      "Episode 963, loss: 2.983266, iterations: 703994, cum_reward: -2474.974045\n",
      "Episode 964, loss: 4.206801, iterations: 704494, cum_reward: -2475.707726\n",
      "Episode 965, loss: 4.174822, iterations: 704994, cum_reward: -2467.508064\n",
      "Episode 966, loss: 3.747550, iterations: 705494, cum_reward: -2480.643574\n",
      "Episode 967, loss: 4.209597, iterations: 705994, cum_reward: -2476.623658\n",
      "Episode 968, loss: 3.932619, iterations: 706494, cum_reward: -2467.712644\n",
      "Episode 969, loss: 4.341931, iterations: 706994, cum_reward: -2466.357022\n",
      "Episode 970, loss: 4.307253, iterations: 707494, cum_reward: -2470.006931\n",
      "Episode 971, loss: 4.383762, iterations: 707994, cum_reward: -2469.570958\n",
      "Episode 972, loss: 4.604899, iterations: 708494, cum_reward: -2485.486610\n",
      "Episode 973, loss: 4.351511, iterations: 708994, cum_reward: -2482.715392\n",
      "Episode 974, loss: 4.645018, iterations: 709494, cum_reward: -2459.081844\n",
      "Episode 975, loss: 4.702138, iterations: 709994, cum_reward: -2482.911270\n",
      "Episode 976, loss: 4.342302, iterations: 710494, cum_reward: -2468.675862\n",
      "Episode 977, loss: 3.309857, iterations: 710994, cum_reward: -2474.955017\n",
      "Episode 978, loss: 4.458313, iterations: 711494, cum_reward: -2458.619167\n",
      "Episode 979, loss: 4.473351, iterations: 711994, cum_reward: -2477.683473\n",
      "Episode 980, loss: 4.295534, iterations: 712494, cum_reward: -2475.207923\n",
      "Episode 981, loss: 3.470758, iterations: 712994, cum_reward: -2486.850411\n",
      "Episode 982, loss: 4.657975, iterations: 713494, cum_reward: -2478.619080\n",
      "Episode 983, loss: 4.383793, iterations: 713994, cum_reward: -2481.476925\n",
      "Episode 984, loss: 4.384986, iterations: 714494, cum_reward: -2464.288915\n",
      "Episode 985, loss: 4.505308, iterations: 714994, cum_reward: -2474.851738\n",
      "Episode 986, loss: 4.129553, iterations: 715494, cum_reward: -2484.507562\n",
      "Episode 987, loss: 4.396213, iterations: 715994, cum_reward: -2477.955406\n",
      "Episode 988, loss: 4.427049, iterations: 716494, cum_reward: -2472.760265\n",
      "Episode 989, loss: 4.372521, iterations: 716994, cum_reward: -2486.012596\n",
      "Episode 990, loss: 4.164510, iterations: 717494, cum_reward: -2481.301926\n",
      "Episode 991, loss: 4.834393, iterations: 717994, cum_reward: -2470.925642\n",
      "Episode 992, loss: 4.210302, iterations: 718494, cum_reward: -2480.536237\n",
      "Episode 993, loss: 3.488893, iterations: 718994, cum_reward: -2491.024423\n",
      "Episode 994, loss: 4.536413, iterations: 719494, cum_reward: -2468.291281\n",
      "Episode 995, loss: 3.457733, iterations: 719994, cum_reward: -2480.249665\n",
      "Episode 996, loss: 4.266970, iterations: 720494, cum_reward: -2456.620973\n",
      "Episode 997, loss: 3.816788, iterations: 720994, cum_reward: -2482.150860\n",
      "Episode 998, loss: 4.755260, iterations: 721494, cum_reward: -2481.894709\n",
      "Episode 999, loss: 4.783076, iterations: 721994, cum_reward: -2464.154786\n",
      "Episode 1000, loss: 4.442907, iterations: 722494, cum_reward: -2475.814390\n",
      "Episode 1001, loss: 4.083058, iterations: 722994, cum_reward: -2467.054046\n",
      "Episode 1002, loss: 4.616037, iterations: 723494, cum_reward: -2470.159399\n",
      "Episode 1003, loss: 4.157988, iterations: 723994, cum_reward: -2487.593845\n",
      "Episode 1004, loss: 4.409160, iterations: 724494, cum_reward: -2476.386070\n",
      "Episode 1005, loss: 4.011992, iterations: 724994, cum_reward: -2470.688774\n",
      "Episode 1006, loss: 4.543937, iterations: 725494, cum_reward: -2468.979607\n",
      "Episode 1007, loss: 4.162796, iterations: 725994, cum_reward: -2483.720033\n",
      "Episode 1008, loss: 4.457177, iterations: 726494, cum_reward: -2477.448070\n",
      "Episode 1009, loss: 4.793078, iterations: 726994, cum_reward: -2466.273213\n",
      "Episode 1010, loss: 3.579904, iterations: 727494, cum_reward: -2473.453175\n",
      "Episode 1011, loss: 4.548623, iterations: 727994, cum_reward: -2476.879581\n",
      "Episode 1012, loss: 4.198990, iterations: 728494, cum_reward: -2479.073355\n",
      "Episode 1013, loss: 4.421283, iterations: 728994, cum_reward: -2474.623154\n",
      "Episode 1014, loss: 4.040450, iterations: 729494, cum_reward: -2466.060117\n",
      "Episode 1015, loss: 4.434039, iterations: 729994, cum_reward: -2475.248225\n",
      "Episode 1016, loss: 3.486761, iterations: 730494, cum_reward: -2472.927334\n",
      "Episode 1017, loss: 3.929163, iterations: 730994, cum_reward: -2474.855224\n",
      "Episode 1018, loss: 4.424405, iterations: 731494, cum_reward: -2470.008002\n",
      "Episode 1019, loss: 4.685864, iterations: 731994, cum_reward: -2474.035695\n",
      "Episode 1020, loss: 3.046239, iterations: 732494, cum_reward: -2478.099839\n",
      "Episode 1021, loss: 3.816548, iterations: 732994, cum_reward: -2476.918619\n",
      "Episode 1022, loss: 4.518237, iterations: 733494, cum_reward: -2468.710893\n",
      "Episode 1023, loss: 4.487072, iterations: 733994, cum_reward: -2465.185217\n",
      "Episode 1024, loss: 3.770755, iterations: 734494, cum_reward: -2483.665235\n",
      "Episode 1025, loss: 3.677454, iterations: 734994, cum_reward: -2466.695487\n",
      "Episode 1026, loss: 3.401742, iterations: 735494, cum_reward: -2463.058149\n",
      "Episode 1027, loss: 4.200782, iterations: 735994, cum_reward: -2479.881488\n",
      "Episode 1028, loss: 3.882658, iterations: 736494, cum_reward: -2472.086297\n",
      "Episode 1029, loss: 3.817309, iterations: 736994, cum_reward: -2488.638944\n",
      "Episode 1030, loss: 4.551172, iterations: 737494, cum_reward: -2468.773813\n",
      "Episode 1031, loss: 4.432982, iterations: 737994, cum_reward: -2477.066701\n",
      "Episode 1032, loss: 4.226778, iterations: 738494, cum_reward: -2474.532225\n",
      "Episode 1033, loss: 4.206482, iterations: 738994, cum_reward: -2464.287023\n",
      "Episode 1034, loss: 4.634820, iterations: 739494, cum_reward: -2480.686961\n",
      "Episode 1035, loss: 4.650928, iterations: 739994, cum_reward: -2470.913005\n",
      "Episode 1036, loss: 3.814061, iterations: 740494, cum_reward: -2470.279273\n",
      "Episode 1037, loss: 4.614915, iterations: 740994, cum_reward: -2486.960132\n",
      "Episode 1038, loss: 4.695673, iterations: 741494, cum_reward: -2476.836298\n",
      "Episode 1039, loss: 4.458690, iterations: 741994, cum_reward: -2480.954034\n",
      "Episode 1040, loss: 4.275246, iterations: 742494, cum_reward: -2461.932337\n",
      "Episode 1041, loss: 4.724960, iterations: 742994, cum_reward: -2461.177324\n",
      "Episode 1042, loss: 3.927902, iterations: 743494, cum_reward: -2481.182937\n",
      "Episode 1043, loss: 4.379314, iterations: 743994, cum_reward: -2481.002505\n",
      "Episode 1044, loss: 4.204896, iterations: 744494, cum_reward: -2485.772973\n",
      "Episode 1045, loss: 3.917554, iterations: 744994, cum_reward: -2476.196010\n",
      "Episode 1046, loss: 3.467843, iterations: 745494, cum_reward: -2467.422319\n",
      "Episode 1047, loss: 3.758878, iterations: 745994, cum_reward: -2484.265843\n",
      "Episode 1048, loss: 4.134760, iterations: 746494, cum_reward: -2480.873948\n",
      "Episode 1049, loss: 4.063219, iterations: 746994, cum_reward: -2457.717315\n",
      "Episode 1050, loss: 3.861531, iterations: 747494, cum_reward: -2474.646030\n",
      "Episode 1051, loss: 4.610404, iterations: 747994, cum_reward: -2463.385021\n",
      "Episode 1052, loss: 4.270455, iterations: 748494, cum_reward: -2483.030226\n",
      "Episode 1053, loss: 4.443193, iterations: 748994, cum_reward: -2473.607014\n",
      "Episode 1054, loss: 3.988420, iterations: 749494, cum_reward: -2463.368304\n",
      "Episode 1055, loss: 4.381164, iterations: 749994, cum_reward: -2482.700001\n",
      "Episode 1056, loss: 3.876564, iterations: 750494, cum_reward: -2479.835705\n",
      "Episode 1057, loss: 4.139301, iterations: 750994, cum_reward: -2483.745922\n",
      "Episode 1058, loss: 4.173221, iterations: 751494, cum_reward: -2476.165500\n",
      "Episode 1059, loss: 3.987649, iterations: 751994, cum_reward: -2474.226994\n",
      "Episode 1060, loss: 3.902249, iterations: 752494, cum_reward: -2470.410999\n",
      "Episode 1061, loss: 4.124966, iterations: 752994, cum_reward: -2467.397966\n",
      "Episode 1062, loss: 4.283073, iterations: 753494, cum_reward: -2483.078240\n",
      "Episode 1063, loss: 4.555278, iterations: 753994, cum_reward: -2478.906375\n",
      "Episode 1064, loss: 4.292653, iterations: 754494, cum_reward: -2462.877855\n",
      "Episode 1065, loss: 4.423726, iterations: 754994, cum_reward: -2476.232641\n",
      "Episode 1066, loss: 4.128280, iterations: 755494, cum_reward: -2481.446839\n",
      "Episode 1067, loss: 3.923386, iterations: 755994, cum_reward: -2485.919123\n",
      "Episode 1068, loss: 4.224068, iterations: 756494, cum_reward: -2471.682786\n",
      "Episode 1069, loss: 4.469351, iterations: 756994, cum_reward: -2475.777168\n",
      "Episode 1070, loss: 4.202348, iterations: 757494, cum_reward: -2470.728199\n",
      "Episode 1071, loss: 4.394642, iterations: 757994, cum_reward: -2472.660664\n",
      "Episode 1072, loss: 4.558864, iterations: 758494, cum_reward: -2463.947135\n",
      "Episode 1073, loss: 4.642619, iterations: 758994, cum_reward: -2478.094756\n",
      "Episode 1074, loss: 3.739017, iterations: 759494, cum_reward: -2461.873311\n",
      "Episode 1075, loss: 4.208574, iterations: 759994, cum_reward: -2480.906112\n",
      "Episode 1076, loss: 3.985259, iterations: 760494, cum_reward: -2476.099678\n",
      "Episode 1077, loss: 4.568231, iterations: 760994, cum_reward: -2466.690257\n",
      "Episode 1078, loss: 4.596412, iterations: 761494, cum_reward: -2469.673788\n",
      "Episode 1079, loss: 4.397460, iterations: 761994, cum_reward: -2472.665764\n",
      "Episode 1080, loss: 3.517532, iterations: 762494, cum_reward: -2467.320846\n",
      "Episode 1081, loss: 4.581861, iterations: 762994, cum_reward: -2470.404824\n",
      "Episode 1082, loss: 4.555050, iterations: 763494, cum_reward: -2456.721606\n",
      "Episode 1083, loss: 4.769773, iterations: 763994, cum_reward: -2476.475546\n",
      "Episode 1084, loss: 3.731368, iterations: 764494, cum_reward: -2481.789852\n",
      "Episode 1085, loss: 3.990844, iterations: 764994, cum_reward: -2487.323376\n",
      "Episode 1086, loss: 4.553148, iterations: 765494, cum_reward: -2475.431510\n",
      "Episode 1087, loss: 3.578458, iterations: 765994, cum_reward: -2479.247104\n",
      "Episode 1088, loss: 4.447835, iterations: 766494, cum_reward: -2466.668020\n",
      "Episode 1089, loss: 3.982765, iterations: 766994, cum_reward: -2487.996013\n",
      "Episode 1090, loss: 4.746406, iterations: 767494, cum_reward: -2475.858213\n",
      "Episode 1091, loss: 4.214994, iterations: 767994, cum_reward: -2465.412610\n",
      "Episode 1092, loss: 3.905193, iterations: 768494, cum_reward: -2468.232528\n",
      "Episode 1093, loss: 4.596511, iterations: 768994, cum_reward: -2471.516588\n",
      "Episode 1094, loss: 4.679103, iterations: 769494, cum_reward: -2485.473730\n",
      "Episode 1095, loss: 4.150516, iterations: 769994, cum_reward: -2458.361785\n",
      "Episode 1096, loss: 4.302494, iterations: 770494, cum_reward: -2479.371869\n",
      "Episode 1097, loss: 4.652602, iterations: 770994, cum_reward: -2481.983860\n",
      "Episode 1098, loss: 4.187364, iterations: 771494, cum_reward: -2479.110467\n",
      "Episode 1099, loss: 4.507547, iterations: 771994, cum_reward: -2473.741560\n",
      "Episode 1100, loss: 3.953816, iterations: 772494, cum_reward: -2470.823004\n",
      "Episode 1101, loss: 4.478298, iterations: 772994, cum_reward: -2471.270281\n",
      "Episode 1102, loss: 4.268729, iterations: 773494, cum_reward: -2487.721706\n",
      "Episode 1103, loss: 4.468273, iterations: 773994, cum_reward: -2481.081266\n",
      "Episode 1104, loss: 4.426707, iterations: 774494, cum_reward: -2478.092366\n",
      "Episode 1105, loss: 4.592867, iterations: 774994, cum_reward: -2479.049531\n",
      "Episode 1106, loss: 4.341464, iterations: 775494, cum_reward: -2472.849878\n",
      "Episode 1107, loss: 4.414793, iterations: 775994, cum_reward: -2472.685631\n",
      "Episode 1108, loss: 4.580808, iterations: 776494, cum_reward: -2472.557866\n",
      "Episode 1109, loss: 4.451753, iterations: 776994, cum_reward: -2463.105723\n",
      "Episode 1110, loss: 4.085712, iterations: 777494, cum_reward: -2481.214196\n",
      "Episode 1111, loss: 4.656227, iterations: 777994, cum_reward: -2460.810398\n",
      "Episode 1112, loss: 4.338482, iterations: 778494, cum_reward: -2488.333620\n",
      "Episode 1113, loss: 4.576345, iterations: 778994, cum_reward: -2478.389762\n",
      "Episode 1114, loss: 4.403056, iterations: 779494, cum_reward: -2474.379601\n",
      "Episode 1115, loss: 4.220377, iterations: 779994, cum_reward: -2484.993834\n",
      "Episode 1116, loss: 4.451984, iterations: 780494, cum_reward: -2476.493970\n",
      "Episode 1117, loss: 4.462814, iterations: 780994, cum_reward: -2480.158061\n",
      "Episode 1118, loss: 4.427487, iterations: 781494, cum_reward: -2491.732001\n",
      "Episode 1119, loss: 4.538795, iterations: 781994, cum_reward: -2474.733933\n",
      "Episode 1120, loss: 4.443069, iterations: 782494, cum_reward: -2477.871148\n",
      "Episode 1121, loss: 4.147181, iterations: 782994, cum_reward: -2469.296503\n",
      "Episode 1122, loss: 1.896882, iterations: 783494, cum_reward: -2462.660527\n",
      "Episode 1123, loss: 3.366934, iterations: 783994, cum_reward: -2476.969905\n",
      "Episode 1124, loss: 4.470691, iterations: 784494, cum_reward: -2478.194555\n",
      "Episode 1125, loss: 4.610848, iterations: 784994, cum_reward: -2481.076273\n",
      "Episode 1126, loss: 4.667110, iterations: 785494, cum_reward: -2465.809824\n",
      "Episode 1141, loss: 4.313318, iterations: 792994, cum_reward: -2475.500964\n",
      "Episode 1142, loss: 4.288353, iterations: 793494, cum_reward: -2462.528526\n",
      "Episode 1143, loss: 4.113539, iterations: 793994, cum_reward: -2468.184252\n",
      "Episode 1144, loss: 4.556311, iterations: 794494, cum_reward: -2465.969615\n",
      "Episode 1145, loss: 3.807099, iterations: 794994, cum_reward: -2478.613004\n",
      "Episode 1146, loss: 2.831498, iterations: 795494, cum_reward: -2476.925261\n",
      "Episode 1147, loss: 3.620156, iterations: 795994, cum_reward: -2472.338775\n",
      "Episode 1148, loss: 3.549833, iterations: 796494, cum_reward: -2479.065542\n",
      "Episode 1149, loss: 4.578094, iterations: 796994, cum_reward: -2477.077037\n",
      "Episode 1150, loss: 3.541051, iterations: 797494, cum_reward: -2484.866549\n",
      "Episode 1151, loss: 4.350101, iterations: 797994, cum_reward: -2455.290067\n",
      "Episode 1152, loss: 4.834348, iterations: 798494, cum_reward: -2467.490830\n",
      "Episode 1153, loss: 4.698283, iterations: 798994, cum_reward: -2487.118234\n",
      "Episode 1154, loss: 4.822785, iterations: 799494, cum_reward: -2474.471348\n",
      "Episode 1155, loss: 4.310065, iterations: 799994, cum_reward: -2486.187583\n",
      "Episode 1156, loss: 4.520869, iterations: 800494, cum_reward: -2484.804962\n",
      "Episode 1157, loss: 4.504169, iterations: 800994, cum_reward: -2468.889227\n",
      "Episode 1158, loss: 3.960735, iterations: 801494, cum_reward: -2475.480658\n",
      "Episode 1159, loss: 4.374067, iterations: 801994, cum_reward: -2479.503669\n",
      "Episode 1160, loss: 4.477768, iterations: 802494, cum_reward: -2482.258727\n",
      "Episode 1161, loss: 4.411957, iterations: 802994, cum_reward: -2484.456347\n",
      "Episode 1162, loss: 4.117138, iterations: 803494, cum_reward: -2475.333192\n",
      "Episode 1163, loss: 4.325985, iterations: 803994, cum_reward: -2486.294324\n",
      "Episode 1164, loss: 3.833098, iterations: 804494, cum_reward: -2468.595178\n",
      "Episode 1165, loss: 4.643252, iterations: 804994, cum_reward: -2490.742077\n",
      "Episode 1166, loss: 4.706778, iterations: 805494, cum_reward: -2472.412425\n",
      "Episode 1167, loss: 4.534431, iterations: 805994, cum_reward: -2451.386800\n",
      "Episode 1168, loss: 4.162563, iterations: 806494, cum_reward: -2459.906364\n",
      "Episode 1169, loss: 4.053217, iterations: 806994, cum_reward: -2490.252550\n",
      "Episode 1170, loss: 3.867122, iterations: 807494, cum_reward: -2474.912873\n",
      "Episode 1171, loss: 4.428707, iterations: 807994, cum_reward: -2470.594510\n",
      "Episode 1172, loss: 4.255102, iterations: 808494, cum_reward: -2470.114224\n",
      "Episode 1173, loss: 4.370625, iterations: 808994, cum_reward: -2480.108725\n",
      "Episode 1174, loss: 4.386540, iterations: 809494, cum_reward: -2480.119710\n",
      "Episode 1175, loss: 4.227435, iterations: 809994, cum_reward: -2470.215579\n",
      "Episode 1176, loss: 3.779768, iterations: 810494, cum_reward: -2460.583400\n",
      "Episode 1177, loss: 4.661268, iterations: 810994, cum_reward: -2473.274124\n",
      "Episode 1178, loss: 3.941404, iterations: 811494, cum_reward: -2478.988493\n",
      "Episode 1179, loss: 4.686461, iterations: 811994, cum_reward: -2477.500288\n",
      "Episode 1180, loss: 3.776671, iterations: 812494, cum_reward: -2478.629970\n",
      "Episode 1181, loss: 3.823441, iterations: 812994, cum_reward: -2471.694260\n",
      "Episode 1182, loss: 4.237162, iterations: 813494, cum_reward: -2471.678799\n",
      "Episode 1183, loss: 4.426119, iterations: 813994, cum_reward: -2471.419238\n",
      "Episode 1184, loss: 4.129755, iterations: 814494, cum_reward: -2478.080017\n",
      "Episode 1185, loss: 4.541460, iterations: 814994, cum_reward: -2479.897937\n",
      "Episode 1186, loss: 4.411616, iterations: 815494, cum_reward: -2474.501715\n",
      "Episode 1187, loss: 3.654299, iterations: 815994, cum_reward: -2486.157184\n",
      "Episode 1188, loss: 4.226373, iterations: 816494, cum_reward: -2459.784126\n",
      "Episode 1189, loss: 4.291246, iterations: 816994, cum_reward: -2470.732483\n",
      "Episode 1190, loss: 4.349261, iterations: 817494, cum_reward: -2473.077648\n",
      "Episode 1191, loss: 4.368718, iterations: 817994, cum_reward: -2482.773315\n",
      "Episode 1192, loss: 4.379476, iterations: 818494, cum_reward: -2473.214440\n",
      "Episode 1193, loss: 4.533761, iterations: 818994, cum_reward: -2465.475226\n",
      "Episode 1194, loss: 3.922724, iterations: 819494, cum_reward: -2484.948583\n",
      "Episode 1195, loss: 4.461782, iterations: 819994, cum_reward: -2485.590465\n",
      "Episode 1196, loss: 3.934161, iterations: 820494, cum_reward: -2486.375220\n",
      "Episode 1197, loss: 3.964927, iterations: 820994, cum_reward: -2475.665091\n",
      "Episode 1198, loss: 3.955617, iterations: 821494, cum_reward: -2471.179615\n",
      "Episode 1199, loss: 3.917886, iterations: 821994, cum_reward: -2478.821479\n",
      "Episode 1200, loss: 4.163585, iterations: 822494, cum_reward: -2487.244606\n",
      "Episode 1201, loss: 4.658849, iterations: 822994, cum_reward: -2471.499138\n",
      "Episode 1202, loss: 4.609982, iterations: 823494, cum_reward: -2487.664391\n",
      "Episode 1203, loss: 4.688675, iterations: 823994, cum_reward: -2477.984957\n",
      "Episode 1204, loss: 4.303388, iterations: 824494, cum_reward: -2482.239723\n",
      "Episode 1205, loss: 4.686685, iterations: 824994, cum_reward: -2480.646105\n",
      "Episode 1206, loss: 4.363153, iterations: 825494, cum_reward: -2478.278786\n",
      "Episode 1207, loss: 4.565331, iterations: 825994, cum_reward: -2482.858012\n",
      "Episode 1208, loss: 4.424648, iterations: 826494, cum_reward: -2475.321370\n",
      "Episode 1209, loss: 4.409100, iterations: 826994, cum_reward: -2478.129790\n",
      "Episode 1210, loss: 4.667283, iterations: 827494, cum_reward: -2473.842494\n",
      "Episode 1211, loss: 4.319305, iterations: 827994, cum_reward: -2470.965475\n",
      "Episode 1212, loss: 3.863261, iterations: 828494, cum_reward: -2470.788188\n",
      "Episode 1213, loss: 4.515698, iterations: 828994, cum_reward: -2479.538419\n",
      "Episode 1214, loss: 4.645312, iterations: 829494, cum_reward: -2482.513172\n",
      "Episode 1215, loss: 4.143573, iterations: 829994, cum_reward: -2480.702857\n",
      "Episode 1216, loss: 4.338918, iterations: 830494, cum_reward: -2479.937174\n",
      "Episode 1217, loss: 4.542555, iterations: 830994, cum_reward: -2483.934610\n",
      "Episode 1218, loss: 4.538338, iterations: 831494, cum_reward: -2471.088273\n",
      "Episode 1219, loss: 4.624297, iterations: 831994, cum_reward: -2485.060281\n",
      "Episode 1220, loss: 3.735638, iterations: 832494, cum_reward: -2485.592221\n",
      "Episode 1221, loss: 3.440135, iterations: 832994, cum_reward: -2478.198515\n",
      "Episode 1222, loss: 3.470824, iterations: 833494, cum_reward: -2476.966890\n",
      "Episode 1223, loss: 3.750519, iterations: 833994, cum_reward: -2466.165458\n",
      "Episode 1224, loss: 4.200093, iterations: 834494, cum_reward: -2476.952979\n",
      "Episode 1225, loss: 3.387315, iterations: 834994, cum_reward: -2456.151625\n",
      "Episode 1226, loss: 4.568984, iterations: 835494, cum_reward: -2470.491718\n",
      "Episode 1227, loss: 4.654262, iterations: 835994, cum_reward: -2476.496844\n",
      "Episode 1228, loss: 3.715159, iterations: 836494, cum_reward: -2470.641360\n",
      "Episode 1229, loss: 4.460586, iterations: 836994, cum_reward: -2471.156576\n",
      "Episode 1230, loss: 4.274644, iterations: 837494, cum_reward: -2475.114312\n",
      "Episode 1231, loss: 4.413530, iterations: 837994, cum_reward: -2464.804359\n",
      "Episode 1232, loss: 4.589054, iterations: 838494, cum_reward: -2480.082088\n",
      "Episode 1233, loss: 4.533694, iterations: 838994, cum_reward: -2479.178755\n",
      "Episode 1234, loss: 4.371273, iterations: 839494, cum_reward: -2474.569873\n",
      "Episode 1235, loss: 4.561759, iterations: 839994, cum_reward: -2465.382452\n",
      "Episode 1236, loss: 4.274693, iterations: 840494, cum_reward: -2470.621427\n",
      "Episode 1237, loss: 4.207818, iterations: 840994, cum_reward: -2484.018216\n",
      "Episode 1238, loss: 4.568099, iterations: 841494, cum_reward: -2480.589232\n",
      "Episode 1239, loss: 4.543428, iterations: 841994, cum_reward: -2476.353105\n",
      "Episode 1240, loss: 3.824779, iterations: 842494, cum_reward: -2477.320839\n",
      "Episode 1241, loss: 3.646893, iterations: 842994, cum_reward: -2475.217914\n",
      "Episode 1242, loss: 4.033386, iterations: 843494, cum_reward: -2475.167036\n",
      "Episode 1243, loss: 4.521334, iterations: 843994, cum_reward: -2467.847394\n",
      "Episode 1244, loss: 4.672700, iterations: 844494, cum_reward: -2480.262264\n",
      "Episode 1245, loss: 4.697774, iterations: 844994, cum_reward: -2467.884492\n",
      "Episode 1246, loss: 3.740253, iterations: 845494, cum_reward: -2465.333349\n",
      "Episode 1247, loss: 4.368887, iterations: 845994, cum_reward: -2469.992648\n",
      "Episode 1248, loss: 4.924101, iterations: 846494, cum_reward: -2490.084377\n",
      "Episode 1249, loss: 4.470467, iterations: 846994, cum_reward: -2482.256754\n",
      "Episode 1250, loss: 4.052908, iterations: 847494, cum_reward: -2479.591777\n",
      "Episode 1251, loss: 3.238444, iterations: 847994, cum_reward: -2492.717498\n",
      "Episode 1252, loss: 4.342492, iterations: 848494, cum_reward: -2469.452761\n",
      "Episode 1253, loss: 3.834533, iterations: 848994, cum_reward: -2470.869716\n",
      "Episode 1254, loss: 3.693699, iterations: 849494, cum_reward: -2478.583544\n",
      "Episode 1255, loss: 4.366145, iterations: 849994, cum_reward: -2480.356520\n",
      "Episode 1256, loss: 4.548275, iterations: 850494, cum_reward: -2480.435580\n",
      "Episode 1257, loss: 4.403224, iterations: 850994, cum_reward: -2486.891935\n",
      "Episode 1258, loss: 3.813244, iterations: 851494, cum_reward: -2480.085216\n",
      "Episode 1259, loss: 4.496797, iterations: 851994, cum_reward: -2459.716626\n",
      "Episode 1260, loss: 4.379144, iterations: 852494, cum_reward: -2473.129038\n",
      "Episode 1261, loss: 4.797567, iterations: 852994, cum_reward: -2478.068758\n",
      "Episode 1262, loss: 4.335135, iterations: 853494, cum_reward: -2475.522324\n",
      "Episode 1263, loss: 4.354022, iterations: 853994, cum_reward: -2472.184256\n",
      "Episode 1264, loss: 2.572893, iterations: 854494, cum_reward: -2464.446047\n",
      "Episode 1265, loss: 4.513927, iterations: 854994, cum_reward: -2469.168795\n",
      "Episode 1266, loss: 4.443616, iterations: 855494, cum_reward: -2473.021632\n",
      "Episode 1267, loss: 4.809242, iterations: 855994, cum_reward: -2471.017326\n",
      "Episode 1268, loss: 4.287601, iterations: 856494, cum_reward: -2478.709174\n",
      "Episode 1269, loss: 4.249864, iterations: 856994, cum_reward: -2475.538720\n",
      "Episode 1270, loss: 3.705465, iterations: 857494, cum_reward: -2470.044175\n",
      "Episode 1271, loss: 4.042226, iterations: 857994, cum_reward: -2480.970600\n",
      "Episode 1272, loss: 4.569481, iterations: 858494, cum_reward: -2469.515569\n",
      "Episode 1273, loss: 4.658283, iterations: 858994, cum_reward: -2474.169999\n",
      "Episode 1274, loss: 3.078674, iterations: 859494, cum_reward: -2452.550519\n",
      "Episode 1275, loss: 4.204953, iterations: 859994, cum_reward: -2480.735437\n",
      "Episode 1276, loss: 3.502252, iterations: 860494, cum_reward: -2467.070826\n",
      "Episode 1277, loss: 4.284122, iterations: 860994, cum_reward: -2477.046464\n",
      "Episode 1278, loss: 3.922259, iterations: 861494, cum_reward: -2463.134831\n",
      "Episode 1279, loss: 4.579248, iterations: 861994, cum_reward: -2477.651717\n",
      "Episode 1280, loss: 3.902586, iterations: 862494, cum_reward: -2467.682507\n",
      "Episode 1281, loss: 4.192938, iterations: 862994, cum_reward: -2479.242103\n",
      "Episode 1282, loss: 4.490649, iterations: 863494, cum_reward: -2469.561324\n",
      "Episode 1283, loss: 4.043187, iterations: 863994, cum_reward: -2493.702128\n",
      "Episode 1284, loss: 3.710567, iterations: 864494, cum_reward: -2462.376361\n",
      "Episode 1285, loss: 3.878782, iterations: 864994, cum_reward: -2459.159488\n",
      "Episode 1286, loss: 4.567589, iterations: 865494, cum_reward: -2467.565737\n",
      "Episode 1287, loss: 4.411195, iterations: 865994, cum_reward: -2464.259925\n",
      "Episode 1288, loss: 3.862523, iterations: 866494, cum_reward: -2483.709476\n",
      "Episode 1289, loss: 4.477606, iterations: 866994, cum_reward: -2472.637239\n",
      "Episode 1290, loss: 4.416281, iterations: 867494, cum_reward: -2480.326501\n",
      "Episode 1291, loss: 4.709533, iterations: 867994, cum_reward: -2473.974120\n",
      "Episode 1292, loss: 3.931735, iterations: 868494, cum_reward: -2485.688026\n",
      "Episode 1293, loss: 4.715106, iterations: 868994, cum_reward: -2473.364507\n",
      "Episode 1294, loss: 4.352115, iterations: 869494, cum_reward: -2477.723645\n",
      "Episode 1295, loss: 4.449579, iterations: 869994, cum_reward: -2466.107798\n",
      "Episode 1296, loss: 3.928404, iterations: 870494, cum_reward: -2475.854887\n",
      "Episode 1297, loss: 4.178112, iterations: 870994, cum_reward: -2470.597558\n",
      "Episode 1298, loss: 4.611217, iterations: 871494, cum_reward: -2487.019876\n",
      "Episode 1299, loss: 4.022604, iterations: 871994, cum_reward: -2472.286593\n",
      "Episode 1300, loss: 3.558826, iterations: 872494, cum_reward: -2478.675786\n",
      "Episode 1301, loss: 4.413977, iterations: 872994, cum_reward: -2469.443640\n",
      "Episode 1302, loss: 4.648033, iterations: 873494, cum_reward: -2477.818815\n",
      "Episode 1303, loss: 4.135132, iterations: 873994, cum_reward: -2477.018018\n",
      "Episode 1304, loss: 4.658062, iterations: 874494, cum_reward: -2481.933723\n",
      "Episode 1305, loss: 3.028428, iterations: 874994, cum_reward: -2466.870377\n",
      "Episode 1306, loss: 2.261482, iterations: 875494, cum_reward: -2473.841328\n",
      "Episode 1307, loss: 3.626040, iterations: 875994, cum_reward: -2482.148563\n",
      "Episode 1308, loss: 4.170883, iterations: 876494, cum_reward: -2480.671354\n",
      "Episode 1309, loss: 3.559052, iterations: 876994, cum_reward: -2464.158457\n",
      "Episode 1310, loss: 4.454825, iterations: 877494, cum_reward: -2468.547857\n",
      "Episode 1311, loss: 3.330954, iterations: 877994, cum_reward: -2474.761055\n",
      "Episode 1312, loss: 4.137419, iterations: 878494, cum_reward: -2476.321075\n",
      "Episode 1313, loss: 3.652737, iterations: 878994, cum_reward: -2475.926439\n",
      "Episode 1314, loss: 4.240644, iterations: 879494, cum_reward: -2475.139180\n",
      "Episode 1315, loss: 4.117910, iterations: 879994, cum_reward: -2474.891766\n",
      "Episode 1316, loss: 3.493269, iterations: 880494, cum_reward: -2463.436012\n",
      "Episode 1317, loss: 4.742699, iterations: 880994, cum_reward: -2458.909522\n",
      "Episode 1318, loss: 3.690709, iterations: 881494, cum_reward: -2468.850180\n",
      "Episode 1319, loss: 4.704929, iterations: 881994, cum_reward: -2474.216611\n",
      "Episode 1320, loss: 4.062425, iterations: 882494, cum_reward: -2474.388811\n",
      "Episode 1321, loss: 4.139769, iterations: 882994, cum_reward: -2469.124820\n",
      "Episode 1322, loss: 4.541460, iterations: 883494, cum_reward: -2490.909064\n",
      "Episode 1323, loss: 4.090232, iterations: 883994, cum_reward: -2468.862155\n",
      "Episode 1324, loss: 4.361110, iterations: 884494, cum_reward: -2474.591078\n",
      "Episode 1325, loss: 3.905648, iterations: 884994, cum_reward: -2475.194066\n",
      "Episode 1326, loss: 3.941694, iterations: 885494, cum_reward: -2480.789133\n",
      "Episode 1327, loss: 3.912274, iterations: 885994, cum_reward: -2464.517530\n",
      "Episode 1328, loss: 3.920003, iterations: 886494, cum_reward: -2477.776227\n",
      "Episode 1329, loss: 4.007940, iterations: 886994, cum_reward: -2468.265401\n",
      "Episode 1330, loss: 4.412212, iterations: 887494, cum_reward: -2456.189271\n",
      "Episode 1331, loss: 3.851326, iterations: 887994, cum_reward: -2470.678226\n",
      "Episode 1332, loss: 4.205465, iterations: 888494, cum_reward: -2474.114842\n",
      "Episode 1333, loss: 4.486856, iterations: 888994, cum_reward: -2475.121897\n",
      "Episode 1334, loss: 4.289103, iterations: 889494, cum_reward: -2498.104433\n",
      "Episode 1335, loss: 4.628550, iterations: 889994, cum_reward: -2470.371837\n",
      "Episode 1336, loss: 4.481899, iterations: 890494, cum_reward: -2473.715702\n",
      "Episode 1337, loss: 4.515045, iterations: 890994, cum_reward: -2480.193945\n",
      "Episode 1338, loss: 3.506677, iterations: 891494, cum_reward: -2472.396480\n",
      "Episode 1339, loss: 4.244261, iterations: 891994, cum_reward: -2466.576609\n",
      "Episode 1340, loss: 2.858471, iterations: 892494, cum_reward: -2487.404819\n",
      "Episode 1341, loss: 4.572478, iterations: 892994, cum_reward: -2477.177403\n",
      "Episode 1342, loss: 3.972886, iterations: 893494, cum_reward: -2477.202223\n",
      "Episode 1343, loss: 4.341653, iterations: 893994, cum_reward: -2468.063754\n",
      "Episode 1344, loss: 4.607646, iterations: 894494, cum_reward: -2475.173826\n",
      "Episode 1345, loss: 4.355465, iterations: 894994, cum_reward: -2488.602902\n",
      "Episode 1346, loss: 4.527896, iterations: 895494, cum_reward: -2466.117177\n",
      "Episode 1347, loss: 4.469075, iterations: 895994, cum_reward: -2475.231777\n",
      "Episode 1348, loss: 4.444670, iterations: 896494, cum_reward: -2468.018629\n",
      "Episode 1349, loss: 4.428974, iterations: 896994, cum_reward: -2472.223157\n",
      "Episode 1350, loss: 3.399299, iterations: 897494, cum_reward: -2468.407814\n",
      "Episode 1351, loss: 3.852731, iterations: 897994, cum_reward: -2473.075691\n",
      "Episode 1352, loss: 4.540312, iterations: 898494, cum_reward: -2473.612565\n",
      "Episode 1353, loss: 4.268076, iterations: 898994, cum_reward: -2466.198380\n",
      "Episode 1354, loss: 4.247018, iterations: 899494, cum_reward: -2468.611066\n",
      "Episode 1355, loss: 4.570146, iterations: 899994, cum_reward: -2463.967553\n",
      "Episode 1356, loss: 4.111883, iterations: 900494, cum_reward: -2463.198514\n",
      "Episode 1357, loss: 4.336884, iterations: 900994, cum_reward: -2473.880301\n",
      "Episode 1358, loss: 4.549165, iterations: 901494, cum_reward: -2485.875906\n",
      "Episode 1359, loss: 4.347734, iterations: 901994, cum_reward: -2475.029377\n",
      "Episode 1360, loss: 4.429345, iterations: 902494, cum_reward: -2485.914386\n",
      "Episode 1361, loss: 4.309175, iterations: 902994, cum_reward: -2469.819771\n",
      "Episode 1362, loss: 4.092673, iterations: 903494, cum_reward: -2486.996377\n",
      "Episode 1363, loss: 4.443044, iterations: 903994, cum_reward: -2466.381272\n",
      "Episode 1364, loss: 4.366841, iterations: 904494, cum_reward: -2485.368414\n",
      "Episode 1365, loss: 4.289250, iterations: 904994, cum_reward: -2477.590255\n",
      "Episode 1366, loss: 4.050055, iterations: 905494, cum_reward: -2480.851632\n",
      "Episode 1367, loss: 4.112852, iterations: 905994, cum_reward: -2481.732202\n",
      "Episode 1368, loss: 4.625519, iterations: 906494, cum_reward: -2477.244038\n",
      "Episode 1369, loss: 4.350527, iterations: 906994, cum_reward: -2489.506927\n",
      "Episode 1370, loss: 4.545511, iterations: 907494, cum_reward: -2474.778377\n",
      "Episode 1371, loss: 4.535746, iterations: 907994, cum_reward: -2470.263326\n",
      "Episode 1372, loss: 4.165559, iterations: 908494, cum_reward: -2480.184033\n",
      "Episode 1373, loss: 4.482071, iterations: 908994, cum_reward: -2473.123903\n",
      "Episode 1374, loss: 4.244687, iterations: 909494, cum_reward: -2471.094385\n",
      "Episode 1375, loss: 3.921678, iterations: 909994, cum_reward: -2462.507277\n",
      "Episode 1376, loss: 4.002826, iterations: 910494, cum_reward: -2472.364474\n",
      "Episode 1377, loss: 3.197483, iterations: 910994, cum_reward: -2472.217633\n",
      "Episode 1378, loss: 3.367905, iterations: 911494, cum_reward: -2467.407415\n",
      "Episode 1379, loss: 3.412005, iterations: 911994, cum_reward: -2479.142417\n",
      "Episode 1380, loss: 4.220112, iterations: 912494, cum_reward: -2467.804585\n",
      "Episode 1381, loss: 3.947816, iterations: 912994, cum_reward: -2469.610153\n",
      "Episode 1382, loss: 4.526469, iterations: 913494, cum_reward: -2475.886448\n",
      "Episode 1383, loss: 4.616324, iterations: 913994, cum_reward: -2484.926159\n",
      "Episode 1384, loss: 4.315330, iterations: 914494, cum_reward: -2477.947732\n",
      "Episode 1385, loss: 4.134473, iterations: 914994, cum_reward: -2468.551547\n",
      "Episode 1386, loss: 4.642350, iterations: 915494, cum_reward: -2484.247973\n",
      "Episode 1387, loss: 4.477573, iterations: 915994, cum_reward: -2479.593200\n",
      "Episode 1388, loss: 3.986569, iterations: 916494, cum_reward: -2485.734637\n",
      "Episode 1389, loss: 4.381153, iterations: 916994, cum_reward: -2485.687270\n",
      "Episode 1390, loss: 4.674558, iterations: 917494, cum_reward: -2471.275704\n",
      "Episode 1391, loss: 4.417972, iterations: 917994, cum_reward: -2457.264468\n",
      "Episode 1392, loss: 4.148809, iterations: 918494, cum_reward: -2478.000150\n",
      "Episode 1393, loss: 3.917343, iterations: 918994, cum_reward: -2464.393909\n",
      "Episode 1394, loss: 4.483258, iterations: 919494, cum_reward: -2475.018871\n",
      "Episode 1395, loss: 4.214642, iterations: 919994, cum_reward: -2473.177196\n",
      "Episode 1396, loss: 4.406208, iterations: 920494, cum_reward: -2473.607331\n",
      "Episode 1397, loss: 4.315820, iterations: 920994, cum_reward: -2469.548656\n",
      "Episode 1398, loss: 4.636927, iterations: 921494, cum_reward: -2484.662627\n",
      "Episode 1399, loss: 4.138412, iterations: 921994, cum_reward: -2477.099294\n",
      "Episode 1400, loss: 4.024600, iterations: 922494, cum_reward: -2461.931123\n",
      "Episode 1401, loss: 4.192832, iterations: 922994, cum_reward: -2465.081149\n",
      "Episode 1402, loss: 4.227096, iterations: 923494, cum_reward: -2473.356467\n",
      "Episode 1403, loss: 4.624927, iterations: 923994, cum_reward: -2476.180358\n",
      "Episode 1404, loss: 4.039207, iterations: 924494, cum_reward: -2487.405242\n",
      "Episode 1405, loss: 3.401881, iterations: 924994, cum_reward: -2480.317787\n",
      "Episode 1406, loss: 4.013342, iterations: 925494, cum_reward: -2476.938864\n",
      "Episode 1407, loss: 4.604511, iterations: 925994, cum_reward: -2491.019757\n",
      "Episode 1408, loss: 4.611448, iterations: 926494, cum_reward: -2474.207678\n",
      "Episode 1409, loss: 4.094774, iterations: 926994, cum_reward: -2481.014799\n",
      "Episode 1410, loss: 4.480819, iterations: 927494, cum_reward: -2468.200744\n",
      "Episode 1411, loss: 4.084992, iterations: 927994, cum_reward: -2473.744068\n",
      "Episode 1412, loss: 3.941584, iterations: 928494, cum_reward: -2458.819432\n",
      "Episode 1413, loss: 4.699045, iterations: 928994, cum_reward: -2471.531486\n",
      "Episode 1414, loss: 4.437796, iterations: 929494, cum_reward: -2473.123329\n",
      "Episode 1415, loss: 4.071389, iterations: 929994, cum_reward: -2466.259264\n",
      "Episode 1416, loss: 4.295230, iterations: 930494, cum_reward: -2479.874991\n",
      "Episode 1417, loss: 4.004183, iterations: 930994, cum_reward: -2479.116020\n",
      "Episode 1418, loss: 4.438019, iterations: 931494, cum_reward: -2483.511551\n",
      "Episode 1419, loss: 4.735483, iterations: 931994, cum_reward: -2477.750416\n",
      "Episode 1420, loss: 4.353255, iterations: 932494, cum_reward: -2477.448620\n",
      "Episode 1421, loss: 4.053043, iterations: 932994, cum_reward: -2465.569652\n",
      "Episode 1422, loss: 3.181422, iterations: 933494, cum_reward: -2475.936789\n",
      "Episode 1423, loss: 4.063254, iterations: 933994, cum_reward: -2468.831485\n",
      "Episode 1424, loss: 4.326258, iterations: 934494, cum_reward: -2484.158136\n",
      "Episode 1425, loss: 4.434028, iterations: 934994, cum_reward: -2475.489685\n",
      "Episode 1426, loss: 4.358744, iterations: 935494, cum_reward: -2473.416371\n",
      "Episode 1427, loss: 4.149405, iterations: 935994, cum_reward: -2473.333265\n",
      "Episode 1428, loss: 4.556201, iterations: 936494, cum_reward: -2470.387831\n",
      "Episode 1429, loss: 3.113877, iterations: 936994, cum_reward: -2466.637272\n",
      "Episode 1430, loss: 4.307149, iterations: 937494, cum_reward: -2470.439908\n",
      "Episode 1431, loss: 4.054522, iterations: 937994, cum_reward: -2475.473516\n",
      "Episode 1432, loss: 4.262185, iterations: 938494, cum_reward: -2453.609837\n",
      "Episode 1433, loss: 4.106877, iterations: 938994, cum_reward: -2475.640520\n",
      "Episode 1434, loss: 3.986432, iterations: 939494, cum_reward: -2480.297973\n",
      "Episode 1435, loss: 4.240811, iterations: 939994, cum_reward: -2463.564063\n",
      "Episode 1436, loss: 4.092944, iterations: 940494, cum_reward: -2472.332172\n",
      "Episode 1437, loss: 3.846371, iterations: 940994, cum_reward: -2475.200464\n",
      "Episode 1438, loss: 4.510939, iterations: 941494, cum_reward: -2470.879136\n",
      "Episode 1439, loss: 4.150708, iterations: 941994, cum_reward: -2474.921307\n",
      "Episode 1440, loss: 4.447222, iterations: 942494, cum_reward: -2475.239672\n",
      "Episode 1441, loss: 4.476310, iterations: 942994, cum_reward: -2481.781171\n",
      "Episode 1442, loss: 3.408363, iterations: 943494, cum_reward: -2487.604699\n",
      "Episode 1443, loss: 4.295530, iterations: 943994, cum_reward: -2485.433436\n",
      "Episode 1444, loss: 4.497092, iterations: 944494, cum_reward: -2477.609148\n",
      "Episode 1445, loss: 3.871376, iterations: 944994, cum_reward: -2461.955006\n",
      "Episode 1446, loss: 4.200073, iterations: 945494, cum_reward: -2466.029764\n",
      "Episode 1447, loss: 3.704216, iterations: 945994, cum_reward: -2475.376845\n",
      "Episode 1448, loss: 4.373917, iterations: 946494, cum_reward: -2468.315923\n",
      "Episode 1449, loss: 3.470289, iterations: 946994, cum_reward: -2470.104909\n",
      "Episode 1450, loss: 3.568881, iterations: 947494, cum_reward: -2480.750202\n",
      "Episode 1451, loss: 3.294036, iterations: 947994, cum_reward: -2480.922110\n",
      "Episode 1452, loss: 4.389108, iterations: 948494, cum_reward: -2475.278429\n",
      "Episode 1453, loss: 4.467143, iterations: 948994, cum_reward: -2468.588814\n",
      "Episode 1454, loss: 4.619129, iterations: 949494, cum_reward: -2463.487781\n",
      "Episode 1455, loss: 4.609473, iterations: 949994, cum_reward: -2470.444734\n",
      "Episode 1456, loss: 4.695974, iterations: 950494, cum_reward: -2473.789706\n",
      "Episode 1457, loss: 4.547198, iterations: 950994, cum_reward: -2451.468164\n",
      "Episode 1458, loss: 4.291451, iterations: 951494, cum_reward: -2466.637913\n",
      "Episode 1459, loss: 4.156802, iterations: 951994, cum_reward: -2491.279610\n",
      "Episode 1460, loss: 3.931748, iterations: 952494, cum_reward: -2479.629656\n",
      "Episode 1461, loss: 3.688050, iterations: 952994, cum_reward: -2470.394047\n",
      "Episode 1462, loss: 4.222321, iterations: 953494, cum_reward: -2467.329068\n",
      "Episode 1463, loss: 4.612355, iterations: 953994, cum_reward: -2468.945391\n",
      "Episode 1464, loss: 4.306290, iterations: 954494, cum_reward: -2467.021298\n",
      "Episode 1465, loss: 4.806251, iterations: 954994, cum_reward: -2467.938725\n",
      "Episode 1466, loss: 4.521517, iterations: 955494, cum_reward: -2467.602167\n",
      "Episode 1467, loss: 4.610545, iterations: 955994, cum_reward: -2477.323225\n",
      "Episode 1468, loss: 4.269057, iterations: 956494, cum_reward: -2469.016743\n",
      "Episode 1469, loss: 2.703298, iterations: 956994, cum_reward: -2465.287730\n",
      "Episode 1470, loss: 4.450846, iterations: 957494, cum_reward: -2462.404665\n",
      "Episode 1471, loss: 4.161998, iterations: 957994, cum_reward: -2469.963393\n",
      "Episode 1472, loss: 4.056646, iterations: 958494, cum_reward: -2476.407695\n",
      "Episode 1473, loss: 4.482844, iterations: 958994, cum_reward: -2476.966536\n",
      "Episode 1474, loss: 4.000722, iterations: 959494, cum_reward: -2476.926535\n",
      "Episode 1475, loss: 3.953580, iterations: 959994, cum_reward: -2476.746531\n",
      "Episode 1476, loss: 4.108975, iterations: 960494, cum_reward: -2473.986481\n",
      "Episode 1477, loss: 4.237807, iterations: 960994, cum_reward: -2468.822388\n",
      "Episode 1478, loss: 4.235424, iterations: 961494, cum_reward: -2473.837299\n",
      "Episode 1479, loss: 4.346609, iterations: 961994, cum_reward: -2474.031168\n",
      "Episode 1480, loss: 3.711221, iterations: 962494, cum_reward: -2479.603101\n",
      "Episode 1481, loss: 4.545201, iterations: 962994, cum_reward: -2466.486477\n",
      "Episode 1482, loss: 3.971667, iterations: 963494, cum_reward: -2478.452991\n",
      "Episode 1483, loss: 2.918653, iterations: 963994, cum_reward: -2481.670994\n",
      "Episode 1484, loss: 3.712838, iterations: 964494, cum_reward: -2483.892091\n",
      "Episode 1485, loss: 4.234731, iterations: 964994, cum_reward: -2470.496706\n",
      "Episode 1486, loss: 4.206691, iterations: 965494, cum_reward: -2479.853630\n",
      "Episode 1487, loss: 4.465670, iterations: 965994, cum_reward: -2480.341950\n",
      "Episode 1488, loss: 4.282860, iterations: 966494, cum_reward: -2461.668654\n",
      "Episode 1489, loss: 3.428274, iterations: 966994, cum_reward: -2476.067339\n",
      "Episode 1490, loss: 3.778430, iterations: 967494, cum_reward: -2485.074566\n",
      "Episode 1491, loss: 4.246675, iterations: 967994, cum_reward: -2458.059315\n",
      "Episode 1492, loss: 4.078737, iterations: 968494, cum_reward: -2467.432093\n",
      "Episode 1493, loss: 3.373528, iterations: 968994, cum_reward: -2476.754266\n",
      "Episode 1494, loss: 4.232663, iterations: 969494, cum_reward: -2466.906568\n",
      "Episode 1495, loss: 4.316676, iterations: 969994, cum_reward: -2459.622761\n",
      "Episode 1496, loss: 4.304588, iterations: 970494, cum_reward: -2474.787394\n",
      "Episode 1497, loss: 4.600759, iterations: 970994, cum_reward: -2469.785671\n",
      "Episode 1498, loss: 4.466471, iterations: 971494, cum_reward: -2477.288579\n",
      "Episode 1499, loss: 4.083041, iterations: 971994, cum_reward: -2480.442174\n",
      "Episode 1500, loss: 4.532637, iterations: 972494, cum_reward: -2478.001200\n",
      "Episode 1501, loss: 3.696499, iterations: 972994, cum_reward: -2476.163733\n",
      "Episode 1502, loss: 3.261400, iterations: 973494, cum_reward: -2477.283523\n",
      "Episode 1503, loss: 4.644850, iterations: 973994, cum_reward: -2482.553716\n",
      "Episode 1504, loss: 3.976875, iterations: 974494, cum_reward: -2467.674652\n",
      "Episode 1505, loss: 3.975212, iterations: 974994, cum_reward: -2465.027676\n",
      "Episode 1506, loss: 4.145032, iterations: 975494, cum_reward: -2478.978128\n",
      "Episode 1507, loss: 4.571230, iterations: 975994, cum_reward: -2479.422939\n",
      "Episode 1508, loss: 4.520843, iterations: 976494, cum_reward: -2473.707184\n",
      "Episode 1509, loss: 4.346673, iterations: 976994, cum_reward: -2474.010501\n",
      "Episode 1510, loss: 4.635240, iterations: 977494, cum_reward: -2470.252478\n",
      "Episode 1511, loss: 4.350257, iterations: 977994, cum_reward: -2454.286817\n",
      "Episode 1512, loss: 4.082675, iterations: 978494, cum_reward: -2473.845144\n",
      "Episode 1513, loss: 4.693199, iterations: 978994, cum_reward: -2465.489513\n",
      "Episode 1514, loss: 4.370881, iterations: 979494, cum_reward: -2485.247346\n",
      "Episode 1515, loss: 4.431789, iterations: 979994, cum_reward: -2466.741022\n",
      "Episode 1516, loss: 3.513019, iterations: 980494, cum_reward: -2461.147566\n",
      "Episode 1517, loss: 4.429691, iterations: 980994, cum_reward: -2473.731520\n",
      "Episode 1518, loss: 4.771952, iterations: 981494, cum_reward: -2468.524777\n",
      "Episode 1519, loss: 4.562283, iterations: 981994, cum_reward: -2472.400005\n",
      "Episode 1520, loss: 4.082162, iterations: 982494, cum_reward: -2471.306812\n",
      "Episode 1521, loss: 4.079198, iterations: 982994, cum_reward: -2492.962835\n",
      "Episode 1522, loss: 4.167629, iterations: 983494, cum_reward: -2474.949510\n",
      "Episode 1523, loss: 3.424583, iterations: 983994, cum_reward: -2482.701509\n",
      "Episode 1524, loss: 4.827648, iterations: 984494, cum_reward: -2479.107043\n",
      "Episode 1525, loss: 3.895109, iterations: 984994, cum_reward: -2472.899546\n",
      "Episode 1526, loss: 4.504963, iterations: 985494, cum_reward: -2477.660829\n",
      "Episode 1527, loss: 3.931728, iterations: 985994, cum_reward: -2470.018951\n",
      "Episode 1528, loss: 4.387740, iterations: 986494, cum_reward: -2457.381473\n",
      "Episode 1529, loss: 3.872355, iterations: 986994, cum_reward: -2463.056265\n",
      "Episode 1530, loss: 3.509449, iterations: 987494, cum_reward: -2470.607985\n",
      "Episode 1531, loss: 3.080429, iterations: 987994, cum_reward: -2467.285534\n",
      "Episode 1532, loss: 3.204796, iterations: 988494, cum_reward: -2469.939588\n",
      "Episode 1533, loss: 4.567298, iterations: 988994, cum_reward: -2482.081999\n",
      "Episode 1534, loss: 4.424627, iterations: 989494, cum_reward: -2462.516551\n",
      "Episode 1535, loss: 3.936348, iterations: 989994, cum_reward: -2473.687540\n",
      "Episode 1536, loss: 4.068931, iterations: 990494, cum_reward: -2459.824496\n",
      "Episode 1537, loss: 3.786708, iterations: 990994, cum_reward: -2473.582828\n",
      "Episode 1538, loss: 2.729003, iterations: 991494, cum_reward: -2465.000786\n",
      "Episode 1539, loss: 3.064847, iterations: 991994, cum_reward: -2460.974071\n",
      "Episode 1540, loss: 3.794598, iterations: 992494, cum_reward: -2476.416307\n",
      "Episode 1541, loss: 4.226145, iterations: 992994, cum_reward: -2472.768282\n",
      "Episode 1542, loss: 3.529416, iterations: 993494, cum_reward: -2476.333313\n",
      "Episode 1543, loss: 3.624518, iterations: 993994, cum_reward: -2475.731631\n",
      "Episode 1544, loss: 4.500755, iterations: 994494, cum_reward: -2472.105170\n",
      "Episode 1545, loss: 4.505610, iterations: 994994, cum_reward: -2472.155911\n",
      "Episode 1546, loss: 3.496960, iterations: 995494, cum_reward: -2481.022764\n",
      "Episode 1547, loss: 4.179377, iterations: 995994, cum_reward: -2471.765921\n",
      "Episode 1548, loss: 4.503013, iterations: 996494, cum_reward: -2480.459726\n",
      "Episode 1549, loss: 4.166442, iterations: 996994, cum_reward: -2465.047182\n",
      "Episode 1550, loss: 4.395381, iterations: 997494, cum_reward: -2477.758258\n",
      "Episode 1551, loss: 4.846645, iterations: 997994, cum_reward: -2467.166228\n",
      "Episode 1552, loss: 4.757622, iterations: 998494, cum_reward: -2467.886227\n",
      "Episode 1553, loss: 4.104970, iterations: 998994, cum_reward: -2480.356416\n",
      "Episode 1554, loss: 4.626750, iterations: 999494, cum_reward: -2481.026912\n",
      "Episode 1555, loss: 4.183573, iterations: 999994, cum_reward: -2475.221648\n",
      "Episode 1556, loss: 4.553645, iterations: 1000494, cum_reward: -2475.415361\n",
      "Episode 1557, loss: 4.511987, iterations: 1000994, cum_reward: -2473.496844\n",
      "Episode 1558, loss: 3.311871, iterations: 1001494, cum_reward: -2462.725417\n",
      "Episode 1559, loss: 4.452927, iterations: 1001994, cum_reward: -2475.149756\n",
      "Episode 1560, loss: 4.641158, iterations: 1002494, cum_reward: -2483.072121\n",
      "Episode 1561, loss: 4.624475, iterations: 1002994, cum_reward: -2475.140019\n",
      "Episode 1562, loss: 4.106534, iterations: 1003494, cum_reward: -2468.884743\n",
      "Episode 1563, loss: 4.209261, iterations: 1003994, cum_reward: -2474.161543\n",
      "Episode 1564, loss: 4.299565, iterations: 1004494, cum_reward: -2477.350645\n",
      "Episode 1565, loss: 3.295476, iterations: 1004994, cum_reward: -2462.822015\n",
      "Episode 1566, loss: 4.216456, iterations: 1005494, cum_reward: -2469.173222\n",
      "Episode 1594, loss: 4.460476, iterations: 1019494, cum_reward: -2468.718658\n",
      "Episode 1595, loss: 4.269328, iterations: 1019994, cum_reward: -2468.647563\n",
      "Episode 1596, loss: 4.307837, iterations: 1020494, cum_reward: -2483.920967\n",
      "Episode 1597, loss: 3.597476, iterations: 1020994, cum_reward: -2470.728485\n",
      "Episode 1598, loss: 3.632411, iterations: 1021494, cum_reward: -2469.025065\n",
      "Episode 1599, loss: 4.311623, iterations: 1021994, cum_reward: -2475.308120\n",
      "Episode 1600, loss: 4.590987, iterations: 1022494, cum_reward: -2486.497005\n",
      "Episode 1601, loss: 3.377403, iterations: 1022994, cum_reward: -2466.258584\n",
      "Episode 1602, loss: 4.246257, iterations: 1023494, cum_reward: -2473.633433\n",
      "Episode 1603, loss: 4.546790, iterations: 1023994, cum_reward: -2464.698381\n",
      "Episode 1604, loss: 4.285839, iterations: 1024494, cum_reward: -2469.327632\n",
      "Episode 1605, loss: 4.477977, iterations: 1024994, cum_reward: -2480.445578\n",
      "Episode 1606, loss: 4.153421, iterations: 1025494, cum_reward: -2480.681740\n",
      "Episode 1607, loss: 4.451594, iterations: 1025994, cum_reward: -2487.139588\n",
      "Episode 1608, loss: 4.564044, iterations: 1026494, cum_reward: -2472.509437\n",
      "Episode 1609, loss: 4.354170, iterations: 1026994, cum_reward: -2471.125445\n",
      "Episode 1610, loss: 3.722239, iterations: 1027494, cum_reward: -2468.181124\n",
      "Episode 1611, loss: 4.433198, iterations: 1027994, cum_reward: -2459.315655\n",
      "Episode 1612, loss: 4.691427, iterations: 1028494, cum_reward: -2480.928639\n",
      "Episode 1613, loss: 4.518539, iterations: 1028994, cum_reward: -2483.868275\n",
      "Episode 1614, loss: 4.569270, iterations: 1029494, cum_reward: -2470.235753\n",
      "Episode 1615, loss: 3.573678, iterations: 1029994, cum_reward: -2482.321572\n",
      "Episode 1616, loss: 4.252996, iterations: 1030494, cum_reward: -2453.028306\n",
      "Episode 1617, loss: 4.489417, iterations: 1030994, cum_reward: -2476.210773\n",
      "Episode 1618, loss: 4.306672, iterations: 1031494, cum_reward: -2476.112086\n",
      "Episode 1619, loss: 4.382831, iterations: 1031994, cum_reward: -2477.341809\n",
      "Episode 1620, loss: 4.626573, iterations: 1032494, cum_reward: -2483.289894\n",
      "Episode 1621, loss: 3.919986, iterations: 1032994, cum_reward: -2476.585466\n",
      "Episode 1622, loss: 3.994050, iterations: 1033494, cum_reward: -2471.703534\n",
      "Episode 1623, loss: 4.493972, iterations: 1033994, cum_reward: -2463.746316\n",
      "Episode 1624, loss: 4.140454, iterations: 1034494, cum_reward: -2471.458836\n",
      "Episode 1625, loss: 3.961325, iterations: 1034994, cum_reward: -2473.804216\n",
      "Episode 1626, loss: 3.468724, iterations: 1035494, cum_reward: -2469.227294\n",
      "Episode 1627, loss: 4.424038, iterations: 1035994, cum_reward: -2481.759600\n",
      "Episode 1628, loss: 4.270128, iterations: 1036494, cum_reward: -2471.685004\n",
      "Episode 1629, loss: 4.478577, iterations: 1036994, cum_reward: -2468.066924\n",
      "Episode 1630, loss: 4.656418, iterations: 1037494, cum_reward: -2492.273119\n",
      "Episode 1631, loss: 4.076146, iterations: 1037994, cum_reward: -2469.262761\n",
      "Episode 1632, loss: 4.309570, iterations: 1038494, cum_reward: -2467.247819\n",
      "Episode 1633, loss: 4.029150, iterations: 1038994, cum_reward: -2459.809297\n",
      "Episode 1634, loss: 4.379527, iterations: 1039494, cum_reward: -2478.983659\n",
      "Episode 1635, loss: 3.734107, iterations: 1039994, cum_reward: -2458.529239\n",
      "Episode 1636, loss: 4.724173, iterations: 1040494, cum_reward: -2475.046510\n",
      "Episode 1637, loss: 4.180358, iterations: 1040994, cum_reward: -2472.524201\n",
      "Episode 1638, loss: 4.372919, iterations: 1041494, cum_reward: -2480.108321\n",
      "Episode 1639, loss: 3.892400, iterations: 1041994, cum_reward: -2477.142633\n",
      "Episode 1640, loss: 3.962837, iterations: 1042494, cum_reward: -2476.841883\n",
      "Episode 1641, loss: 4.671339, iterations: 1042994, cum_reward: -2464.130150\n",
      "Episode 1642, loss: 4.506014, iterations: 1043494, cum_reward: -2467.427840\n",
      "Episode 1643, loss: 4.223397, iterations: 1043994, cum_reward: -2472.000972\n",
      "Episode 1644, loss: 4.582465, iterations: 1044494, cum_reward: -2477.599745\n",
      "Episode 1645, loss: 4.298515, iterations: 1044994, cum_reward: -2471.977245\n",
      "Episode 1646, loss: 4.018682, iterations: 1045494, cum_reward: -2481.125695\n",
      "Episode 1647, loss: 4.035516, iterations: 1045994, cum_reward: -2476.616472\n",
      "Episode 1648, loss: 4.224887, iterations: 1046494, cum_reward: -2476.776471\n",
      "Episode 1649, loss: 4.853525, iterations: 1046994, cum_reward: -2488.186590\n",
      "Episode 1650, loss: 3.316563, iterations: 1047494, cum_reward: -2474.476449\n",
      "Episode 1651, loss: 4.566684, iterations: 1047994, cum_reward: -2461.729214\n",
      "Episode 1652, loss: 4.594026, iterations: 1048494, cum_reward: -2479.379910\n",
      "Episode 1653, loss: 4.334177, iterations: 1048994, cum_reward: -2474.555770\n",
      "Episode 1654, loss: 4.131258, iterations: 1049494, cum_reward: -2488.604450\n",
      "Episode 1655, loss: 4.484189, iterations: 1049994, cum_reward: -2476.764048\n",
      "Episode 1656, loss: 4.132821, iterations: 1050494, cum_reward: -2475.450927\n",
      "Episode 1657, loss: 4.243977, iterations: 1050994, cum_reward: -2472.579369\n",
      "Episode 1658, loss: 4.436985, iterations: 1051494, cum_reward: -2470.793397\n",
      "Episode 1659, loss: 4.314408, iterations: 1051994, cum_reward: -2463.622992\n",
      "Episode 1660, loss: 4.339901, iterations: 1052494, cum_reward: -2482.233143\n",
      "Episode 1661, loss: 4.035701, iterations: 1052994, cum_reward: -2470.827255\n",
      "Episode 1662, loss: 4.769624, iterations: 1053494, cum_reward: -2476.828360\n",
      "Episode 1663, loss: 4.272962, iterations: 1053994, cum_reward: -2463.996873\n",
      "Episode 1664, loss: 4.082557, iterations: 1054494, cum_reward: -2478.831591\n",
      "Episode 1665, loss: 4.322086, iterations: 1054994, cum_reward: -2475.180039\n",
      "Episode 1666, loss: 4.160427, iterations: 1055494, cum_reward: -2484.127699\n",
      "Episode 1667, loss: 4.491531, iterations: 1055994, cum_reward: -2468.311027\n",
      "Episode 1668, loss: 4.579174, iterations: 1056494, cum_reward: -2485.378574\n",
      "Episode 1669, loss: 4.228625, iterations: 1056994, cum_reward: -2483.345928\n",
      "Episode 1670, loss: 3.783597, iterations: 1057494, cum_reward: -2464.498521\n",
      "Episode 1671, loss: 3.923613, iterations: 1057994, cum_reward: -2474.433588\n",
      "Episode 1672, loss: 4.704151, iterations: 1058494, cum_reward: -2483.120358\n",
      "Episode 1673, loss: 3.830866, iterations: 1058994, cum_reward: -2482.160492\n",
      "Episode 1674, loss: 4.190089, iterations: 1059494, cum_reward: -2476.097802\n",
      "Episode 1675, loss: 4.636102, iterations: 1059994, cum_reward: -2477.651297\n",
      "Episode 1676, loss: 4.735529, iterations: 1060494, cum_reward: -2473.749034\n",
      "Episode 1677, loss: 4.004061, iterations: 1060994, cum_reward: -2478.767256\n",
      "Episode 1678, loss: 4.104612, iterations: 1061494, cum_reward: -2464.465224\n",
      "Episode 1679, loss: 3.837606, iterations: 1061994, cum_reward: -2486.159405\n",
      "Episode 1680, loss: 4.200583, iterations: 1062494, cum_reward: -2476.989559\n",
      "Episode 1681, loss: 3.823942, iterations: 1062994, cum_reward: -2476.120255\n",
      "Episode 1682, loss: 3.455405, iterations: 1063494, cum_reward: -2479.937182\n",
      "Episode 1683, loss: 4.252536, iterations: 1063994, cum_reward: -2490.359074\n",
      "Episode 1684, loss: 4.540030, iterations: 1064494, cum_reward: -2476.322111\n",
      "Episode 1685, loss: 3.818154, iterations: 1064994, cum_reward: -2468.231518\n",
      "Episode 1686, loss: 4.397994, iterations: 1065494, cum_reward: -2479.221024\n",
      "Episode 1687, loss: 3.773726, iterations: 1065994, cum_reward: -2482.036216\n",
      "Episode 1688, loss: 4.086173, iterations: 1066494, cum_reward: -2470.698736\n",
      "Episode 1689, loss: 4.543035, iterations: 1066994, cum_reward: -2472.395468\n",
      "Episode 1690, loss: 4.594491, iterations: 1067494, cum_reward: -2482.895057\n",
      "Episode 1691, loss: 4.337339, iterations: 1067994, cum_reward: -2483.120191\n",
      "Episode 1692, loss: 4.493072, iterations: 1068494, cum_reward: -2471.934624\n",
      "Episode 1693, loss: 4.717916, iterations: 1068994, cum_reward: -2468.143404\n",
      "Episode 1694, loss: 4.399092, iterations: 1069494, cum_reward: -2454.762599\n",
      "Episode 1695, loss: 3.891202, iterations: 1069994, cum_reward: -2478.627190\n",
      "Episode 1696, loss: 4.263834, iterations: 1070494, cum_reward: -2478.555554\n",
      "Episode 1697, loss: 4.133408, iterations: 1070994, cum_reward: -2481.181188\n",
      "Episode 1698, loss: 4.152748, iterations: 1071494, cum_reward: -2482.088696\n",
      "Episode 1699, loss: 4.528297, iterations: 1071994, cum_reward: -2488.219538\n",
      "Episode 1700, loss: 3.751220, iterations: 1072494, cum_reward: -2487.130050\n",
      "Episode 1701, loss: 4.434163, iterations: 1072994, cum_reward: -2478.043446\n",
      "Episode 1702, loss: 4.516479, iterations: 1073494, cum_reward: -2463.910018\n",
      "Episode 1703, loss: 4.705577, iterations: 1073994, cum_reward: -2463.685728\n",
      "Episode 1704, loss: 4.472816, iterations: 1074494, cum_reward: -2485.031864\n",
      "Episode 1705, loss: 4.677790, iterations: 1074994, cum_reward: -2477.272438\n",
      "Episode 1706, loss: 4.581862, iterations: 1075494, cum_reward: -2488.944409\n",
      "Episode 1707, loss: 4.424816, iterations: 1075994, cum_reward: -2469.575414\n",
      "Episode 1708, loss: 3.135428, iterations: 1076494, cum_reward: -2469.390870\n",
      "Episode 1709, loss: 4.690425, iterations: 1076994, cum_reward: -2447.276546\n",
      "Episode 1710, loss: 4.523853, iterations: 1077494, cum_reward: -2474.523540\n",
      "Episode 1711, loss: 4.628972, iterations: 1077994, cum_reward: -2474.113315\n",
      "Episode 1712, loss: 4.181018, iterations: 1078494, cum_reward: -2470.712442\n",
      "Episode 1713, loss: 4.492031, iterations: 1078994, cum_reward: -2473.539082\n",
      "Episode 1714, loss: 3.932992, iterations: 1079494, cum_reward: -2470.232493\n",
      "Episode 1715, loss: 4.209897, iterations: 1079994, cum_reward: -2474.385911\n",
      "Episode 1716, loss: 4.281761, iterations: 1080494, cum_reward: -2477.542653\n",
      "Episode 1717, loss: 4.499706, iterations: 1080994, cum_reward: -2476.047381\n",
      "Episode 1718, loss: 3.546524, iterations: 1081494, cum_reward: -2471.370064\n",
      "Episode 1719, loss: 3.988357, iterations: 1081994, cum_reward: -2476.048937\n",
      "Episode 1720, loss: 3.322844, iterations: 1082494, cum_reward: -2471.399135\n",
      "Episode 1721, loss: 3.656106, iterations: 1082994, cum_reward: -2471.389895\n",
      "Episode 1722, loss: 3.594864, iterations: 1083494, cum_reward: -2483.761580\n",
      "Episode 1723, loss: 4.597039, iterations: 1083994, cum_reward: -2485.271322\n",
      "Episode 1724, loss: 4.240473, iterations: 1084494, cum_reward: -2473.063261\n",
      "Episode 1725, loss: 3.237768, iterations: 1084994, cum_reward: -2472.755380\n",
      "Episode 1726, loss: 4.161166, iterations: 1085494, cum_reward: -2474.963184\n",
      "Episode 1727, loss: 4.439274, iterations: 1085994, cum_reward: -2472.336700\n",
      "Episode 1728, loss: 4.279408, iterations: 1086494, cum_reward: -2473.609662\n",
      "Episode 1729, loss: 4.591443, iterations: 1086994, cum_reward: -2482.728221\n",
      "Episode 1730, loss: 3.877553, iterations: 1087494, cum_reward: -2466.580284\n",
      "Episode 1731, loss: 3.984072, iterations: 1087994, cum_reward: -2482.073198\n",
      "Episode 1732, loss: 4.437693, iterations: 1088494, cum_reward: -2464.458588\n",
      "Episode 1733, loss: 4.602418, iterations: 1088994, cum_reward: -2459.379273\n",
      "Episode 1734, loss: 4.466102, iterations: 1089494, cum_reward: -2478.467791\n",
      "Episode 1735, loss: 4.784630, iterations: 1089994, cum_reward: -2477.425811\n",
      "Episode 1736, loss: 4.298015, iterations: 1090494, cum_reward: -2470.142066\n",
      "Episode 1737, loss: 3.596464, iterations: 1090994, cum_reward: -2487.503913\n",
      "Episode 1738, loss: 4.153219, iterations: 1091494, cum_reward: -2475.006882\n",
      "Episode 1739, loss: 4.370804, iterations: 1091994, cum_reward: -2482.371646\n",
      "Episode 1740, loss: 4.414443, iterations: 1092494, cum_reward: -2480.183375\n",
      "Episode 1741, loss: 4.478139, iterations: 1092994, cum_reward: -2477.117267\n",
      "Episode 1742, loss: 4.432682, iterations: 1093494, cum_reward: -2482.681219\n",
      "Episode 1743, loss: 4.590254, iterations: 1093994, cum_reward: -2476.372379\n",
      "Episode 1744, loss: 4.678965, iterations: 1094494, cum_reward: -2480.548289\n",
      "Episode 1745, loss: 4.461878, iterations: 1094994, cum_reward: -2483.991519\n",
      "Episode 1746, loss: 4.127748, iterations: 1095494, cum_reward: -2465.366922\n",
      "Episode 1747, loss: 4.672152, iterations: 1095994, cum_reward: -2482.829926\n",
      "Episode 1748, loss: 4.617498, iterations: 1096494, cum_reward: -2471.508393\n",
      "Episode 1749, loss: 4.810275, iterations: 1096994, cum_reward: -2479.428304\n",
      "Episode 1750, loss: 3.752476, iterations: 1097494, cum_reward: -2456.325594\n",
      "Episode 1751, loss: 4.342805, iterations: 1097994, cum_reward: -2479.316725\n",
      "Episode 1752, loss: 4.634248, iterations: 1098494, cum_reward: -2472.580959\n",
      "Episode 1753, loss: 4.145392, iterations: 1098994, cum_reward: -2479.890540\n",
      "Episode 1754, loss: 4.440867, iterations: 1099494, cum_reward: -2467.092304\n",
      "Episode 1755, loss: 4.275735, iterations: 1099994, cum_reward: -2473.597337\n",
      "Episode 1756, loss: 4.010635, iterations: 1100494, cum_reward: -2474.725265\n",
      "Episode 1757, loss: 3.565589, iterations: 1100994, cum_reward: -2465.699368\n",
      "Episode 1758, loss: 4.662387, iterations: 1101494, cum_reward: -2479.323999\n",
      "Episode 1759, loss: 4.368284, iterations: 1101994, cum_reward: -2460.281985\n",
      "Episode 1760, loss: 4.490527, iterations: 1102494, cum_reward: -2466.294032\n",
      "Episode 1761, loss: 3.270095, iterations: 1102994, cum_reward: -2484.860004\n",
      "Episode 1762, loss: 3.795917, iterations: 1103494, cum_reward: -2482.286782\n",
      "Episode 1763, loss: 4.394715, iterations: 1103994, cum_reward: -2477.251566\n",
      "Episode 1764, loss: 4.603953, iterations: 1104494, cum_reward: -2473.163383\n",
      "Episode 1765, loss: 4.429474, iterations: 1104994, cum_reward: -2479.861433\n",
      "Episode 1766, loss: 3.913805, iterations: 1105494, cum_reward: -2474.242258\n",
      "Episode 1767, loss: 3.718944, iterations: 1105994, cum_reward: -2470.876951\n",
      "Episode 1768, loss: 4.513305, iterations: 1106494, cum_reward: -2470.150514\n",
      "Episode 1769, loss: 4.280209, iterations: 1106994, cum_reward: -2462.787427\n",
      "Episode 1770, loss: 3.796112, iterations: 1107494, cum_reward: -2478.318154\n",
      "Episode 1771, loss: 4.773691, iterations: 1107994, cum_reward: -2472.870158\n",
      "Episode 1772, loss: 4.539687, iterations: 1108494, cum_reward: -2483.809157\n",
      "Episode 1773, loss: 3.823162, iterations: 1108994, cum_reward: -2478.900786\n",
      "Episode 1774, loss: 4.411814, iterations: 1109494, cum_reward: -2471.111988\n",
      "Episode 1775, loss: 4.055918, iterations: 1109994, cum_reward: -2447.356513\n",
      "Episode 1776, loss: 4.925982, iterations: 1110494, cum_reward: -2464.432626\n",
      "Episode 1777, loss: 4.176472, iterations: 1110994, cum_reward: -2479.169327\n",
      "Episode 1778, loss: 4.540909, iterations: 1111494, cum_reward: -2471.122290\n",
      "Episode 1779, loss: 3.750844, iterations: 1111994, cum_reward: -2469.529935\n",
      "Episode 1780, loss: 4.415916, iterations: 1112494, cum_reward: -2460.208522\n",
      "Episode 1781, loss: 4.368499, iterations: 1112994, cum_reward: -2475.000998\n",
      "Episode 1782, loss: 4.500843, iterations: 1113494, cum_reward: -2488.696002\n",
      "Episode 1783, loss: 4.031242, iterations: 1113994, cum_reward: -2481.281878\n",
      "Episode 1784, loss: 4.472412, iterations: 1114494, cum_reward: -2487.248463\n",
      "Episode 1785, loss: 4.116940, iterations: 1114994, cum_reward: -2470.053914\n",
      "Episode 1786, loss: 4.375793, iterations: 1115494, cum_reward: -2473.821373\n",
      "Episode 1787, loss: 2.451823, iterations: 1115994, cum_reward: -2462.971952\n",
      "Episode 1788, loss: 4.386442, iterations: 1116494, cum_reward: -2474.206812\n",
      "Episode 1789, loss: 4.236289, iterations: 1116994, cum_reward: -2472.206566\n",
      "Episode 1790, loss: 3.744902, iterations: 1117494, cum_reward: -2465.035067\n",
      "Episode 1791, loss: 4.004257, iterations: 1117994, cum_reward: -2477.217911\n",
      "Episode 1792, loss: 3.997886, iterations: 1118494, cum_reward: -2475.621346\n",
      "Episode 1793, loss: 4.108197, iterations: 1118994, cum_reward: -2471.469046\n",
      "Episode 1794, loss: 4.602083, iterations: 1119494, cum_reward: -2463.710253\n",
      "Episode 1795, loss: 4.496324, iterations: 1119994, cum_reward: -2474.765804\n",
      "Episode 1796, loss: 3.852211, iterations: 1120494, cum_reward: -2473.915561\n",
      "Episode 1797, loss: 4.165919, iterations: 1120994, cum_reward: -2476.255899\n",
      "Episode 1798, loss: 4.284801, iterations: 1121494, cum_reward: -2469.309285\n",
      "Episode 1799, loss: 4.722730, iterations: 1121994, cum_reward: -2477.516796\n",
      "Episode 1800, loss: 4.553985, iterations: 1122494, cum_reward: -2465.798293\n",
      "Episode 1801, loss: 4.645902, iterations: 1122994, cum_reward: -2468.836164\n",
      "Episode 1802, loss: 4.480969, iterations: 1123494, cum_reward: -2462.360327\n",
      "Episode 1803, loss: 4.707499, iterations: 1123994, cum_reward: -2484.054383\n",
      "Episode 1804, loss: 4.341214, iterations: 1124494, cum_reward: -2480.932499\n",
      "Episode 1805, loss: 4.043231, iterations: 1124994, cum_reward: -2465.931247\n",
      "Episode 1806, loss: 3.268292, iterations: 1125494, cum_reward: -2489.455504\n",
      "Episode 1807, loss: 4.503368, iterations: 1125994, cum_reward: -2473.750979\n",
      "Episode 1808, loss: 3.975601, iterations: 1126494, cum_reward: -2475.243825\n",
      "Episode 1809, loss: 4.189762, iterations: 1126994, cum_reward: -2474.964519\n",
      "Episode 1810, loss: 4.008862, iterations: 1127494, cum_reward: -2471.572986\n",
      "Episode 1811, loss: 4.460679, iterations: 1127994, cum_reward: -2460.112257\n",
      "Episode 1812, loss: 4.322840, iterations: 1128494, cum_reward: -2480.885469\n",
      "Episode 1813, loss: 4.443333, iterations: 1128994, cum_reward: -2474.729889\n",
      "Episode 1814, loss: 4.387727, iterations: 1129494, cum_reward: -2463.281878\n",
      "Episode 1815, loss: 4.749828, iterations: 1129994, cum_reward: -2474.998322\n",
      "Episode 1816, loss: 4.373768, iterations: 1130494, cum_reward: -2493.068082\n",
      "Episode 1817, loss: 4.666683, iterations: 1130994, cum_reward: -2475.524702\n",
      "Episode 1818, loss: 4.588574, iterations: 1131494, cum_reward: -2481.625223\n",
      "Episode 1819, loss: 3.966717, iterations: 1131994, cum_reward: -2477.292706\n",
      "Episode 1820, loss: 4.258858, iterations: 1132494, cum_reward: -2477.339344\n",
      "Episode 1821, loss: 4.556958, iterations: 1132994, cum_reward: -2484.153861\n",
      "Episode 1822, loss: 4.533451, iterations: 1133494, cum_reward: -2475.501323\n",
      "Episode 1823, loss: 4.218142, iterations: 1133994, cum_reward: -2459.093127\n",
      "Episode 1824, loss: 4.481531, iterations: 1134494, cum_reward: -2476.142092\n",
      "Episode 1825, loss: 4.742339, iterations: 1134994, cum_reward: -2472.092016\n",
      "Episode 1826, loss: 4.365471, iterations: 1135494, cum_reward: -2472.553674\n",
      "Episode 1827, loss: 4.035567, iterations: 1135994, cum_reward: -2485.951280\n",
      "Episode 1828, loss: 3.308686, iterations: 1136494, cum_reward: -2473.013784\n",
      "Episode 1829, loss: 4.289128, iterations: 1136994, cum_reward: -2474.902678\n",
      "Episode 1830, loss: 4.231358, iterations: 1137494, cum_reward: -2478.730507\n",
      "Episode 1831, loss: 4.266686, iterations: 1137994, cum_reward: -2481.778793\n",
      "Episode 1832, loss: 4.631656, iterations: 1138494, cum_reward: -2464.991627\n",
      "Episode 1833, loss: 4.516469, iterations: 1138994, cum_reward: -2473.952852\n",
      "Episode 1834, loss: 4.545932, iterations: 1139494, cum_reward: -2479.824243\n",
      "Episode 1835, loss: 3.419844, iterations: 1139994, cum_reward: -2469.289361\n",
      "Episode 1836, loss: 4.356134, iterations: 1140494, cum_reward: -2472.738472\n",
      "Episode 1837, loss: 4.531104, iterations: 1140994, cum_reward: -2470.102481\n",
      "Episode 1838, loss: 4.238933, iterations: 1141494, cum_reward: -2479.731324\n",
      "Episode 1839, loss: 4.453969, iterations: 1141994, cum_reward: -2475.572617\n",
      "Episode 1840, loss: 3.994069, iterations: 1142494, cum_reward: -2480.449603\n",
      "Episode 1841, loss: 4.372106, iterations: 1142994, cum_reward: -2473.246383\n",
      "Episode 1842, loss: 3.869414, iterations: 1143494, cum_reward: -2483.240929\n",
      "Episode 1843, loss: 4.097340, iterations: 1143994, cum_reward: -2469.871182\n",
      "Episode 1844, loss: 4.263771, iterations: 1144494, cum_reward: -2467.119898\n",
      "Episode 1845, loss: 3.978849, iterations: 1144994, cum_reward: -2469.965151\n",
      "Episode 1846, loss: 4.263233, iterations: 1145494, cum_reward: -2492.592126\n",
      "Episode 1847, loss: 3.676611, iterations: 1145994, cum_reward: -2454.398042\n",
      "Episode 1848, loss: 4.659995, iterations: 1146494, cum_reward: -2470.287824\n",
      "Episode 1849, loss: 4.721001, iterations: 1146994, cum_reward: -2479.965822\n",
      "Episode 1850, loss: 4.368526, iterations: 1147494, cum_reward: -2465.109681\n",
      "Episode 1851, loss: 4.318467, iterations: 1147994, cum_reward: -2483.181504\n",
      "Episode 1852, loss: 4.501679, iterations: 1148494, cum_reward: -2472.048637\n",
      "Episode 1853, loss: 3.943582, iterations: 1148994, cum_reward: -2482.486309\n",
      "Episode 1854, loss: 4.222393, iterations: 1149494, cum_reward: -2481.722157\n",
      "Episode 1855, loss: 4.160480, iterations: 1149994, cum_reward: -2485.603684\n",
      "Episode 1856, loss: 4.313947, iterations: 1150494, cum_reward: -2477.278658\n",
      "Episode 1857, loss: 4.433872, iterations: 1150994, cum_reward: -2481.086142\n",
      "Episode 1858, loss: 4.096052, iterations: 1151494, cum_reward: -2464.358332\n",
      "Episode 1859, loss: 4.058286, iterations: 1151994, cum_reward: -2476.547572\n",
      "Episode 1860, loss: 4.324327, iterations: 1152494, cum_reward: -2483.587766\n",
      "Episode 1861, loss: 4.049171, iterations: 1152994, cum_reward: -2473.959339\n",
      "Episode 1862, loss: 3.981272, iterations: 1153494, cum_reward: -2482.196412\n",
      "Episode 1863, loss: 4.046043, iterations: 1153994, cum_reward: -2476.548879\n",
      "Episode 1864, loss: 4.466412, iterations: 1154494, cum_reward: -2478.800979\n",
      "Episode 1865, loss: 4.107900, iterations: 1154994, cum_reward: -2466.150988\n",
      "Episode 1866, loss: 4.272896, iterations: 1155494, cum_reward: -2485.705019\n",
      "Episode 1867, loss: 2.531221, iterations: 1155994, cum_reward: -2465.748729\n",
      "Episode 1868, loss: 4.753890, iterations: 1156494, cum_reward: -2473.486547\n",
      "Episode 1869, loss: 4.623870, iterations: 1156994, cum_reward: -2467.878047\n",
      "Episode 1870, loss: 4.023480, iterations: 1157494, cum_reward: -2474.757365\n",
      "Episode 1871, loss: 3.660423, iterations: 1157994, cum_reward: -2475.049129\n",
      "Episode 1872, loss: 4.558662, iterations: 1158494, cum_reward: -2482.932863\n",
      "Episode 1873, loss: 4.684188, iterations: 1158994, cum_reward: -2469.131751\n",
      "Episode 1874, loss: 4.268127, iterations: 1159494, cum_reward: -2463.314983\n",
      "Episode 1875, loss: 2.868730, iterations: 1159994, cum_reward: -2475.691978\n",
      "Episode 1876, loss: 4.503938, iterations: 1160494, cum_reward: -2476.864993\n",
      "Episode 1877, loss: 4.005380, iterations: 1160994, cum_reward: -2476.102716\n",
      "Episode 1878, loss: 4.542410, iterations: 1161494, cum_reward: -2478.254004\n",
      "Episode 1879, loss: 4.572983, iterations: 1161994, cum_reward: -2477.613180\n",
      "Episode 1880, loss: 3.670928, iterations: 1162494, cum_reward: -2475.925857\n",
      "Episode 1881, loss: 4.319824, iterations: 1162994, cum_reward: -2472.955716\n",
      "Episode 1882, loss: 4.109508, iterations: 1163494, cum_reward: -2463.957718\n",
      "Episode 1883, loss: 4.098274, iterations: 1163994, cum_reward: -2470.378735\n",
      "Episode 1884, loss: 4.215789, iterations: 1164494, cum_reward: -2483.637254\n",
      "Episode 1885, loss: 4.343637, iterations: 1164994, cum_reward: -2474.946101\n",
      "Episode 1886, loss: 3.755942, iterations: 1165494, cum_reward: -2470.661619\n",
      "Episode 1887, loss: 4.466101, iterations: 1165994, cum_reward: -2478.283553\n",
      "Episode 1888, loss: 4.133791, iterations: 1166494, cum_reward: -2469.976350\n",
      "Episode 1889, loss: 4.371350, iterations: 1166994, cum_reward: -2476.716017\n",
      "Episode 1890, loss: 4.483607, iterations: 1167494, cum_reward: -2475.708177\n",
      "Episode 1891, loss: 4.141608, iterations: 1167994, cum_reward: -2472.101100\n",
      "Episode 1892, loss: 4.643507, iterations: 1168494, cum_reward: -2480.699006\n",
      "Episode 1893, loss: 4.228007, iterations: 1168994, cum_reward: -2474.644441\n",
      "Episode 1894, loss: 3.906555, iterations: 1169494, cum_reward: -2470.818118\n",
      "Episode 1895, loss: 4.340603, iterations: 1169994, cum_reward: -2474.670145\n",
      "Episode 1896, loss: 4.266695, iterations: 1170494, cum_reward: -2480.559000\n",
      "Episode 1897, loss: 4.309543, iterations: 1170994, cum_reward: -2476.752180\n",
      "Episode 1898, loss: 4.549750, iterations: 1171494, cum_reward: -2456.663595\n",
      "Episode 1899, loss: 4.708164, iterations: 1171994, cum_reward: -2482.269280\n",
      "Episode 1900, loss: 4.709343, iterations: 1172494, cum_reward: -2484.685210\n",
      "Episode 1901, loss: 3.972639, iterations: 1172994, cum_reward: -2470.900901\n",
      "Episode 1902, loss: 4.519870, iterations: 1173494, cum_reward: -2469.223751\n",
      "Episode 1903, loss: 4.186753, iterations: 1173994, cum_reward: -2471.619151\n",
      "Episode 1904, loss: 4.111466, iterations: 1174494, cum_reward: -2481.845718\n",
      "Episode 1905, loss: 4.141941, iterations: 1174994, cum_reward: -2469.918409\n",
      "Episode 1906, loss: 3.441931, iterations: 1175494, cum_reward: -2473.078246\n",
      "Episode 1907, loss: 4.118247, iterations: 1175994, cum_reward: -2481.775685\n",
      "Episode 1908, loss: 3.677240, iterations: 1176494, cum_reward: -2476.153936\n",
      "Episode 1909, loss: 4.369509, iterations: 1176994, cum_reward: -2477.564761\n",
      "Episode 1910, loss: 4.553062, iterations: 1177494, cum_reward: -2469.518073\n",
      "Episode 1911, loss: 3.733223, iterations: 1177994, cum_reward: -2469.472968\n",
      "Episode 1912, loss: 4.185448, iterations: 1178494, cum_reward: -2479.820692\n",
      "Episode 1913, loss: 4.116195, iterations: 1178994, cum_reward: -2462.930724\n",
      "Episode 1914, loss: 4.397954, iterations: 1179494, cum_reward: -2463.883338\n",
      "Episode 1915, loss: 4.171221, iterations: 1179994, cum_reward: -2475.406493\n",
      "Episode 1916, loss: 4.395643, iterations: 1180494, cum_reward: -2470.540314\n",
      "Episode 1917, loss: 4.376536, iterations: 1180994, cum_reward: -2483.615091\n",
      "Episode 1918, loss: 4.548203, iterations: 1181494, cum_reward: -2477.603197\n",
      "Episode 1919, loss: 4.387372, iterations: 1181994, cum_reward: -2481.481484\n",
      "Episode 1920, loss: 4.643574, iterations: 1182494, cum_reward: -2482.896960\n",
      "Episode 1921, loss: 4.902163, iterations: 1182994, cum_reward: -2474.564203\n",
      "Episode 1922, loss: 5.094382, iterations: 1183494, cum_reward: -2468.330505\n",
      "Episode 1923, loss: 4.551301, iterations: 1183994, cum_reward: -2467.678190\n",
      "Episode 1924, loss: 2.855173, iterations: 1184494, cum_reward: -2466.731398\n",
      "Episode 1925, loss: 4.283782, iterations: 1184994, cum_reward: -2477.072580\n",
      "Episode 1926, loss: 4.265176, iterations: 1185494, cum_reward: -2484.542020\n",
      "Episode 1927, loss: 4.440184, iterations: 1185994, cum_reward: -2482.628937\n",
      "Episode 1928, loss: 3.938128, iterations: 1186494, cum_reward: -2482.182652\n",
      "Episode 1929, loss: 4.401502, iterations: 1186994, cum_reward: -2487.639907\n",
      "Episode 1930, loss: 4.289280, iterations: 1187494, cum_reward: -2477.128297\n",
      "Episode 1931, loss: 3.929558, iterations: 1187994, cum_reward: -2475.047384\n",
      "Episode 1932, loss: 3.876180, iterations: 1188494, cum_reward: -2472.918025\n",
      "Episode 1933, loss: 4.489767, iterations: 1188994, cum_reward: -2465.850566\n",
      "Episode 1934, loss: 3.935906, iterations: 1189494, cum_reward: -2479.515821\n",
      "Episode 1935, loss: 4.629474, iterations: 1189994, cum_reward: -2481.417772\n",
      "Episode 1976, loss: 4.560925, iterations: 1210494, cum_reward: -2481.831533\n",
      "Episode 1977, loss: 3.093495, iterations: 1210994, cum_reward: -2475.021372\n",
      "Episode 1978, loss: 4.527858, iterations: 1211494, cum_reward: -2477.325472\n",
      "Episode 1979, loss: 3.414747, iterations: 1211994, cum_reward: -2472.599643\n",
      "Episode 1980, loss: 4.413040, iterations: 1212494, cum_reward: -2487.912417\n",
      "Episode 1981, loss: 3.458952, iterations: 1212994, cum_reward: -2470.826668\n",
      "Episode 1982, loss: 4.083580, iterations: 1213494, cum_reward: -2484.105153\n",
      "Episode 1983, loss: 4.059954, iterations: 1213994, cum_reward: -2475.381064\n",
      "Episode 1984, loss: 4.698035, iterations: 1214494, cum_reward: -2478.895785\n",
      "Episode 1985, loss: 3.781464, iterations: 1214994, cum_reward: -2459.904756\n",
      "Episode 1986, loss: 3.867458, iterations: 1215494, cum_reward: -2478.436470\n",
      "Episode 1987, loss: 3.991350, iterations: 1215994, cum_reward: -2474.746533\n",
      "Episode 1988, loss: 4.595216, iterations: 1216494, cum_reward: -2482.157888\n",
      "Episode 1989, loss: 3.155796, iterations: 1216994, cum_reward: -2487.523638\n",
      "Episode 1990, loss: 3.666491, iterations: 1217494, cum_reward: -2469.193717\n",
      "Episode 1991, loss: 4.198371, iterations: 1217994, cum_reward: -2478.752323\n",
      "Episode 1992, loss: 2.500293, iterations: 1218494, cum_reward: -2466.776052\n",
      "Episode 1993, loss: 4.381341, iterations: 1218994, cum_reward: -2465.716630\n",
      "Episode 1994, loss: 3.686526, iterations: 1219494, cum_reward: -2461.080195\n",
      "Episode 1995, loss: 4.474570, iterations: 1219994, cum_reward: -2463.844714\n",
      "Episode 1996, loss: 4.350594, iterations: 1220494, cum_reward: -2474.089131\n",
      "Episode 1997, loss: 3.915222, iterations: 1220994, cum_reward: -2464.538012\n",
      "Episode 1998, loss: 3.519377, iterations: 1221494, cum_reward: -2465.948288\n",
      "Episode 1999, loss: 4.315903, iterations: 1221994, cum_reward: -2484.756977\n",
      "Episode 2000, loss: 3.832371, iterations: 1222494, cum_reward: -2486.084697\n"
     ]
    }
   ],
   "source": [
    "train_helper.train(2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "action = 1\n",
    "env.reset()\n",
    "actions = [0] * 10\n",
    "for i in range(500):\n",
    "    t_t = Transition(*env.step(action))\n",
    "    t_s = samplizer_.process(t_t)\n",
    "    t_q = dqn_preprocess(t_s)\n",
    "    \n",
    "    # It's intersting that the policy network and target network don't give consistent results sometimes\n",
    "    action = random.choice((int(agent.predict(t_q.state)), int(agent.predict_by_t(t_q.state))))\n",
    "    # action = int(agent.predict(t_q.state))\n",
    "    \n",
    "    actions[action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01c5c271f574aafb10944e18ed78c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'name': 'CPU intensive jobs',\n",
       "              'type': 'bar',\n",
       "              'uid': '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from status_render import StatusRender\n",
    "render = StatusRender(env)\n",
    "render.job_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([nan]),\n",
      "indices=tensor([0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    print(agent._policy_net(t_q.state).max(1))\n",
    "agent._target_net(t_q.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_reward(env, policy):\n",
    "    env.reset()\n",
    "    while not env.is_terminal():\n",
    "        env.step(policy())\n",
    "    return env.cum_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward by bseline policies:\n",
      "random: -4068.133191\n",
      "earlist: -2679.829649\n",
      "round_robin: -3925.302407\n",
      "sensible: -3948.135600\n",
      "bestfit: -3205.975242\n"
     ]
    }
   ],
   "source": [
    "print(\"Reward by bseline policies:\\n\"\n",
    "      \"random: {:3f}\\n\"\n",
    "      \"earlist: {:3f}\\n\"\n",
    "      \"round_robin: {:3f}\\n\"\n",
    "      \"sensible: {:3f}\\n\"\n",
    "      \"bestfit: {:3f}\".format(policy_reward(env, baseline_policy.random_policy),\n",
    "                           policy_reward(env, baseline_policy.earlist_policy),\n",
    "                           policy_reward(env, baseline_policy.round_robin_policy),\n",
    "                           policy_reward(env, baseline_policy.sensible_policy),\n",
    "                           policy_reward(env, baseline_policy.bestfit_policy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-38fc08d9d259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't_q' is not defined"
     ]
    }
   ],
   "source": [
    "t_q.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(torch, \"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(234.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'int'>\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(type(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((123, 1), dtype=None).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getattr(): attribute name must be string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-99362f3b000b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: getattr(): attribute name must be string"
     ]
    }
   ],
   "source": [
    "getattr(torch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
